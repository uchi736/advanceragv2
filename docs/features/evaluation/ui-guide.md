# RAG評価結果ダッシュボード ガイド

## 概要

RAG評価機能が**シンプルで直感的なダッシュボード**に生まれ変わりました。評価実行はコマンドラインで行い、結果の分析・可視化はWebUIで行う分離型アプローチにより、高速で安定した評価体験を提供します。

## 🔄 新しい評価フロー

### Step 1: コマンドラインで評価実行
```bash
# 評価データCSVを準備
# 形式: 質問,想定の引用元1,想定の引用元2,想定の引用元3

# 評価を実行
python evaluate_rag.py
```

### Step 2: UIダッシュボードで結果分析
1. Streamlitアプリを起動: `streamlit run app.py`
2. **🎯 Evaluation**タブを開く
3. 生成された結果CSVを自動読み込み
4. インタラクティブな分析・可視化を実行

## 🎯 ダッシュボード機能

### 📈 結果ダッシュボード

最も基本的な結果表示・分析機能：

#### 結果ファイル読み込み
- **自動検出**: `evaluation_results*.csv`や`*evaluation*.csv`を自動検出
- **最新優先**: 最新ファイルを上部に表示
- **手動アップロード**: 任意のCSVファイルをドラッグ&ドロップ
- **ファイル情報**: サイズ、最終更新日時を表示

#### 評価サマリー
- **主要指標**: MRR、Recall@5、Precision@5、nDCG@5、Hit Rate@5
- **総合評価**: 性能レーティング（優秀/良好/普通/要改善/不良）
- **基本統計**: 総質問数、データ概要

#### 詳細分析
- **レーダーチャート**: 全指標の総合比較
- **ヒストグラム**: 指標別の分布表示
- **インタラクティブ**: Plotlyによる動的グラフ

#### 質問別詳細
- **質問選択**: ドロップダウンで個別質問を選択
- **詳細表示**: 質問、評価指標、期待される引用元
- **全データ表示**: 展開可能なデータテーブル

### 📊 比較分析

複数の評価結果を比較分析：

#### 複数ファイル比較
- **自動検出**: 複数の結果ファイルを検出
- **選択可能**: 最大3ファイルまで同時比較
- **名前識別**: ファイル名で結果を識別

#### 性能比較チャート
- **比較棒グラフ**: 手法別の指標比較
- **比較レーダーチャート**: 複数手法の総合比較
- **色分け表示**: 各手法を異なる色で識別

#### 改善分析
- **ベースライン設定**: 比較元の選択
- **改善率計算**: パーセンテージでの改善度表示
- **視覚的メトリクス**: 改善/悪化を色とアイコンで表示

### 📝 レポート生成

評価結果の自動レポート生成：

#### レポートタイプ
- **標準レポート**: 基本的な評価結果
- **詳細分析レポート**: 詳細な統計情報
- **比較レポート**: 複数結果の比較分析

#### 出力形式
- **CSV**: 数値データの構造化出力
- **HTML**: Webブラウザで表示可能
- **Markdown**: 文書形式での出力

#### 出力オプション
- **グラフ包含**: チャートをレポートに含める
- **生データ包含**: 詳細データテーブルを含める
- **カスタムファイル名**: タイムスタンプ付きファイル名

## 🎨 技術仕様

### 対応CSVフォーマット

**柔軟なカラム名対応**
```csv
# 質問カラム
question, Question, 質問

# 指標カラム（大文字小文字を自動判別）
MRR, mrr
Recall@5, recall@5, recall_5
Precision@5, precision@5, precision_5
nDCG@5, ndcg@5, ndcg_5
Hit Rate@5, hit_rate@5, hitrate_5
```

### インタラクティブ可視化

**Plotlyによる高度なグラフ機能**
- レーダーチャート（総合性能表示）
- ヒストグラム（分布分析）
- 比較棒グラフ（手法別比較）
- 多重レーダーチャート（複数手法比較）

### ファイル自動検出

**Globパターン対応**
```python
# 検出パターン
evaluation_results*.csv
*evaluation*.csv
```

## 🚀 使用例

### 基本的な評価・分析フロー

```bash
# 1. コマンドラインで評価実行
python evaluate_rag.py
# → evaluation_results_from_csv.csv が生成される

# 2. Streamlitアプリを起動
streamlit run app.py
```

**UIでの操作**:
1. **🎯 Evaluation**タブを開く
2. **📈 結果ダッシュボード**を選択
3. 自動検出されたファイルを確認
4. 評価サマリーとグラフを確認
5. 必要に応じて詳細分析・レポート生成

### 複数手法の比較分析

```bash
# 異なる設定で複数回評価実行
python evaluate_rag.py --similarity-method azure_embedding
# → evaluation_results_azure_embedding.csv

python evaluate_rag.py --similarity-method text_overlap  
# → evaluation_results_text_overlap.csv
```

**UIでの比較**:
1. **📊 比較分析**タブを選択
2. 比較するファイルを選択
3. 性能比較チャートを確認
4. 改善分析で具体的な差異を確認

### レポート生成・共有

1. **📝 レポート**タブを選択
2. レポートタイプと出力形式を選択
3. **📊 レポート生成**をクリック
4. **📥 ダウンロード**で結果を取得

## 📊 メリット

### ユーザー体験
- **高速レスポンス**: UI がブロックされない
- **シンプル操作**: 結果確認に特化した直感的UI
- **柔軟な分析**: 複数ファイルの自由な比較

### 技術面
- **安定性向上**: 評価処理とUI の分離
- **スケーラビリティ**: 大規模評価も影響なし
- **メンテナンス性**: シンプルな実装

### 運用面
- **バッチ処理対応**: 複数評価の一括実行
- **結果蓄積**: 履歴管理と長期分析
- **共有しやすさ**: レポート生成機能

## 🔧 トラブルシューティング

### よくある問題

**問題**: 評価結果ファイルが見つからない
**解決**: 
- `python evaluate_rag.py`が正常に実行されているか確認
- 現在のディレクトリに結果CSVが生成されているか確認
- 手動アップロードでCSVファイルを直接選択

**問題**: グラフが表示されない
**解決**:
- ブラウザのJavaScriptが有効になっているか確認
- ページを再読み込み
- Streamlitアプリを再起動

**問題**: 比較分析で結果ファイルが不足
**解決**:
- 2つ以上の結果ファイルが必要
- 異なる設定で複数回評価を実行
- ファイル名に重複がないか確認

### データ形式エラー

**問題**: CSVが正しく読み込まれない
**解決**:
- UTF-8エンコーディングで保存されているか確認
- 必要なカラム（質問、各種指標）が含まれているか確認
- カンマ区切りの形式が正しいか確認

## 🔮 今後の拡張予定

### 機能拡張
- **リアルタイム評価**: コマンドライン実行のUI統合
- **履歴管理**: 評価結果の時系列分析
- **アラート機能**: 性能劣化の自動検知
- **カスタムメトリクス**: 独自指標の追加

### UI改善
- **ドラッグ&ドロップ**: ファイルアップロードの改善
- **フィルタリング**: 結果の動的フィルタリング
- **エクスポート**: より多様な出力形式

## 📚 関連ドキュメント

- [RAG評価用CSVデータ作成ガイド](./evaluation_csv_format.md) - 評価データ準備
- [RAGシステム評価ガイド](../EVALUATION_README.md) - コマンドライン評価の詳細

## まとめ

新しい評価ダッシュボードは、**評価実行の安定性**と**結果分析の使いやすさ**を両立しています。コマンドラインでの確実な評価実行と、Webブラウザでの直感的な分析により、RAGシステムの性能を効率的に評価・改善できます。

シンプルな操作フローと豊富な可視化機能により、評価結果の深い洞察を得ることができ、RAGシステムの継続的な改善をサポートします。