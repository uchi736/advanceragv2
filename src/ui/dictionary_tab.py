import streamlit as st
import pandas as pd
import tempfile
import shutil
import asyncio
from pathlib import Path
from datetime import datetime
from sqlalchemy import text
from src.rag.term_extraction import JargonDictionaryManager
from src.rag.config import Config
from src.utils.helpers import render_term_card

@st.cache_data(ttl=300, show_spinner=False)  # 60ç§’â†’300ç§’ã«å»¶é•·ã—ã¦DBè² è·ã‚’å‰Šæ¸›
def get_all_terms_cached(_jargon_manager):
    return pd.DataFrame(_jargon_manager.get_all_terms())

def check_vector_store_has_data(rag_system):
    """Check if vector store or document chunks have any data."""
    try:
        if not rag_system or not hasattr(rag_system, 'engine'):
            return False

        with rag_system.engine.connect() as conn:
            # Check vector store (langchain_pg_embedding)
            try:
                result = conn.execute(text("SELECT COUNT(*) FROM langchain_pg_embedding"))
                vector_count = result.scalar()
            except:
                vector_count = 0

            # Check keyword search chunks (document_chunks)
            try:
                result = conn.execute(text("SELECT COUNT(*) FROM document_chunks"))
                chunk_count = result.scalar()
            except:
                chunk_count = 0

            # Return True if either table has data
            return vector_count > 0 or chunk_count > 0
    except Exception as e:
        import logging
        logging.error(f"Error checking vector store: {e}")
        return False

def render_dictionary_tab(rag_system):
    """Renders the dictionary tab."""
    st.markdown("### ğŸ“– å°‚é–€ç”¨èªè¾æ›¸")
    st.caption("ç™»éŒ²ã•ã‚ŒãŸå°‚é–€ç”¨èªãƒ»é¡ç¾©èªã‚’æ¤œç´¢ãƒ»ç¢ºèªãƒ»å‰Šé™¤ã§ãã¾ã™ã€‚")

    if not rag_system:
        st.warning("âš ï¸ RAGã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    # Check if jargon manager is available
    if not hasattr(rag_system, 'jargon_manager') or rag_system.jargon_manager is None:
        st.warning("âš ï¸ å°‚é–€ç”¨èªè¾æ›¸æ©Ÿèƒ½ã¯ç¾åœ¨åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        return

    jargon_manager = rag_system.jargon_manager

    # Manual term registration form
    with st.expander("â• æ–°ã—ã„ç”¨èªã‚’æ‰‹å‹•ã§ç™»éŒ²ã™ã‚‹"):
        with st.form(key="add_term_form"):
            new_term = st.text_input("ç”¨èª*", help="ç™»éŒ²ã™ã‚‹å°‚é–€ç”¨èª")
            new_definition = st.text_area("å®šç¾©*", help="ç”¨èªã®å®šç¾©ã‚„èª¬æ˜")
            new_domain = st.text_input("åˆ†é‡", help="é–¢é€£ã™ã‚‹æŠ€è¡“åˆ†é‡ã‚„ãƒ‰ãƒ¡ã‚¤ãƒ³")
            new_aliases = st.text_input("é¡ç¾©èª (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)", help="ä¾‹: RAG, æ¤œç´¢æ‹¡å¼µç”Ÿæˆ")
            new_related_terms = st.text_input("é–¢é€£èª (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)", help="ä¾‹: LLM, Vector Search")
            
            submitted = st.form_submit_button("ç™»éŒ²")
            if submitted:
                if not new_term or not new_definition:
                    st.error("ã€Œç”¨èªã€ã¨ã€Œå®šç¾©ã€ã¯å¿…é ˆé …ç›®ã§ã™ã€‚")
                else:
                    aliases_list = [alias.strip() for alias in new_aliases.split(',') if alias.strip()]
                    related_list = [rel.strip() for rel in new_related_terms.split(',') if rel.strip()]
                    
                    if jargon_manager.add_term(
                        term=new_term,
                        definition=new_definition,
                        domain=new_domain,
                        aliases=aliases_list,
                        related_terms=related_list
                    ):
                        st.success(f"ç”¨èªã€Œ{new_term}ã€ã‚’ç™»éŒ²ã—ã¾ã—ãŸã€‚")
                        get_all_terms_cached.clear()
                        # æ¬¡å›ã®ãƒšãƒ¼ã‚¸æ›´æ–°æ™‚ã«è‡ªå‹•çš„ã«åæ˜ ã•ã‚Œã¾ã™
                    else:
                        st.error(f"ç”¨èªã€Œ{new_term}ã€ã®ç™»éŒ²ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
    
    st.markdown("---")

    # Search and refresh buttons
    col1, col2 = st.columns([3, 1])
    with col1:
        search_keyword = st.text_input(
            "ğŸ” ç”¨èªæ¤œç´¢",
            placeholder="æ¤œç´¢ã—ãŸã„ç”¨èªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...",
            key="term_search_input"
        )
    with col2:
        st.markdown("<br>", unsafe_allow_html=True)
        if st.button("ğŸ”„ æ›´æ–°", key="refresh_terms", use_container_width=True):
            get_all_terms_cached.clear()
            st.success("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿ã—ã¦ãã ã•ã„ã€‚")

    # Load term data
    with st.spinner("ç”¨èªè¾æ›¸ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        all_terms_df = get_all_terms_cached(jargon_manager)

    # ç”¨èªç”ŸæˆUI - Always show at top
    st.markdown("### ğŸ“š ç”¨èªè¾æ›¸ã‚’ç”Ÿæˆ")

    # Check vector store status
    has_vector_data = check_vector_store_has_data(rag_system)
    if not has_vector_data:
        st.warning("âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.info("""
ğŸ’¡ **äº‹å‰æº–å‚™ãŒå¿…è¦ã§ã™**:
1. ã€Œ**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**ã€ã‚¿ãƒ–ã§PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»ç™»éŒ²
2. ã“ã®ã‚¿ãƒ–ã«æˆ»ã£ã¦ç”¨èªã‚’ç”Ÿæˆ

å®šç¾©ç”Ÿæˆã¨LLMåˆ¤å®šã‚’æœ‰åŠ¹ã«ã™ã‚‹ã«ã¯ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²ãŒå¿…é ˆã§ã™ã€‚
        """)
    else:
        st.success("âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã™ã€‚ç”¨èªç”Ÿæˆã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚")

    st.markdown("""
**ğŸ“š ç”¨èªè¾æ›¸ç”Ÿæˆã®æµã‚Œ**:
1. PDFã‹ã‚‰å€™è£œç”¨èªã‚’æŠ½å‡º (Sudachiå½¢æ…‹ç´ è§£æ + SemReRank)
2. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã§é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ â†’ å®šç¾©ç”Ÿæˆ
3. LLMã§å°‚é–€ç”¨èªã‚’åˆ¤å®šãƒ»ãƒ•ã‚£ãƒ«ã‚¿
    """)

    # Input mode selection
    input_mode = st.radio(
        "å…¥åŠ›ã‚½ãƒ¼ã‚¹",
        ("ç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æŠ½å‡º", "æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"),
        horizontal=True,
        key="term_input_mode"
    )

    uploaded_files = None
    input_dir = ""
    if input_mode == "ç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æŠ½å‡º":
        st.info("ç™»éŒ²æ¸ˆã¿ã®å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ç”¨èªã‚’æŠ½å‡ºã—ã¾ã™ã€‚")
        input_dir = "./docs"  # Placeholder, will use vector store docs
    else:
        uploaded_files = st.file_uploader(
            "ç”¨èªæŠ½å‡ºç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (PDFæ¨å¥¨)",
            accept_multiple_files=True,
            type=["pdf", "txt", "md"],
            key="term_input_files"
        )

    output_json = st.text_input(
        "å‡ºåŠ›å…ˆ (JSON)",
        value="./output/terms.json",
        key="term_output_json"
    )

    if st.button("ğŸš€ ç”¨èªã‚’æŠ½å‡ºãƒ»ç”Ÿæˆ", type="primary", use_container_width=True, key="run_term_extraction", disabled=not has_vector_data):
        if not hasattr(rag_system, 'jargon_manager') or rag_system.jargon_manager is None:
            st.error("ç”¨èªè¾æ›¸æ©Ÿèƒ½ã¯ç¾åœ¨åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        else:
            temp_dir_path = None
            try:
                if input_mode == "ç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æŠ½å‡º":
                    # Extract text from registered documents in database
                    with rag_system.engine.connect() as conn:
                        result = conn.execute(text("""
                            SELECT content
                            FROM document_chunks
                            ORDER BY created_at
                        """))
                        all_chunks = [row[0] for row in result]

                    if not all_chunks:
                        st.error("ç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                    else:
                        # Create temporary file with all content
                        temp_dir_path = Path(tempfile.mkdtemp(prefix="term_extract_registered_"))
                        temp_file = temp_dir_path / "registered_documents.txt"

                        # Write all chunks to file
                        with open(temp_file, "w", encoding="utf-8") as f:
                            f.write("\n\n".join(all_chunks))

                        input_path = str(temp_dir_path)
                        st.info(f"ç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ {len(all_chunks)} ãƒãƒ£ãƒ³ã‚¯ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")

                        output_path = Path(output_json)
                        output_path.parent.mkdir(parents=True, exist_ok=True)

                        # WebSocketã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–: ç–‘ä¼¼é€²æ—ãƒãƒ¼ã§å®šæœŸçš„ã«ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆé€ä¿¡
                        progress_bar = st.progress(0, text="åˆæœŸåŒ–ä¸­...")
                        status_text = st.empty()

                        import threading
                        import time

                        # é€²æ—æ›´æ–°ç”¨ã®ãƒ•ãƒ©ã‚°
                        extraction_complete = threading.Event()

                        def update_progress_periodically():
                            """1åˆ†ã”ã¨ã«é€²æ—ã‚’æ›´æ–°ã—ã¦WebSocketãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆã‚’ç¶­æŒ"""
                            steps = [
                                (10, "ğŸ“Š ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿ï¼†çµ±è¨ˆå‡¦ç†ä¸­..."),
                                (20, "ğŸ” å€™è£œç”¨èªæŠ½å‡ºä¸­..."),
                                (30, "ğŸ“ˆ TF-IDF/C-valueè¨ˆç®—ä¸­..."),
                                (40, "ğŸ¯ SemReRankå‡¦ç†ä¸­..."),
                                (50, "ğŸ“ å®šç¾©ç”Ÿæˆä¸­... (ã“ã‚Œã«ã¯æ•°åˆ†ã‹ã‹ã‚Šã¾ã™)"),
                                (60, "ğŸ“ å®šç¾©ç”Ÿæˆä¸­... (60%)"),
                                (70, "ğŸ“ å®šç¾©ç”Ÿæˆä¸­... (70%)"),
                                (80, "ğŸ”¬ LLMå°‚é–€ç”¨èªåˆ¤å®šä¸­... (80%)"),
                                (90, "ğŸ”¬ LLMå°‚é–€ç”¨èªåˆ¤å®šä¸­... (90%)"),
                                (95, "ğŸ“¦ çµæœã‚’ä¿å­˜ä¸­..."),
                            ]

                            for percent, message in steps:
                                if extraction_complete.is_set():
                                    break
                                progress_bar.progress(percent / 100, text=message)
                                status_text.info(f"â³ å‡¦ç†ä¸­: {message}")
                                time.sleep(60)  # 1åˆ†å¾…æ©Ÿï¼ˆWebSocketãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆç¶­æŒï¼‰

                        try:
                            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§é€²æ—æ›´æ–°ã‚’é–‹å§‹
                            progress_thread = threading.Thread(target=update_progress_periodically, daemon=True)
                            progress_thread.start()

                            # å®Ÿéš›ã®ç”¨èªæŠ½å‡ºå‡¦ç†ã‚’å®Ÿè¡Œ
                            asyncio.run(rag_system.extract_terms(input_path, str(output_path)))

                            # å‡¦ç†å®Œäº†ã‚’é€šçŸ¥
                            extraction_complete.set()
                            progress_bar.progress(1.0, text="âœ… å®Œäº†ï¼")

                        finally:
                            extraction_complete.set()
                            time.sleep(0.5)  # æœ€å¾Œã®é€²æ—è¡¨ç¤ºã‚’ç¢ºèª
                            progress_bar.empty()
                            status_text.empty()

                        st.session_state['term_extraction_output'] = str(output_path)
                        st.success(f"âœ… ç”¨èªè¾æ›¸ã‚’ç”Ÿæˆã—ã¾ã—ãŸ â†’ {output_path}")
                        st.balloons()
                        get_all_terms_cached.clear()
                        # æ¬¡å›ã®ãƒšãƒ¼ã‚¸æ›´æ–°æ™‚ã«è‡ªå‹•çš„ã«åæ˜ ã•ã‚Œã¾ã™
                else:
                    if not uploaded_files:
                        st.error("æŠ½å‡ºã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
                    else:
                        temp_dir_path = Path(tempfile.mkdtemp(prefix="term_extract_"))
                        for uploaded in uploaded_files:
                            target = temp_dir_path / uploaded.name
                            with open(target, "wb") as f:
                                f.write(uploaded.getbuffer())
                        input_path = str(temp_dir_path)

                        output_path = Path(output_json)
                        output_path.parent.mkdir(parents=True, exist_ok=True)

                        # WebSocketã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–: ç–‘ä¼¼é€²æ—ãƒãƒ¼ã§å®šæœŸçš„ã«ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆé€ä¿¡
                        progress_bar = st.progress(0, text="åˆæœŸåŒ–ä¸­...")
                        status_text = st.empty()

                        import threading
                        import time

                        # é€²æ—æ›´æ–°ç”¨ã®ãƒ•ãƒ©ã‚°
                        extraction_complete = threading.Event()

                        def update_progress_periodically():
                            """1åˆ†ã”ã¨ã«é€²æ—ã‚’æ›´æ–°ã—ã¦WebSocketãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆã‚’ç¶­æŒ"""
                            steps = [
                                (10, "ğŸ“Š ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿ï¼†çµ±è¨ˆå‡¦ç†ä¸­..."),
                                (20, "ğŸ” å€™è£œç”¨èªæŠ½å‡ºä¸­..."),
                                (30, "ğŸ“ˆ TF-IDF/C-valueè¨ˆç®—ä¸­..."),
                                (40, "ğŸ¯ SemReRankå‡¦ç†ä¸­..."),
                                (50, "ğŸ“ å®šç¾©ç”Ÿæˆä¸­... (ã“ã‚Œã«ã¯æ•°åˆ†ã‹ã‹ã‚Šã¾ã™)"),
                                (60, "ğŸ“ å®šç¾©ç”Ÿæˆä¸­... (60%)"),
                                (70, "ğŸ“ å®šç¾©ç”Ÿæˆä¸­... (70%)"),
                                (80, "ğŸ”¬ LLMå°‚é–€ç”¨èªåˆ¤å®šä¸­... (80%)"),
                                (90, "ğŸ”¬ LLMå°‚é–€ç”¨èªåˆ¤å®šä¸­... (90%)"),
                                (95, "ğŸ“¦ çµæœã‚’ä¿å­˜ä¸­..."),
                            ]

                            for percent, message in steps:
                                if extraction_complete.is_set():
                                    break
                                progress_bar.progress(percent / 100, text=message)
                                status_text.info(f"â³ å‡¦ç†ä¸­: {message}")
                                time.sleep(60)  # 1åˆ†å¾…æ©Ÿï¼ˆWebSocketãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆç¶­æŒï¼‰

                        try:
                            # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§é€²æ—æ›´æ–°ã‚’é–‹å§‹
                            progress_thread = threading.Thread(target=update_progress_periodically, daemon=True)
                            progress_thread.start()

                            # å®Ÿéš›ã®ç”¨èªæŠ½å‡ºå‡¦ç†ã‚’å®Ÿè¡Œ
                            asyncio.run(rag_system.extract_terms(input_path, str(output_path)))

                            # å‡¦ç†å®Œäº†ã‚’é€šçŸ¥
                            extraction_complete.set()
                            progress_bar.progress(1.0, text="âœ… å®Œäº†ï¼")

                        finally:
                            extraction_complete.set()
                            time.sleep(0.5)  # æœ€å¾Œã®é€²æ—è¡¨ç¤ºã‚’ç¢ºèª
                            progress_bar.empty()
                            status_text.empty()

                        st.session_state['term_extraction_output'] = str(output_path)
                        st.success(f"âœ… ç”¨èªè¾æ›¸ã‚’ç”Ÿæˆã—ã¾ã—ãŸ â†’ {output_path}")
                        st.balloons()
                        get_all_terms_cached.clear()
                        # æ¬¡å›ã®ãƒšãƒ¼ã‚¸æ›´æ–°æ™‚ã«è‡ªå‹•çš„ã«åæ˜ ã•ã‚Œã¾ã™

            except Exception as e:
                st.error(f"ç”¨èªæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                import traceback
                st.code(traceback.format_exc())
            finally:
                if temp_dir_path and temp_dir_path.exists():
                    shutil.rmtree(temp_dir_path, ignore_errors=True)

    # ç”¨èªæŠ½å‡ºçµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    output_file = st.session_state.get('term_extraction_output', '')
    if output_file and Path(output_file).exists():
        st.markdown("---")
        with st.expander("ğŸ“Š æŠ½å‡ºçµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=False):
            import json
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    terms = data.get('terms', [])

                st.success(f"âœ… {len(terms)}ä»¶ã®ç”¨èªã‚’æŠ½å‡ºã—ã¾ã—ãŸ")

                # ä¸Šä½10ä»¶ã‚’è¡¨ç¤º
                st.markdown("**ä¸Šä½10ä»¶ã®ç”¨èª:**")
                for i, term in enumerate(terms[:10], 1):
                    with st.container():
                        col1, col2 = st.columns([3, 1])
                        with col1:
                            st.markdown(f"**{i}. {term['headword']}**")
                            if term.get('definition'):
                                st.caption(term['definition'][:100] + "..." if len(term['definition']) > 100 else term['definition'])
                        with col2:
                            st.metric("ã‚¹ã‚³ã‚¢", f"{term.get('score', 0):.3f}")
                            st.caption(f"é »åº¦: {term.get('frequency', 0)}")

            except Exception as e:
                st.error(f"çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    # ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•æ§‹ç¯‰ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    terms_file = Path(output_json) if output_json else Path("./output/terms.json")
    clustering_file = Path("output/term_clusters.json")

    if terms_file.exists() and clustering_file.exists():
        st.markdown("---")
        with st.expander("ğŸ•¸ï¸ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•æ§‹ç¯‰", expanded=False):
            st.caption("æŠ½å‡ºã—ãŸç”¨èªã‹ã‚‰ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚ã‚°ãƒ©ãƒ•ã‚¨ã‚¯ã‚¹ãƒ—ãƒ­ãƒ¼ãƒ©ãƒ¼ã§å¯è¦–åŒ–ã§ãã¾ã™ã€‚")

            st.success(f"âœ… ç”¨èªãƒ•ã‚¡ã‚¤ãƒ«: {terms_file.name}")
            st.success(f"âœ… ã‚¯ãƒ©ã‚¹ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {clustering_file.name}")

            if st.button("ğŸš€ ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‚’æ§‹ç¯‰", type="primary", use_container_width=True):
                try:
                    with st.spinner("ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•æ§‹ç¯‰ä¸­..."):
                        from src.scripts.knowledge_graph.graph_builder import (
                            KnowledgeGraphDB,
                            build_nodes_from_terms,
                            build_category_nodes_from_clusters,
                            build_hierarchy_from_clustering,
                            build_similarity_from_clusters,
                            build_term_category_relationships,
                            load_terms_from_json,
                            load_clustering_results
                        )

                        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                        terms = load_terms_from_json(str(terms_file))
                        clustering_results = load_clustering_results(str(clustering_file))

                        st.info(f"ğŸ“Š èª­ã¿è¾¼ã¿: {len(terms)}ä»¶ã®ç”¨èªã€{len(clustering_results.get('categories', {}))}å€‹ã®ã‚«ãƒ†ã‚´ãƒª")

                        # ã‚°ãƒ©ãƒ•æ§‹ç¯‰
                        config = Config()
                        pg_url = f"host={config.db_host} port={config.db_port} dbname={config.db_name} user={config.db_user} password={config.db_password}"

                        with KnowledgeGraphDB(pg_url) as db:
                            # 1. ãƒãƒ¼ãƒ‰ä½œæˆ
                            progress_text = st.empty()
                            progress_text.text("1/5: ç”¨èªãƒãƒ¼ãƒ‰ä½œæˆä¸­...")
                            term_to_id = build_nodes_from_terms(db, terms)

                            # 2. ã‚«ãƒ†ã‚´ãƒªãƒãƒ¼ãƒ‰ä½œæˆ
                            progress_text.text("2/5: ã‚«ãƒ†ã‚´ãƒªãƒãƒ¼ãƒ‰ä½œæˆä¸­...")
                            category_to_id = build_category_nodes_from_clusters(db, clustering_results)

                            # 3. éšå±¤é–¢ä¿‚æ§‹ç¯‰
                            progress_text.text("3/5: éšå±¤é–¢ä¿‚æ§‹ç¯‰ä¸­...")
                            hierarchy_edges = build_hierarchy_from_clustering(db, clustering_results, term_to_id)

                            # 4. é¡ä¼¼é–¢ä¿‚æ§‹ç¯‰
                            progress_text.text("4/5: é¡ä¼¼é–¢ä¿‚æ§‹ç¯‰ä¸­...")
                            similarity_edges = build_similarity_from_clusters(db, clustering_results, term_to_id)

                            # 5. ã‚«ãƒ†ã‚´ãƒªé–¢ä¿‚æ§‹ç¯‰
                            progress_text.text("5/5: ã‚«ãƒ†ã‚´ãƒªé–¢ä¿‚æ§‹ç¯‰ä¸­...")
                            category_edges = build_term_category_relationships(
                                db, clustering_results, term_to_id, category_to_id
                            )

                            progress_text.empty()

                        # çµæœè¡¨ç¤º
                        st.success("âœ… ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•æ§‹ç¯‰å®Œäº†ï¼")

                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("ç”¨èªãƒãƒ¼ãƒ‰", f"{len(term_to_id):,}")
                            st.metric("ã‚«ãƒ†ã‚´ãƒªãƒãƒ¼ãƒ‰", f"{len(category_to_id):,}")
                        with col2:
                            st.metric("éšå±¤ã‚¨ãƒƒã‚¸", f"{hierarchy_edges:,}")
                            st.metric("é¡ä¼¼ã‚¨ãƒƒã‚¸", f"{similarity_edges:,}")
                            st.metric("ã‚«ãƒ†ã‚´ãƒªã‚¨ãƒƒã‚¸", f"{category_edges:,}")

                        total_edges = hierarchy_edges + similarity_edges + category_edges
                        st.info(f"ğŸ“Š ç·ã‚¨ãƒƒã‚¸æ•°: {total_edges:,}")
                        st.info("ğŸ’¡ ã€Œã‚°ãƒ©ãƒ•ã€ã‚¿ãƒ–ã§ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•ã‚’å¯è¦–åŒ–ã§ãã¾ã™")

                except Exception as e:
                    st.error(f"ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}")
                    import traceback
                    st.code(traceback.format_exc())

    st.markdown("---")

    # Show registered terms section
    if all_terms_df.empty:
        st.info("ã¾ã ç”¨èªãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ä¸Šè¨˜ã®ã€Œç”¨èªè¾æ›¸ã‚’ç”Ÿæˆã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return

    # Filter terms
    if search_keyword:
        terms_df = all_terms_df[
            all_terms_df['term'].str.contains(search_keyword, case=False) |
            all_terms_df['definition'].str.contains(search_keyword, case=False) |
            all_terms_df['aliases'].apply(lambda x: any(search_keyword.lower() in str(s).lower() for s in x) if x else False)
        ]
    else:
        terms_df = all_terms_df

    if terms_df.empty:
        st.info(f"ã€Œ{search_keyword}ã€ã«è©²å½“ã™ã‚‹ç”¨èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # Statistics
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ç™»éŒ²ç”¨èªæ•°", f"{len(terms_df):,}")
    with col2:
        total_synonyms = sum(len(syn_list) if syn_list else 0 for syn_list in terms_df['aliases'])
        st.metric("é¡ç¾©èªç·æ•°", f"{total_synonyms:,}")

    st.markdown("---")

    # View mode selection
    view_mode = st.radio(
        "è¡¨ç¤ºå½¢å¼",
        ["ã‚«ãƒ¼ãƒ‰å½¢å¼", "ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼"],
        horizontal=True,
        key="dict_view_mode"
    )

    if view_mode == "ã‚«ãƒ¼ãƒ‰å½¢å¼":
        for idx, row in terms_df.iterrows():
            render_term_card(row)
            # Use term as unique key instead of id (which doesn't exist in ChromaDB)
            delete_key = f"delete_card_{row['term']}_{idx}" if 'id' not in row else f"delete_card_{row['id']}"
            if st.button("å‰Šé™¤", key=delete_key, use_container_width=True):
                deleted, errors = rag_system.delete_jargon_terms([row['term']])
                if deleted:
                    st.success(f"ç”¨èªã€Œ{row['term']}ã€ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
                    get_all_terms_cached.clear()
                    # æ¬¡å›ã®ãƒšãƒ¼ã‚¸æ›´æ–°æ™‚ã«è‡ªå‹•çš„ã«åæ˜ ã•ã‚Œã¾ã™
                else:
                    st.error(f"ç”¨èªã€Œ{row['term']}ã€ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    else: # ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ï¼ˆä»®æƒ³ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾å¿œï¼‰
        display_df = terms_df.copy()
        display_df['aliases'] = display_df['aliases'].apply(lambda x: ', '.join(x) if x else '')
        display_df['related_terms'] = display_df['related_terms'].apply(lambda x: ', '.join(x) if x else '')

        # ã‚«ãƒ©ãƒ åã‚’æ—¥æœ¬èªã«
        column_mapping = {
            'term': 'ç”¨èª', 'definition': 'å®šç¾©', 'domain': 'åˆ†é‡',
            'aliases': 'é¡ç¾©èª', 'related_terms': 'é–¢é€£èª',
            'updated_at': 'æ›´æ–°æ—¥æ™‚'
        }
        # Add 'id' mapping only if it exists
        if 'id' in display_df.columns:
            column_mapping['id'] = 'ID'
        display_df.rename(columns=column_mapping, inplace=True)

        # å‰Šé™¤ãƒœã‚¿ãƒ³ç”¨ã®åˆ—ã‚’è¿½åŠ 
        display_df['å‰Šé™¤'] = False

        # ä»®æƒ³ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾å¿œ: å›ºå®šé«˜ã•ã§å¤§é‡ãƒ‡ãƒ¼ã‚¿ã§ã‚‚é«˜é€Ÿ
        edited_df = st.data_editor(
            display_df[['ç”¨èª', 'å®šç¾©', 'åˆ†é‡', 'é¡ç¾©èª', 'é–¢é€£èª', 'æ›´æ–°æ—¥æ™‚', 'å‰Šé™¤']],
            use_container_width=True,
            hide_index=True,
            height=600,  # å›ºå®šé«˜ã•ã§ä»®æƒ³ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«æœ‰åŠ¹åŒ–
            column_config={
                "å‰Šé™¤": st.column_config.CheckboxColumn(
                    "å‰Šé™¤",
                    default=False,
                ),
                "ç”¨èª": st.column_config.TextColumn("ç”¨èª", width="medium"),
                "å®šç¾©": st.column_config.TextColumn("å®šç¾©", width="large"),
                "åˆ†é‡": st.column_config.TextColumn("åˆ†é‡", width="small"),
                "é¡ç¾©èª": st.column_config.TextColumn("é¡ç¾©èª", width="medium"),
                "é–¢é€£èª": st.column_config.TextColumn("é–¢é€£èª", width="medium"),
            },
            key="dictionary_editor"
        )
        
        terms_to_delete = edited_df[edited_df['å‰Šé™¤']]
        if not terms_to_delete.empty:
            if st.button("é¸æŠã—ãŸç”¨èªã‚’å‰Šé™¤", type="primary"):
                terms_list = terms_to_delete['ç”¨èª'].tolist()
                deleted_count, error_count = rag_system.delete_jargon_terms(terms_list)
                if deleted_count:
                    st.success(f"{deleted_count}ä»¶ã®ç”¨èªã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
                if error_count:
                    st.warning(f"{error_count}ä»¶ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                get_all_terms_cached.clear()
                # æ¬¡å›ã®ãƒšãƒ¼ã‚¸æ›´æ–°æ™‚ã«è‡ªå‹•çš„ã«åæ˜ ã•ã‚Œã¾ã™

    # CSV download
    st.markdown("---")
    with st.expander("âš ï¸ ç”¨èªè¾æ›¸ã‚’å…¨å‰Šé™¤ã™ã‚‹"):
        st.warning("ã“ã®æ“ä½œã¯å–ã‚Šæ¶ˆã›ã¾ã›ã‚“ã€‚å…¨ã¦ã®å°‚é–€ç”¨èªãƒ¬ã‚³ãƒ¼ãƒ‰ãŒå‰Šé™¤ã•ã‚Œã¾ã™ã€‚", icon="âš ï¸")
        if st.button("â€¼ï¸ å…¨ç”¨èªã‚’å‰Šé™¤", type="secondary"):
            deleted_count, error_count = rag_system.delete_jargon_terms(terms_df['term'].tolist())
            if deleted_count:
                st.success(f"{deleted_count}ä»¶ã®ç”¨èªã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
            if error_count:
                st.warning(f"{error_count}ä»¶ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸã€‚", icon="âš ï¸")
            get_all_terms_cached.clear()
            # æ¬¡å›ã®ãƒšãƒ¼ã‚¸æ›´æ–°æ™‚ã«è‡ªå‹•çš„ã«åæ˜ ã•ã‚Œã¾ã™
    csv = terms_df.to_csv(index=False)
    st.download_button(
        label="ğŸ“¥ è¡¨ç¤ºä¸­ã®ç”¨èªã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv,
        file_name=f"jargon_dictionary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        mime="text/csv",
        key="csv_download_button"
    )
