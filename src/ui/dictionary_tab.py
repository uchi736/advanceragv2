import streamlit as st
import pandas as pd
import tempfile
import shutil
import asyncio
from pathlib import Path
from datetime import datetime
from sqlalchemy import text
from src.rag.term_extraction import JargonDictionaryManager
from src.rag.config import Config
from src.utils.helpers import render_term_card

@st.cache_data(ttl=300, show_spinner=False)
def get_all_terms_cached(_jargon_manager, collection_name: str):
    return pd.DataFrame(_jargon_manager.get_all_terms())

def check_vector_store_has_data(rag_system):
    """Check if vector store or document chunks have any data."""
    try:
        if not rag_system or not hasattr(rag_system, 'engine'):
            return False

        with rag_system.engine.connect() as conn:
            # Check vector store (langchain_pg_embedding)
            try:
                result = conn.execute(text("SELECT COUNT(*) FROM langchain_pg_embedding"))
                vector_count = result.scalar()
            except:
                vector_count = 0

            # Check keyword search chunks (document_chunks)
            try:
                result = conn.execute(text("SELECT COUNT(*) FROM document_chunks"))
                chunk_count = result.scalar()
            except:
                chunk_count = 0

            # Return True if either table has data
            return vector_count > 0 or chunk_count > 0
    except Exception as e:
        import logging
        logging.error(f"Error checking vector store: {e}")
        return False


def render_dictionary_tab(rag_system):
    """Renders the dictionary tab with 3 sub-tabs."""
    st.markdown("### ğŸ“– å°‚é–€ç”¨èªè¾æ›¸")
    st.caption("ç™»éŒ²ã•ã‚ŒãŸå°‚é–€ç”¨èªãƒ»é¡ç¾©èªã‚’æ¤œç´¢ãƒ»ç¢ºèªãƒ»å‰Šé™¤ã§ãã¾ã™ã€‚")

    if not rag_system:
        st.warning("âš ï¸ RAGã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    # Check if jargon manager is available
    if not hasattr(rag_system, 'jargon_manager') or rag_system.jargon_manager is None:
        st.warning("âš ï¸ å°‚é–€ç”¨èªè¾æ›¸æ©Ÿèƒ½ã¯ç¾åœ¨åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        return

    jargon_manager = rag_system.jargon_manager

    # 3ã¤ã®ã‚¿ãƒ–ã‚’ä½œæˆ
    tabs = st.tabs(["ğŸ“‹ ç”¨èªä¸€è¦§", "ğŸ”§ ç”¨èªæŠ½å‡º", "ğŸ“Š æŠ½å‡ºåˆ†æ"])

    with tabs[0]:
        render_term_list(rag_system, jargon_manager)

    with tabs[1]:
        render_term_extraction(rag_system, jargon_manager)

    with tabs[2]:
        render_term_analysis()


def render_term_list(rag_system, jargon_manager):
    """ğŸ“‹ ç”¨èªä¸€è¦§ã‚¿ãƒ–"""

    # Manual term registration form
    with st.expander("â• æ–°ã—ã„ç”¨èªã‚’æ‰‹å‹•ã§ç™»éŒ²ã™ã‚‹"):
        with st.form(key="add_term_form"):
            new_term = st.text_input("ç”¨èª*", help="ç™»éŒ²ã™ã‚‹å°‚é–€ç”¨èª")
            new_definition = st.text_area("å®šç¾©*", help="ç”¨èªã®å®šç¾©ã‚„èª¬æ˜")
            new_domain = st.text_input("åˆ†é‡", help="é–¢é€£ã™ã‚‹æŠ€è¡“åˆ†é‡ã‚„ãƒ‰ãƒ¡ã‚¤ãƒ³")
            new_aliases = st.text_input("é¡ç¾©èª (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)", help="ä¾‹: RAG, æ¤œç´¢æ‹¡å¼µç”Ÿæˆ")
            new_related_terms = st.text_input("é–¢é€£èª (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)", help="ä¾‹: LLM, Vector Search")

            submitted = st.form_submit_button("ç™»éŒ²")
            if submitted:
                if not new_term or not new_definition:
                    st.error("ã€Œç”¨èªã€ã¨ã€Œå®šç¾©ã€ã¯å¿…é ˆé …ç›®ã§ã™ã€‚")
                else:
                    aliases_list = [alias.strip() for alias in new_aliases.split(',') if alias.strip()]
                    related_list = [rel.strip() for rel in new_related_terms.split(',') if rel.strip()]

                    if jargon_manager.add_term(
                        term=new_term,
                        definition=new_definition,
                        domain=new_domain,
                        aliases=aliases_list,
                        related_terms=related_list
                    ):
                        st.success(f"ç”¨èªã€Œ{new_term}ã€ã‚’ç™»éŒ²ã—ã¾ã—ãŸã€‚")
                        get_all_terms_cached.clear()
                    else:
                        st.error(f"ç”¨èªã€Œ{new_term}ã€ã®ç™»éŒ²ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    st.markdown("---")

    # Search and refresh buttons
    col1, col2 = st.columns([3, 1])
    with col1:
        search_keyword = st.text_input(
            "ğŸ” ç”¨èªæ¤œç´¢",
            placeholder="æ¤œç´¢ã—ãŸã„ç”¨èªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...",
            key="term_search_input"
        )
    with col2:
        st.markdown("<br>", unsafe_allow_html=True)
        if st.button("ğŸ”„ æ›´æ–°", key="refresh_terms", use_container_width=True):
            get_all_terms_cached.clear()
            st.success("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿ã—ã¦ãã ã•ã„ã€‚")

    # Load term data
    with st.spinner("ç”¨èªè¾æ›¸ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        all_terms_df = get_all_terms_cached(jargon_manager, jargon_manager.collection_name)

    # Show registered terms section
    if all_terms_df.empty:
        st.info("ã¾ã ç”¨èªãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã€Œç”¨èªæŠ½å‡ºã€ã‚¿ãƒ–ã‹ã‚‰å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return

    # Filter terms
    if search_keyword:
        terms_df = all_terms_df[
            all_terms_df['term'].str.contains(search_keyword, case=False) |
            all_terms_df['definition'].str.contains(search_keyword, case=False) |
            all_terms_df['aliases'].apply(lambda x: any(search_keyword.lower() in str(s).lower() for s in x) if x else False)
        ]
    else:
        terms_df = all_terms_df

    if terms_df.empty:
        st.info(f"ã€Œ{search_keyword}ã€ã«è©²å½“ã™ã‚‹ç”¨èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # Statistics
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ç™»éŒ²ç”¨èªæ•°", f"{len(terms_df):,}")
    with col2:
        total_synonyms = sum(len(syn_list) if syn_list else 0 for syn_list in terms_df['aliases'])
        st.metric("é¡ç¾©èªç·æ•°", f"{total_synonyms:,}")

    st.markdown("---")

    # View mode selection
    view_mode = st.radio(
        "è¡¨ç¤ºå½¢å¼",
        ["ã‚«ãƒ¼ãƒ‰å½¢å¼", "ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼"],
        horizontal=True,
        key="dict_view_mode"
    )

    if view_mode == "ã‚«ãƒ¼ãƒ‰å½¢å¼":
        for idx, row in terms_df.iterrows():
            render_term_card(row)
            delete_key = f"delete_card_{row['term']}_{idx}" if 'id' not in row else f"delete_card_{row['id']}"
            if st.button("å‰Šé™¤", key=delete_key, use_container_width=True):
                deleted, errors = rag_system.delete_jargon_terms([row['term']])
                if deleted:
                    st.success(f"ç”¨èªã€Œ{row['term']}ã€ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
                    get_all_terms_cached.clear()
                else:
                    st.error(f"ç”¨èªã€Œ{row['term']}ã€ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    else:  # ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ï¼ˆä»®æƒ³ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾å¿œï¼‰
        display_df = terms_df.copy()
        display_df['aliases'] = display_df['aliases'].apply(lambda x: ', '.join(x) if x else '')
        display_df['related_terms'] = display_df['related_terms'].apply(lambda x: ', '.join(x) if x else '')

        # ã‚«ãƒ©ãƒ åã‚’æ—¥æœ¬èªã«
        column_mapping = {
            'term': 'ç”¨èª', 'definition': 'å®šç¾©', 'domain': 'åˆ†é‡',
            'aliases': 'é¡ç¾©èª', 'related_terms': 'é–¢é€£èª',
            'updated_at': 'æ›´æ–°æ—¥æ™‚'
        }
        if 'id' in display_df.columns:
            column_mapping['id'] = 'ID'
        display_df.rename(columns=column_mapping, inplace=True)

        # å‰Šé™¤ãƒœã‚¿ãƒ³ç”¨ã®åˆ—ã‚’è¿½åŠ 
        display_df['å‰Šé™¤'] = False

        # ä»®æƒ³ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾å¿œ: å›ºå®šé«˜ã•ã§å¤§é‡ãƒ‡ãƒ¼ã‚¿ã§ã‚‚é«˜é€Ÿ
        edited_df = st.data_editor(
            display_df[['ç”¨èª', 'å®šç¾©', 'åˆ†é‡', 'é¡ç¾©èª', 'é–¢é€£èª', 'æ›´æ–°æ—¥æ™‚', 'å‰Šé™¤']],
            use_container_width=True,
            hide_index=True,
            height=600,
            column_config={
                "å‰Šé™¤": st.column_config.CheckboxColumn("å‰Šé™¤", default=False),
                "ç”¨èª": st.column_config.TextColumn("ç”¨èª", width="medium"),
                "å®šç¾©": st.column_config.TextColumn("å®šç¾©", width="large"),
                "åˆ†é‡": st.column_config.TextColumn("åˆ†é‡", width="small"),
                "é¡ç¾©èª": st.column_config.TextColumn("é¡ç¾©èª", width="medium"),
                "é–¢é€£èª": st.column_config.TextColumn("é–¢é€£èª", width="medium"),
            },
            key="dictionary_editor"
        )

        terms_to_delete = edited_df[edited_df['å‰Šé™¤']]
        if not terms_to_delete.empty:
            if st.button("é¸æŠã—ãŸç”¨èªã‚’å‰Šé™¤", type="primary"):
                terms_list = terms_to_delete['ç”¨èª'].tolist()
                deleted_count, error_count = rag_system.delete_jargon_terms(terms_list)
                if deleted_count:
                    st.success(f"{deleted_count}ä»¶ã®ç”¨èªã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
                if error_count:
                    st.warning(f"{error_count}ä»¶ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                get_all_terms_cached.clear()

    # CSV download
    st.markdown("---")
    with st.expander("âš ï¸ ç”¨èªè¾æ›¸ã‚’å…¨å‰Šé™¤ã™ã‚‹"):
        st.warning("ã“ã®æ“ä½œã¯å–ã‚Šæ¶ˆã›ã¾ã›ã‚“ã€‚å…¨ã¦ã®å°‚é–€ç”¨èªãƒ¬ã‚³ãƒ¼ãƒ‰ãŒå‰Šé™¤ã•ã‚Œã¾ã™ã€‚", icon="âš ï¸")
        if st.button("â€¼ï¸ å…¨ç”¨èªã‚’å‰Šé™¤", type="secondary"):
            deleted_count, error_count = rag_system.delete_jargon_terms(terms_df['term'].tolist())
            if deleted_count:
                st.success(f"{deleted_count}ä»¶ã®ç”¨èªã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
            if error_count:
                st.warning(f"{error_count}ä»¶ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸã€‚", icon="âš ï¸")
            get_all_terms_cached.clear()

    csv = terms_df.to_csv(index=False)
    st.download_button(
        label="ğŸ“¥ è¡¨ç¤ºä¸­ã®ç”¨èªã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv,
        file_name=f"jargon_dictionary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        mime="text/csv",
        key="csv_download_button"
    )


def render_term_extraction(rag_system, jargon_manager):
    """ğŸ”§ ç”¨èªæŠ½å‡ºã‚¿ãƒ–"""
    st.markdown("### ğŸ“š ç”¨èªè¾æ›¸ã‚’ç”Ÿæˆ")

    # Check vector store status
    has_vector_data = check_vector_store_has_data(rag_system)
    if not has_vector_data:
        st.warning("âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.info("""
ğŸ’¡ **äº‹å‰æº–å‚™ãŒå¿…è¦ã§ã™**:
1. ã€Œ**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**ã€ã‚¿ãƒ–ã§PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»ç™»éŒ²
2. ã“ã®ã‚¿ãƒ–ã«æˆ»ã£ã¦ç”¨èªã‚’ç”Ÿæˆ

å®šç¾©ç”Ÿæˆã¨LLMåˆ¤å®šã‚’æœ‰åŠ¹ã«ã™ã‚‹ã«ã¯ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²ãŒå¿…é ˆã§ã™ã€‚
        """)
    else:
        st.success("âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã™ã€‚ç”¨èªç”Ÿæˆã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚")

    st.markdown("""
**ğŸ“š ç”¨èªè¾æ›¸ç”Ÿæˆã®æµã‚Œ**:
1. PDFã‹ã‚‰å€™è£œç”¨èªã‚’æŠ½å‡º (Sudachiå½¢æ…‹ç´ è§£æ + SemReRank)
2. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã§é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ â†’ å®šç¾©ç”Ÿæˆ
3. LLMã§å°‚é–€ç”¨èªã‚’åˆ¤å®šãƒ»ãƒ•ã‚£ãƒ«ã‚¿
    """)

    # Input mode selection
    input_mode = st.radio(
        "å…¥åŠ›ã‚½ãƒ¼ã‚¹",
        ("ç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æŠ½å‡º", "æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"),
        horizontal=True,
        key="term_input_mode"
    )

    uploaded_files = None
    input_dir = ""
    if input_mode == "ç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æŠ½å‡º":
        st.info("ç™»éŒ²æ¸ˆã¿ã®å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ç”¨èªã‚’æŠ½å‡ºã—ã¾ã™ã€‚")
        input_dir = "./docs"
    else:
        uploaded_files = st.file_uploader(
            "ç”¨èªæŠ½å‡ºç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (PDFæ¨å¥¨)",
            accept_multiple_files=True,
            type=["pdf", "txt", "md"],
            key="term_input_files"
        )

    output_json = st.text_input(
        "å‡ºåŠ›å…ˆ (JSON)",
        value="./output/terms.json",
        key="term_output_json"
    )

    if st.button("ğŸš€ ç”¨èªã‚’æŠ½å‡ºãƒ»ç”Ÿæˆ", type="primary", use_container_width=True, key="run_term_extraction", disabled=not has_vector_data):
        temp_dir_path = None
        try:
            if input_mode == "ç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æŠ½å‡º":
                # Extract text from registered documents in database
                with rag_system.engine.connect() as conn:
                    result = conn.execute(text("""
                        SELECT content
                        FROM document_chunks
                        ORDER BY created_at
                    """))
                    all_chunks = [row[0] for row in result]

                if not all_chunks:
                    st.error("ç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                else:
                    # Create temporary file with all content
                    temp_dir_path = Path(tempfile.mkdtemp(prefix="term_extract_registered_"))
                    temp_file = temp_dir_path / "registered_documents.txt"

                    # Write all chunks to file
                    with open(temp_file, "w", encoding="utf-8") as f:
                        f.write("\n\n".join(all_chunks))

                    input_path = str(temp_dir_path)
                    st.info(f"ç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ {len(all_chunks)} ãƒãƒ£ãƒ³ã‚¯ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")

                    output_path = Path(output_json)
                    output_path.parent.mkdir(parents=True, exist_ok=True)

                    # WebSocketã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–
                    progress_bar = st.progress(0, text="åˆæœŸåŒ–ä¸­...")
                    status_text = st.empty()

                    import threading
                    import time

                    extraction_complete = threading.Event()

                    def update_progress_periodically():
                        steps = [
                            (10, "ğŸ“Š ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿ï¼†çµ±è¨ˆå‡¦ç†ä¸­..."),
                            (20, "ğŸ” å€™è£œç”¨èªæŠ½å‡ºä¸­..."),
                            (30, "ğŸ“ˆ TF-IDF/C-valueè¨ˆç®—ä¸­..."),
                            (40, "ğŸ¯ SemReRankå‡¦ç†ä¸­..."),
                            (50, "ğŸ“ å®šç¾©ç”Ÿæˆä¸­... (ã“ã‚Œã«ã¯æ•°åˆ†ã‹ã‹ã‚Šã¾ã™)"),
                            (60, "ğŸ“ å®šç¾©ç”Ÿæˆä¸­... (60%)"),
                            (70, "ğŸ“ å®šç¾©ç”Ÿæˆä¸­... (70%)"),
                            (80, "ğŸ”¬ LLMå°‚é–€ç”¨èªåˆ¤å®šä¸­... (80%)"),
                            (90, "ğŸ”¬ LLMå°‚é–€ç”¨èªåˆ¤å®šä¸­... (90%)"),
                            (95, "ğŸ“¦ çµæœã‚’ä¿å­˜ä¸­..."),
                        ]

                        for percent, message in steps:
                            if extraction_complete.is_set():
                                break
                            progress_bar.progress(percent / 100, text=message)
                            status_text.info(f"â³ å‡¦ç†ä¸­: {message}")
                            time.sleep(60)

                    try:
                        progress_thread = threading.Thread(target=update_progress_periodically, daemon=True)
                        progress_thread.start()

                        asyncio.run(rag_system.extract_terms(input_path, str(output_path)))

                        extraction_complete.set()
                        progress_bar.progress(1.0, text="âœ… å®Œäº†ï¼")

                    finally:
                        extraction_complete.set()
                        time.sleep(0.5)
                        progress_bar.empty()
                        status_text.empty()

                    st.session_state['term_extraction_output'] = str(output_path)

                    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•çš„ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²
                    try:
                        import json
                        with open(output_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            terms = data.get('terms', [])

                        if terms:
                            st.info(f"ğŸ“¥ {len(terms)}ä»¶ã®ç”¨èªã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²ä¸­...")

                            registered_count = 0
                            skipped_count = 0
                            error_count = 0

                            for term_data in terms:
                                try:
                                    if jargon_manager.add_term(
                                        term=term_data.get('headword', ''),
                                        definition=term_data.get('definition', ''),
                                        domain='',
                                        aliases=term_data.get('synonyms', []),
                                        related_terms=term_data.get('related_terms', [])
                                    ):
                                        registered_count += 1
                                    else:
                                        skipped_count += 1
                                except Exception as e:
                                    error_count += 1
                                    import logging
                                    logging.error(f"Failed to register term '{term_data.get('headword', '')}': {e}")

                            st.success(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç™»éŒ²å®Œäº†: {registered_count}ä»¶ç™»éŒ²ã€{skipped_count}ä»¶ã‚¹ã‚­ãƒƒãƒ—ã€{error_count}ä»¶ã‚¨ãƒ©ãƒ¼")
                        else:
                            st.warning("æŠ½å‡ºã•ã‚ŒãŸç”¨èªãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

                    except Exception as e:
                        st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç™»éŒ²ã‚¨ãƒ©ãƒ¼: {e}")

                    st.success(f"âœ… ç”¨èªè¾æ›¸ã‚’ç”Ÿæˆã—ã¾ã—ãŸ â†’ {output_path}")
                    get_all_terms_cached.clear()
                    st.rerun()
            else:
                if not uploaded_files:
                    st.error("æŠ½å‡ºã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
                else:
                    temp_dir_path = Path(tempfile.mkdtemp(prefix="term_extract_"))
                    for uploaded in uploaded_files:
                        target = temp_dir_path / uploaded.name
                        with open(target, "wb") as f:
                            f.write(uploaded.getbuffer())
                    input_path = str(temp_dir_path)

                    output_path = Path(output_json)
                    output_path.parent.mkdir(parents=True, exist_ok=True)

                    # Similar progress handling as above
                    # ... (åŒæ§˜ã®å‡¦ç†)

        except Exception as e:
            st.error(f"ç”¨èªæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            st.code(traceback.format_exc())
        finally:
            if temp_dir_path and temp_dir_path.exists():
                shutil.rmtree(temp_dir_path, ignore_errors=True)

    # ç”¨èªæŠ½å‡ºçµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    output_file = st.session_state.get('term_extraction_output', '')
    if output_file and Path(output_file).exists():
        st.markdown("---")
        with st.expander("ğŸ“Š æŠ½å‡ºçµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=False):
            import json
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    terms = data.get('terms', [])

                st.success(f"âœ… {len(terms)}ä»¶ã®ç”¨èªã‚’æŠ½å‡ºã—ã¾ã—ãŸ")

                for i, term in enumerate(terms[:10], 1):
                    with st.container():
                        col1, col2 = st.columns([3, 1])
                        with col1:
                            st.markdown(f"**{i}. {term['headword']}**")
                            if term.get('definition'):
                                st.caption(term['definition'][:100] + "..." if len(term['definition']) > 100 else term['definition'])
                        with col2:
                            st.metric("ã‚¹ã‚³ã‚¢", f"{term.get('score', 0):.3f}")
                            st.caption(f"é »åº¦: {term.get('frequency', 0)}")

            except Exception as e:
                st.error(f"çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")


def render_term_analysis():
    """ğŸ“Š æŠ½å‡ºåˆ†æã‚¿ãƒ–"""
    st.subheader("ğŸ“Š å°‚é–€ç”¨èªæŠ½å‡ºã®ç‰¹å¾´åˆ†æ")
    st.caption("Ground Truthã¨ã®æ¯”è¼ƒã«ã‚ˆã‚Šã€TF-IDF+C-valueã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æœ‰åŠ¹æ€§ã‚’æ¤œè¨¼ã—ã¾ã™")

    st.info("""
**ã“ã®åˆ†æã§ã¯ä»¥ä¸‹ã‚’ç¢ºèªã§ãã¾ã™:**
- ã‚«ãƒ†ã‚´ãƒªåˆ¥Recallï¼ˆã©ã®ã‚¿ã‚¤ãƒ—ã®ç”¨èªãŒæŠ½å‡ºã•ã‚Œã¦ã„ã‚‹ã‹ï¼‰
- é »åº¦åˆ¥Recallï¼ˆä½é »åº¦ç”¨èªã¯è¦‹é€ƒã•ã‚Œã¦ã„ãªã„ã‹ï¼‰
- TF-IDF/C-valueã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ
- è¦‹é€ƒã•ã‚ŒãŸç”¨èªï¼ˆFalse Negativesï¼‰
- èª¤æ¤œå‡ºã•ã‚ŒãŸç”¨èªï¼ˆFalse Positivesï¼‰
    """)

    # 1. Ground Truth ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    ground_truth_file = st.file_uploader(
        "Ground Truth JSON",
        type=['json'],
        help="æ­£è§£ãƒ‡ãƒ¼ã‚¿ (ä¾‹: test_data/ground_truth.json)",
        key="gt_upload"
    )

    # 2. å€™è£œç”¨èªï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã®è‡ªå‹•æ¤œå‡º
    candidates_path = None
    candidates_file_obj = None

    # 2-1. ã‚»ãƒƒã‚·ãƒ§ãƒ³å¤‰æ•°ã‹ã‚‰æŠ½å‡ºçµæœã®å ´æ‰€ã‚’æ¨æ¸¬
    if 'term_extraction_output' in st.session_state:
        output_path = Path(st.session_state['term_extraction_output'])
        debug_path = output_path.parent / "term_extraction_debug.json"
        if debug_path.exists():
            candidates_path = debug_path
            st.success(f"âœ… å€™è£œç”¨èªãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•æ¤œå‡º: {candidates_path}")
        else:
            candidates_path = None

    # 2-2. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ã‹ã‚‰æ¤œå‡º
    if not candidates_path and Path("./output/term_extraction_debug.json").exists():
        candidates_path = Path("./output/term_extraction_debug.json")
        st.success(f"âœ… å€™è£œç”¨èªãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•æ¤œå‡º: {candidates_path}")

    # 2-3. ã©ã¡ã‚‰ã‚‚ãªã„å ´åˆ
    if not candidates_path:
        st.warning("âš ï¸ å€™è£œç”¨èªãƒ‡ãƒ¼ã‚¿ï¼ˆterm_extraction_debug.jsonï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        st.info("å…ˆã«ã€Œç”¨èªæŠ½å‡ºã€ã‚¿ãƒ–ã§æŠ½å‡ºã‚’å®Ÿè¡Œã™ã‚‹ã‹ã€æ‰‹å‹•ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")

    # 2-4. æ‰‹å‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆä»»æ„ or å¿…é ˆï¼‰
    if candidates_path:
        st.caption("åˆ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã†å ´åˆã¯ä¸‹è¨˜ã‹ã‚‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰â†“")
        label = "åˆ¥ã®å€™è£œç”¨èªJSONã‚’ä½¿ã†ï¼ˆä»»æ„ï¼‰"
    else:
        label = "å€™è£œç”¨èª JSONï¼ˆå¿…é ˆï¼‰"

    manual_candidates = st.file_uploader(
        label,
        type=['json'],
        help="å€™è£œç”¨èªãƒ‡ãƒ¼ã‚¿ (ä¾‹: output/term_extraction_debug.json)",
        key="candidates_upload"
    )

    # æ‰‹å‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒã‚ã‚Œã°ãã¡ã‚‰ã‚’å„ªå…ˆ
    if manual_candidates:
        candidates_file_obj = manual_candidates
        st.info("âœ… æ‰‹å‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
    elif candidates_path:
        # è‡ªå‹•æ¤œå‡ºã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
        pass
    else:
        candidates_file_obj = None

    # 3. åˆ†æå®Ÿè¡Œãƒœã‚¿ãƒ³ã®æœ‰åŠ¹/ç„¡åŠ¹
    can_analyze = ground_truth_file and (candidates_path or candidates_file_obj)

    if not can_analyze:
        missing = []
        if not ground_truth_file:
            missing.append("Ground Truth JSON")
        if not (candidates_path or candidates_file_obj):
            missing.append("å€™è£œç”¨èª JSON")
        st.warning(f"âš ï¸ ä¸è¶³: {', '.join(missing)}")

    # 4. åˆ†æå®Ÿè¡Œ
    if st.button("ğŸ” åˆ†æã‚’å®Ÿè¡Œ", type="primary", use_container_width=True, disabled=not can_analyze):
        with st.spinner("åˆ†æä¸­..."):
            try:
                import json
                from src.rag.term_analysis import TermFeatureAnalyzer

                # Ground Truthèª­ã¿è¾¼ã¿
                ground_truth = json.load(ground_truth_file)

                # å€™è£œç”¨èªãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
                if candidates_file_obj:
                    candidates_data = json.load(candidates_file_obj)
                else:
                    with open(candidates_path, 'r', encoding='utf-8') as f:
                        candidates_data = json.load(f)

                # candidatesã‚­ãƒ¼ã‹ã‚‰å€™è£œç”¨èªãƒªã‚¹ãƒˆã‚’å–å¾—
                candidate_terms = candidates_data.get('candidates', [])

                st.info(f"ğŸ“Š å€™è£œç”¨èªæ•°: {len(candidate_terms)}ä»¶")

                # åˆ†æå®Ÿè¡Œï¼ˆdocumentsã¯ç©ºãƒªã‚¹ãƒˆï¼‰
                analyzer = TermFeatureAnalyzer(ground_truth, candidate_terms, [])
                results = analyzer.analyze()

                # çµæœè¡¨ç¤º
                st.success("âœ… åˆ†æå®Œäº†")

                # 1. æ¦‚è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                metrics = results['overall_metrics']
                col1, col2, col3, col4 = st.columns(4)
                col1.metric("Recall", f"{metrics['recall']:.1%}")
                col2.metric("Precision", f"{metrics['precision']:.1%}")
                col3.metric("F1 Score", f"{metrics['f1_score']:.1%}")
                col4.metric("Ground Truth", metrics['total_ground_truth'])

                st.markdown("---")

                # 2. ã‚«ãƒ†ã‚´ãƒªåˆ¥Recall
                st.subheader("ğŸ“ˆ ã‚«ãƒ†ã‚´ãƒªåˆ¥Recall")
                category_analysis = results['category_analysis']

                category_df = pd.DataFrame([
                    {
                        'ã‚«ãƒ†ã‚´ãƒª': cat,
                        'Ground Truthæ•°': data['total'],
                        'æŠ½å‡ºæ•°': data['extracted'],
                        'Recall': f"{data['recall']:.1%}"
                    }
                    for cat, data in sorted(category_analysis.items(), key=lambda x: x[1]['recall'], reverse=True)
                ])
                st.dataframe(category_df, use_container_width=True, hide_index=True)

                # 3. é »åº¦åˆ¥Recall
                st.markdown("---")
                st.subheader("ğŸ“Š é »åº¦åˆ¥Recall")
                freq_analysis = results['frequency_analysis']

                freq_df = pd.DataFrame([
                    {
                        'é »åº¦ç¯„å›²': label,
                        'Ground Truthæ•°': data['total'],
                        'æŠ½å‡ºæ•°': data['extracted'],
                        'Recall': f"{data['recall']:.1%}"
                    }
                    for label, data in freq_analysis.items()
                ])
                st.dataframe(freq_df, use_container_width=True, hide_index=True)

                # 4. è¦‹é€ƒã•ã‚ŒãŸç”¨èª
                st.markdown("---")
                with st.expander("âŒ è¦‹é€ƒã•ã‚ŒãŸç”¨èª (False Negatives)", expanded=False):
                    missed_terms = results['missed_terms'][:30]
                    missed_df = pd.DataFrame(missed_terms)
                    st.dataframe(missed_df, use_container_width=True, hide_index=True)

                # 5. èª¤æ¤œå‡ºã•ã‚ŒãŸç”¨èª
                with st.expander("âš ï¸ èª¤æ¤œå‡ºã•ã‚ŒãŸç”¨èª (False Positives)", expanded=False):
                    false_positives = results['false_positives'][:30]
                    fp_df = pd.DataFrame(false_positives)
                    st.dataframe(fp_df, use_container_width=True, hide_index=True)

                # 6. ãƒ¬ãƒãƒ¼ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                st.markdown("---")
                md_report = analyzer.generate_markdown_report(results)
                st.download_button(
                    "ğŸ“¥ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (Markdown)",
                    data=md_report,
                    file_name="term_analysis_report.md",
                    mime="text/markdown",
                    use_container_width=True
                )

            except Exception as e:
                st.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                import traceback
                st.code(traceback.format_exc())
