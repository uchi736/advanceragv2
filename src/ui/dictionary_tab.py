import streamlit as st
import pandas as pd
import tempfile
import shutil
import asyncio
from pathlib import Path
from datetime import datetime
from sqlalchemy import text
from src.rag.term_extraction import JargonDictionaryManager
from src.rag.config import Config
from src.utils.helpers import render_term_card

def get_all_terms_cached(_jargon_manager, collection_name: str):
    # ä»¥å‰ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦ã„ãŸãŒã€DBæ›´æ–°ã‚’å³æ™‚UIã«åæ˜ ã•ã›ã‚‹ãŸã‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å¤–ã™
    return pd.DataFrame(_jargon_manager.get_all_terms())

def check_vector_store_has_data(rag_system, collection_name: str):
    """Check if vector store or document chunks have any data for the specified collection."""
    try:
        if not rag_system or not hasattr(rag_system, 'engine'):
            return False

        with rag_system.engine.connect() as conn:
            # Check vector store (langchain_pg_embedding) for this collection
            try:
                # Get collection_id for this collection_name
                result = conn.execute(
                    text("SELECT uuid FROM langchain_pg_collection WHERE name = :cname"),
                    {"cname": collection_name}
                )
                collection_id = result.scalar()

                if collection_id:
                    result = conn.execute(
                        text("SELECT COUNT(*) FROM langchain_pg_embedding WHERE collection_id = :cid"),
                        {"cid": collection_id}
                    )
                    vector_count = result.scalar()
                else:
                    vector_count = 0
            except:
                vector_count = 0

            # Check keyword search chunks (document_chunks) for this collection
            try:
                result = conn.execute(
                    text("SELECT COUNT(*) FROM document_chunks WHERE collection_name = :cname"),
                    {"cname": collection_name}
                )
                chunk_count = result.scalar()
            except:
                chunk_count = 0

            # Return True if either table has data
            return vector_count > 0 or chunk_count > 0
    except Exception as e:
        import logging
        logging.error(f"Error checking vector store: {e}")
        return False


def render_dictionary_tab(rag_system):
    """Renders the dictionary tab with 3 sub-tabs."""
    st.markdown("### ğŸ“– å°‚é–€ç”¨èªè¾æ›¸")
    st.caption("ç™»éŒ²ã•ã‚ŒãŸå°‚é–€ç”¨èªãƒ»é¡ç¾©èªã‚’æ¤œç´¢ãƒ»ç¢ºèªãƒ»å‰Šé™¤ã§ãã¾ã™ã€‚")

    if not rag_system:
        st.warning("âš ï¸ RAGã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    # Check if jargon manager is available
    if not hasattr(rag_system, 'jargon_manager') or rag_system.jargon_manager is None:
        st.warning("âš ï¸ å°‚é–€ç”¨èªè¾æ›¸æ©Ÿèƒ½ã¯ç¾åœ¨åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        return

    jargon_manager = rag_system.jargon_manager

    # 3ã¤ã®ã‚¿ãƒ–ã‚’ä½œæˆ
    tabs = st.tabs(["ğŸ“‹ ç”¨èªä¸€è¦§", "ğŸ”§ ç”¨èªæŠ½å‡º", "ğŸ“Š æŠ½å‡ºåˆ†æ"])

    with tabs[0]:
        render_term_list(rag_system, jargon_manager)

    with tabs[1]:
        render_term_extraction(rag_system, jargon_manager)

    with tabs[2]:
        render_term_analysis()


def render_term_list(rag_system, jargon_manager):
    """ğŸ“‹ ç”¨èªä¸€è¦§ã‚¿ãƒ–"""

    # Manual term registration form
    with st.expander("â• æ–°ã—ã„ç”¨èªã‚’æ‰‹å‹•ã§ç™»éŒ²ã™ã‚‹"):
        with st.form(key="add_term_form"):
            new_term = st.text_input("ç”¨èª*", help="ç™»éŒ²ã™ã‚‹å°‚é–€ç”¨èª")
            new_definition = st.text_area("å®šç¾©*", help="ç”¨èªã®å®šç¾©ã‚„èª¬æ˜")
            new_domain = st.text_input("åˆ†é‡", help="é–¢é€£ã™ã‚‹æŠ€è¡“åˆ†é‡ã‚„ãƒ‰ãƒ¡ã‚¤ãƒ³")
            new_aliases = st.text_input("é¡ç¾©èª (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)", help="ä¾‹: RAG, æ¤œç´¢æ‹¡å¼µç”Ÿæˆ")
            new_related_terms = st.text_input("é–¢é€£èª (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)", help="ä¾‹: LLM, Vector Search")

            submitted = st.form_submit_button("ç™»éŒ²")
            if submitted:
                if not new_term or not new_definition:
                    st.error("ã€Œç”¨èªã€ã¨ã€Œå®šç¾©ã€ã¯å¿…é ˆé …ç›®ã§ã™ã€‚")
                else:
                    aliases_list = [alias.strip() for alias in new_aliases.split(',') if alias.strip()]
                    related_list = [rel.strip() for rel in new_related_terms.split(',') if rel.strip()]

                    if jargon_manager.add_term(
                        term=new_term,
                        definition=new_definition,
                        domain=new_domain,
                        aliases=aliases_list,
                        related_terms=related_list
                    ):
                        st.success(f"ç”¨èªã€Œ{new_term}ã€ã‚’ç™»éŒ²ã—ã¾ã—ãŸã€‚")
                        if hasattr(get_all_terms_cached, "clear"):
                            get_all_terms_cached.clear()
                    else:
                        st.error(f"ç”¨èªã€Œ{new_term}ã€ã®ç™»éŒ²ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    st.markdown("---")

    # Search and refresh buttons
    col1, col2 = st.columns([3, 1])
    with col1:
        search_keyword = st.text_input(
            "ğŸ” ç”¨èªæ¤œç´¢",
            placeholder="æ¤œç´¢ã—ãŸã„ç”¨èªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...",
            key="term_search_input"
        )
    with col2:
        st.markdown("<br>", unsafe_allow_html=True)
        if st.button("ğŸ”„ æ›´æ–°", key="refresh_terms", use_container_width=True):
            if hasattr(get_all_terms_cached, "clear"):
                if hasattr(get_all_terms_cached, "clear"):
                    get_all_terms_cached.clear()
            st.success("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã—ãŸã€‚ãƒšãƒ¼ã‚¸ã‚’å†èª­ã¿è¾¼ã¿ã—ã¦ãã ã•ã„ã€‚")

    # Load term data
    with st.spinner("ç”¨èªè¾æ›¸ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        all_terms_df = get_all_terms_cached(jargon_manager, jargon_manager.collection_name)

    # Show registered terms section
    if all_terms_df.empty:
        st.info("ã¾ã ç”¨èªãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã€Œç”¨èªæŠ½å‡ºã€ã‚¿ãƒ–ã‹ã‚‰å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return

    # Filter terms
    if search_keyword:
        terms_df = all_terms_df[
            all_terms_df['term'].str.contains(search_keyword, case=False) |
            all_terms_df['definition'].str.contains(search_keyword, case=False) |
            all_terms_df['aliases'].apply(lambda x: any(search_keyword.lower() in str(s).lower() for s in x) if x else False)
        ]
    else:
        terms_df = all_terms_df

    if terms_df.empty:
        st.info(f"ã€Œ{search_keyword}ã€ã«è©²å½“ã™ã‚‹ç”¨èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # Statistics
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ç™»éŒ²ç”¨èªæ•°", f"{len(terms_df):,}")
    with col2:
        total_synonyms = sum(len(syn_list) if syn_list else 0 for syn_list in terms_df['aliases'])
        st.metric("é¡ç¾©èªç·æ•°", f"{total_synonyms:,}")

    st.markdown("---")

    # View mode selection
    view_mode = st.radio(
        "è¡¨ç¤ºå½¢å¼",
        ["ã‚«ãƒ¼ãƒ‰å½¢å¼", "ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼"],
        horizontal=True,
        key="dict_view_mode"
    )

    if view_mode == "ã‚«ãƒ¼ãƒ‰å½¢å¼":
        for idx, row in terms_df.iterrows():
            render_term_card(row)
            delete_key = f"delete_card_{row['term']}_{idx}" if 'id' not in row else f"delete_card_{row['id']}"
            if st.button("å‰Šé™¤", key=delete_key, use_container_width=True):
                deleted, errors = rag_system.delete_jargon_terms([row['term']])
                if deleted:
                    st.success(f"ç”¨èªã€Œ{row['term']}ã€ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
                    if hasattr(get_all_terms_cached, "clear"):
                        get_all_terms_cached.clear()
                else:
                    st.error(f"ç”¨èªã€Œ{row['term']}ã€ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    else:  # ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ï¼ˆä»®æƒ³ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾å¿œï¼‰
        display_df = terms_df.copy()
        display_df['aliases'] = display_df['aliases'].apply(lambda x: ', '.join(x) if x else '')
        display_df['related_terms'] = display_df['related_terms'].apply(lambda x: ', '.join(x) if x else '')

        # ã‚«ãƒ©ãƒ åã‚’æ—¥æœ¬èªã«
        column_mapping = {
            'term': 'ç”¨èª', 'definition': 'å®šç¾©', 'domain': 'åˆ†é‡',
            'aliases': 'é¡ç¾©èª', 'related_terms': 'é–¢é€£èª',
            'updated_at': 'æ›´æ–°æ—¥æ™‚'
        }
        if 'id' in display_df.columns:
            column_mapping['id'] = 'ID'
        display_df.rename(columns=column_mapping, inplace=True)

        # å‰Šé™¤ãƒœã‚¿ãƒ³ç”¨ã®åˆ—ã‚’è¿½åŠ 
        display_df['å‰Šé™¤'] = False

        # ä»®æƒ³ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾å¿œ: å›ºå®šé«˜ã•ã§å¤§é‡ãƒ‡ãƒ¼ã‚¿ã§ã‚‚é«˜é€Ÿ
        edited_df = st.data_editor(
            display_df[['ç”¨èª', 'å®šç¾©', 'åˆ†é‡', 'é¡ç¾©èª', 'é–¢é€£èª', 'æ›´æ–°æ—¥æ™‚', 'å‰Šé™¤']],
            use_container_width=True,
            hide_index=True,
            height=600,
            column_config={
                "å‰Šé™¤": st.column_config.CheckboxColumn("å‰Šé™¤", default=False),
                "ç”¨èª": st.column_config.TextColumn("ç”¨èª", width="medium"),
                "å®šç¾©": st.column_config.TextColumn("å®šç¾©", width="large"),
                "åˆ†é‡": st.column_config.TextColumn("åˆ†é‡", width="small"),
                "é¡ç¾©èª": st.column_config.TextColumn("é¡ç¾©èª", width="medium"),
                "é–¢é€£èª": st.column_config.TextColumn("é–¢é€£èª", width="medium"),
            },
            key="dictionary_editor"
        )

        terms_to_delete = edited_df[edited_df['å‰Šé™¤']]
        if not terms_to_delete.empty:
            if st.button("é¸æŠã—ãŸç”¨èªã‚’å‰Šé™¤", type="primary"):
                terms_list = terms_to_delete['ç”¨èª'].tolist()
                deleted_count, error_count = rag_system.delete_jargon_terms(terms_list)
                if deleted_count:
                    st.success(f"{deleted_count}ä»¶ã®ç”¨èªã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
                if error_count:
                    st.warning(f"{error_count}ä»¶ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                if hasattr(get_all_terms_cached, "clear"):
                    get_all_terms_cached.clear()

    # CSV download
    st.markdown("---")
    with st.expander("âš ï¸ ç”¨èªè¾æ›¸ã‚’å…¨å‰Šé™¤ã™ã‚‹"):
        st.warning("ã“ã®æ“ä½œã¯å–ã‚Šæ¶ˆã›ã¾ã›ã‚“ã€‚å…¨ã¦ã®å°‚é–€ç”¨èªãƒ¬ã‚³ãƒ¼ãƒ‰ãŒå‰Šé™¤ã•ã‚Œã¾ã™ã€‚", icon="âš ï¸")
        if st.button("â€¼ï¸ å…¨ç”¨èªã‚’å‰Šé™¤", type="secondary"):
            deleted_count, error_count = rag_system.delete_jargon_terms(terms_df['term'].tolist())
            if deleted_count:
                st.success(f"{deleted_count}ä»¶ã®ç”¨èªã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
            if error_count:
                st.warning(f"{error_count}ä»¶ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸã€‚", icon="âš ï¸")
            if hasattr(get_all_terms_cached, "clear"):
                get_all_terms_cached.clear()

    csv = terms_df.to_csv(index=False)
    st.download_button(
        label="ğŸ“¥ è¡¨ç¤ºä¸­ã®ç”¨èªã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv,
        file_name=f"jargon_dictionary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        mime="text/csv",
        key="csv_download_button"
    )


def render_term_extraction(rag_system, jargon_manager):
    """ğŸ”§ ç”¨èªæŠ½å‡ºã‚¿ãƒ–"""
    st.markdown("### ğŸ“š ç”¨èªè¾æ›¸ã‚’ç”Ÿæˆ")

    # Check vector store status for current collection
    has_vector_data = check_vector_store_has_data(rag_system, rag_system.config.collection_name)
    if not has_vector_data:
        st.warning(f"âš ï¸ ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{rag_system.config.collection_name}' ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.info("""
ğŸ’¡ **äº‹å‰æº–å‚™ãŒå¿…è¦ã§ã™**:
1. ã€Œ**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**ã€ã‚¿ãƒ–ã§PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»ç™»éŒ²
2. ã“ã®ã‚¿ãƒ–ã«æˆ»ã£ã¦ç”¨èªã‚’ç”Ÿæˆ

å®šç¾©ç”Ÿæˆã¨LLMåˆ¤å®šã‚’æœ‰åŠ¹ã«ã™ã‚‹ã«ã¯ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²ãŒå¿…é ˆã§ã™ã€‚
        """)
    else:
        st.success("âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã™ã€‚ç”¨èªç”Ÿæˆã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚")

    st.markdown("""
**ğŸ“š ç”¨èªè¾æ›¸ç”Ÿæˆã®æµã‚Œ**:
1. PDFã‹ã‚‰å€™è£œç”¨èªã‚’æŠ½å‡º (Sudachiå½¢æ…‹ç´ è§£æ + SemReRank)
2. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã§é¡ä¼¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ â†’ å®šç¾©ç”Ÿæˆ
3. LLMã§å°‚é–€ç”¨èªã‚’åˆ¤å®šãƒ»ãƒ•ã‚£ãƒ«ã‚¿
    """)

    # Input mode selection
    input_mode = st.radio(
        "å…¥åŠ›ã‚½ãƒ¼ã‚¹",
        ("ç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æŠ½å‡º", "æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"),
        horizontal=True,
        key="term_input_mode"
    )

    uploaded_files = None
    input_dir = ""
    if input_mode == "ç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æŠ½å‡º":
        st.info("ç™»éŒ²æ¸ˆã¿ã®å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ç”¨èªã‚’æŠ½å‡ºã—ã¾ã™ã€‚")
        input_dir = "./docs"
    else:
        uploaded_files = st.file_uploader(
            "ç”¨èªæŠ½å‡ºç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (PDFæ¨å¥¨)",
            accept_multiple_files=True,
            type=["pdf", "txt", "md"],
            key="term_input_files"
        )

    output_json = st.text_input(
        "å‡ºåŠ›å…ˆ (JSON)",
        value="./output/terms.json",
        key="term_output_json"
    )

    # ãƒœã‚¿ãƒ³ã®ç„¡åŠ¹åŒ–æ¡ä»¶: ã€Œç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æŠ½å‡ºã€ãƒ¢ãƒ¼ãƒ‰ã‹ã¤ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ãƒ‡ãƒ¼ã‚¿ãªã—
    button_disabled = (input_mode == "ç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æŠ½å‡º" and not has_vector_data)

    if st.button("ğŸš€ ç”¨èªã‚’æŠ½å‡ºãƒ»ç”Ÿæˆ", type="primary", use_container_width=True, key="run_term_extraction", disabled=button_disabled):
        temp_dir_path = None
        try:
            if input_mode == "ç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æŠ½å‡º":
                # Extract text from registered documents in database (current collection only)
                with rag_system.engine.connect() as conn:
                    result = conn.execute(
                        text("""
                            SELECT content
                            FROM document_chunks
                            WHERE collection_name = :cname
                            ORDER BY created_at
                        """),
                        {"cname": rag_system.config.collection_name}
                    )
                    all_chunks = [row[0] for row in result]

                if not all_chunks:
                    st.error(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{rag_system.config.collection_name}' ã«ç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                else:
                    # Create temporary file with all content
                    temp_dir_path = Path(tempfile.mkdtemp(prefix="term_extract_registered_"))
                    temp_file = temp_dir_path / "registered_documents.txt"

                    # Write all chunks to file
                    with open(temp_file, "w", encoding="utf-8") as f:
                        f.write("\n\n".join(all_chunks))

                    input_path = str(temp_dir_path)
                    st.info(f"ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ '{rag_system.config.collection_name}' ã‹ã‚‰ {len(all_chunks)} ãƒãƒ£ãƒ³ã‚¯ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")

                    output_path = Path(output_json)
                    output_path.parent.mkdir(parents=True, exist_ok=True)

                    # WebSocketã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–
                    progress_bar = st.progress(0, text="åˆæœŸåŒ–ä¸­...")
                    status_text = st.empty()

                    import threading
                    import time

                    extraction_complete = threading.Event()

                    def update_progress_periodically():
                        steps = [
                            (10, "ğŸ“Š ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿ï¼†çµ±è¨ˆå‡¦ç†ä¸­..."),
                            (20, "ğŸ” å€™è£œç”¨èªæŠ½å‡ºä¸­..."),
                            (30, "ğŸ“ˆ TF-IDF/C-valueè¨ˆç®—ä¸­..."),
                            (40, "ğŸ¯ SemReRankå‡¦ç†ä¸­..."),
                            (50, "ğŸ“ å®šç¾©ç”Ÿæˆä¸­... (ã“ã‚Œã«ã¯æ•°åˆ†ã‹ã‹ã‚Šã¾ã™)"),
                            (60, "ğŸ“ å®šç¾©ç”Ÿæˆä¸­... (60%)"),
                            (70, "ğŸ“ å®šç¾©ç”Ÿæˆä¸­... (70%)"),
                            (80, "ğŸ”¬ LLMå°‚é–€ç”¨èªåˆ¤å®šä¸­... (80%)"),
                            (90, "ğŸ”¬ LLMå°‚é–€ç”¨èªåˆ¤å®šä¸­... (90%)"),
                            (95, "ğŸ“¦ çµæœã‚’ä¿å­˜ä¸­..."),
                        ]

                        for percent, message in steps:
                            if extraction_complete.is_set():
                                break
                            progress_bar.progress(percent / 100, text=message)
                            status_text.info(f"â³ å‡¦ç†ä¸­: {message}")
                            time.sleep(60)

                    try:
                        progress_thread = threading.Thread(target=update_progress_periodically, daemon=True)
                        progress_thread.start()

                        asyncio.run(rag_system.extract_terms(input_path, str(output_path)))

                        extraction_complete.set()
                        progress_bar.progress(1.0, text="âœ… å®Œäº†ï¼")

                    finally:
                        extraction_complete.set()
                        time.sleep(0.5)
                        progress_bar.empty()
                        status_text.empty()

                    st.session_state['term_extraction_output'] = str(output_path)

                    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•çš„ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²
                    try:
                        import json
                        with open(output_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            terms = data.get('terms', [])

                        if terms:
                            st.info(f"ğŸ“¥ {len(terms)}ä»¶ã®ç”¨èªã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç™»éŒ²ä¸­...")

                            registered_count = 0
                            skipped_count = 0
                            error_count = 0

                            for term_data in terms:
                                try:
                                    if jargon_manager.add_term(
                                        term=term_data.get('headword', ''),
                                        definition=term_data.get('definition', ''),
                                        domain=None,
                                        aliases=term_data.get('synonyms', []),
                                        related_terms=term_data.get('related_terms', [])
                                    ):
                                        registered_count += 1
                                    else:
                                        skipped_count += 1
                                except Exception as e:
                                    error_count += 1
                                    import logging
                                    logging.error(f"Failed to register term '{term_data.get('headword', '')}': {e}")

                            st.success(f"âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç™»éŒ²å®Œäº†: {registered_count}ä»¶ç™»éŒ²ã€{skipped_count}ä»¶ã‚¹ã‚­ãƒƒãƒ—ã€{error_count}ä»¶ã‚¨ãƒ©ãƒ¼")
                        else:
                            st.warning("æŠ½å‡ºã•ã‚ŒãŸç”¨èªãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

                    except Exception as e:
                        st.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç™»éŒ²ã‚¨ãƒ©ãƒ¼: {e}")

                    st.success(f"âœ… ç”¨èªè¾æ›¸ã‚’ç”Ÿæˆã—ã¾ã—ãŸ â†’ {output_path}")
                    if hasattr(get_all_terms_cached, "clear"):
                        get_all_terms_cached.clear()
                    st.rerun()
            else:
                if not uploaded_files:
                    st.error("æŠ½å‡ºã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
                else:
                    temp_dir_path = Path(tempfile.mkdtemp(prefix="term_extract_"))
                    for uploaded in uploaded_files:
                        target = temp_dir_path / uploaded.name
                        with open(target, "wb") as f:
                            f.write(uploaded.getbuffer())
                    input_path = str(temp_dir_path)

                    output_path = Path(output_json)
                    output_path.parent.mkdir(parents=True, exist_ok=True)

                    # Similar progress handling as above
                    # ... (åŒæ§˜ã®å‡¦ç†)

        except Exception as e:
            st.error(f"ç”¨èªæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            st.code(traceback.format_exc())
        finally:
            if temp_dir_path and temp_dir_path.exists():
                shutil.rmtree(temp_dir_path, ignore_errors=True)

    # ç”¨èªæŠ½å‡ºçµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    output_file = st.session_state.get('term_extraction_output', '')
    if output_file and Path(output_file).exists():
        st.markdown("---")
        with st.expander("ğŸ“Š æŠ½å‡ºçµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=False):
            import json
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    terms = data.get('terms', [])

                st.success(f"âœ… {len(terms)}ä»¶ã®ç”¨èªã‚’æŠ½å‡ºã—ã¾ã—ãŸ")

                for i, term in enumerate(terms[:10], 1):
                    with st.container():
                        col1, col2 = st.columns([3, 1])
                        with col1:
                            st.markdown(f"**{i}. {term['headword']}**")
                            if term.get('definition'):
                                st.caption(term['definition'][:100] + "..." if len(term['definition']) > 100 else term['definition'])
                        with col2:
                            st.metric("ã‚¹ã‚³ã‚¢", f"{term.get('score', 0):.3f}")
                            st.caption(f"é »åº¦: {term.get('frequency', 0)}")

            except Exception as e:
                st.error(f"çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")


def render_term_analysis():
    """ğŸ“Š æŠ½å‡ºåˆ†æã‚¿ãƒ–"""
    st.subheader("ğŸ“Š å°‚é–€ç”¨èªæŠ½å‡ºã®ç‰¹å¾´åˆ†æ")
    st.caption("Ground Truthã¨ã®æ¯”è¼ƒã«ã‚ˆã‚Šã€TF-IDF+C-valueã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æœ‰åŠ¹æ€§ã‚’æ¤œè¨¼ã—ã¾ã™")

    st.info("""
**ã“ã®åˆ†æã§ã¯ä»¥ä¸‹ã‚’ç¢ºèªã§ãã¾ã™:**
- ã‚«ãƒ†ã‚´ãƒªåˆ¥Recallï¼ˆã©ã®ã‚¿ã‚¤ãƒ—ã®ç”¨èªãŒæŠ½å‡ºã•ã‚Œã¦ã„ã‚‹ã‹ï¼‰
- é »åº¦åˆ¥Recallï¼ˆä½é »åº¦ç”¨èªã¯è¦‹é€ƒã•ã‚Œã¦ã„ãªã„ã‹ï¼‰
- TF-IDF/C-valueã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒ
- è¦‹é€ƒã•ã‚ŒãŸç”¨èªï¼ˆFalse Negativesï¼‰
- èª¤æ¤œå‡ºã•ã‚ŒãŸç”¨èªï¼ˆFalse Positivesï¼‰
    """)

    # 1. Ground Truth ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    ground_truth_file = st.file_uploader(
        "Ground Truth JSON",
        type=['json'],
        help="æ­£è§£ãƒ‡ãƒ¼ã‚¿ (ä¾‹: test_data/ground_truth.json)",
        key="gt_upload"
    )

    # 2. å€™è£œç”¨èªï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã®è‡ªå‹•æ¤œå‡º
    candidates_path = None
    candidates_file_obj = None

    # 2-1. ã‚»ãƒƒã‚·ãƒ§ãƒ³å¤‰æ•°ã‹ã‚‰æŠ½å‡ºçµæœã®å ´æ‰€ã‚’æ¨æ¸¬
    if 'term_extraction_output' in st.session_state:
        output_path = Path(st.session_state['term_extraction_output'])
        debug_path = output_path.parent / "term_extraction_debug.json"
        if debug_path.exists():
            candidates_path = debug_path
            st.success(f"âœ… å€™è£œç”¨èªãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•æ¤œå‡º: {candidates_path}")
        else:
            candidates_path = None

    # 2-2. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¹ã‹ã‚‰æ¤œå‡º
    if not candidates_path and Path("./output/term_extraction_debug.json").exists():
        candidates_path = Path("./output/term_extraction_debug.json")
        st.success(f"âœ… å€™è£œç”¨èªãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•æ¤œå‡º: {candidates_path}")

    # 2-3. ã©ã¡ã‚‰ã‚‚ãªã„å ´åˆ
    if not candidates_path:
        st.warning("âš ï¸ å€™è£œç”¨èªãƒ‡ãƒ¼ã‚¿ï¼ˆterm_extraction_debug.jsonï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        st.info("å…ˆã«ã€Œç”¨èªæŠ½å‡ºã€ã‚¿ãƒ–ã§æŠ½å‡ºã‚’å®Ÿè¡Œã™ã‚‹ã‹ã€æ‰‹å‹•ã§ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")

    # 2-4. æ‰‹å‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆä»»æ„ or å¿…é ˆï¼‰
    if candidates_path:
        st.caption("åˆ¥ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ã†å ´åˆã¯ä¸‹è¨˜ã‹ã‚‰ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰â†“")
        label = "åˆ¥ã®å€™è£œç”¨èªJSONã‚’ä½¿ã†ï¼ˆä»»æ„ï¼‰"
    else:
        label = "å€™è£œç”¨èª JSONï¼ˆå¿…é ˆï¼‰"

    manual_candidates = st.file_uploader(
        label,
        type=['json'],
        help="å€™è£œç”¨èªãƒ‡ãƒ¼ã‚¿ (ä¾‹: output/term_extraction_debug.json)",
        key="candidates_upload"
    )

    # æ‰‹å‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒã‚ã‚Œã°ãã¡ã‚‰ã‚’å„ªå…ˆ
    if manual_candidates:
        candidates_file_obj = manual_candidates
        st.info("âœ… æ‰‹å‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
    elif candidates_path:
        # è‡ªå‹•æ¤œå‡ºã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
        pass
    else:
        candidates_file_obj = None

    # 3. åˆ†æå®Ÿè¡Œãƒœã‚¿ãƒ³ã®æœ‰åŠ¹/ç„¡åŠ¹
    can_analyze = ground_truth_file and (candidates_path or candidates_file_obj)

    if not can_analyze:
        missing = []
        if not ground_truth_file:
            missing.append("Ground Truth JSON")
        if not (candidates_path or candidates_file_obj):
            missing.append("å€™è£œç”¨èª JSON")
        st.warning(f"âš ï¸ ä¸è¶³: {', '.join(missing)}")

    # 4. åˆ†æå®Ÿè¡Œ
    if st.button("ğŸ” åˆ†æã‚’å®Ÿè¡Œ", type="primary", use_container_width=True, disabled=not can_analyze):
        with st.spinner("åˆ†æä¸­..."):
            try:
                import json
                from src.rag.term_analysis import TermFeatureAnalyzer

                # Ground Truthèª­ã¿è¾¼ã¿
                ground_truth = json.load(ground_truth_file)

                # å€™è£œç”¨èªãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
                if candidates_file_obj:
                    candidates_data = json.load(candidates_file_obj)
                else:
                    with open(candidates_path, 'r', encoding='utf-8') as f:
                        candidates_data = json.load(f)

                # å€™è£œç”¨èªãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆè¤‡æ•°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆå¯¾å¿œï¼‰
                candidate_terms = (
                    candidates_data.get('candidates') or
                    candidates_data.get('terms') or
                    (candidates_data if isinstance(candidates_data, list) else [])
                )

                st.info(f"ğŸ“Š å€™è£œç”¨èªæ•°: {len(candidate_terms)}ä»¶")

                # åˆ†æå®Ÿè¡Œï¼ˆdocumentsã¯ç©ºãƒªã‚¹ãƒˆï¼‰
                analyzer = TermFeatureAnalyzer(ground_truth, candidate_terms, [])
                results = analyzer.analyze()

                # çµæœè¡¨ç¤º
                st.success("âœ… åˆ†æå®Œäº†")

                # 1. æ¦‚è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                metrics = results['overall_metrics']
                col1, col2, col3, col4 = st.columns(4)
                col1.metric("Recall", f"{metrics['recall']:.1%}")
                col2.metric("Precision", f"{metrics['precision']:.1%}")
                col3.metric("F1 Score", f"{metrics['f1_score']:.1%}")
                col4.metric("Ground Truth", metrics['total_ground_truth'])

                st.markdown("---")

                # 2. ã‚«ãƒ†ã‚´ãƒªåˆ¥Recall
                st.subheader("ğŸ“ˆ ã‚«ãƒ†ã‚´ãƒªåˆ¥Recall")
                category_analysis = results['category_analysis']

                category_df = pd.DataFrame([
                    {
                        'ã‚«ãƒ†ã‚´ãƒª': cat,
                        'Ground Truthæ•°': data['total'],
                        'æŠ½å‡ºæ•°': data['extracted'],
                        'Recall': f"{data['recall']:.1%}"
                    }
                    for cat, data in sorted(category_analysis.items(), key=lambda x: x[1]['recall'], reverse=True)
                ])
                st.dataframe(category_df, use_container_width=True, hide_index=True)

                # 3. é »åº¦åˆ¥Recall
                st.markdown("---")
                st.subheader("ğŸ“Š é »åº¦åˆ¥Recall")
                freq_analysis = results['frequency_analysis']

                freq_df = pd.DataFrame([
                    {
                        'é »åº¦ç¯„å›²': label,
                        'Ground Truthæ•°': data['total'],
                        'æŠ½å‡ºæ•°': data['extracted'],
                        'Recall': f"{data['recall']:.1%}"
                    }
                    for label, data in freq_analysis.items()
                ])
                st.dataframe(freq_df, use_container_width=True, hide_index=True)

                # 4. è¦‹é€ƒã•ã‚ŒãŸç”¨èª
                st.markdown("---")
                with st.expander("âŒ è¦‹é€ƒã•ã‚ŒãŸç”¨èª (False Negatives)", expanded=False):
                    missed_terms = results['missed_terms'][:30]
                    missed_df = pd.DataFrame(missed_terms)
                    st.dataframe(missed_df, use_container_width=True, hide_index=True)

                # 5. èª¤æ¤œå‡ºã•ã‚ŒãŸç”¨èª
                with st.expander("âš ï¸ èª¤æ¤œå‡ºã•ã‚ŒãŸç”¨èª (False Positives)", expanded=False):
                    false_positives = results['false_positives'][:30]
                    fp_df = pd.DataFrame(false_positives)
                    st.dataframe(fp_df, use_container_width=True, hide_index=True)

                # 6. SemReRankã‚¹ã‚³ã‚¢æ”¹å–„åˆ†æ
                if 'semrerank_impact' in results and results['semrerank_impact']['all_changes']:
                    st.markdown("---")
                    st.subheader("ğŸ”„ SemReRankã‚¹ã‚³ã‚¢æ”¹å–„åˆ†æ")

                    impact = results['semrerank_impact']
                    freq_impact = impact['frequency_impact']

                    # é »åº¦åˆ¥ã®ã‚¹ã‚³ã‚¢å‘ä¸Šç‡
                    impact_df = pd.DataFrame([
                        {
                            'é »åº¦ç¯„å›²': label,
                            'å¯¾è±¡ç”¨èªæ•°': data['count'],
                            'å¹³å‡ã‚¹ã‚³ã‚¢å‘ä¸Šç‡': f"{(data['mean_ratio'] - 1) * 100:.1f}%",
                            'ä¸­å¤®å€¤ã‚¹ã‚³ã‚¢å‘ä¸Šç‡': f"{(data['median_ratio'] - 1) * 100:.1f}%"
                        }
                        for label, data in freq_impact.items()
                        if data['count'] > 0
                    ])
                    st.dataframe(impact_df, use_container_width=True, hide_index=True)

                    st.caption("ğŸ’¡ ä½é »åº¦ç”¨èªã»ã©SemReRankã®æ©æµã‚’å—ã‘ã‚„ã™ã„å‚¾å‘ãŒã‚ã‚Šã¾ã™")

                    # Ground Truthç”¨èªã®é »åº¦åˆ†å¸ƒ
                    if 'gt_frequencies' in impact and impact['gt_frequencies']:
                        st.markdown("#### ğŸ“Š Ground Truthç”¨èªã®é »åº¦åˆ†å¸ƒ")
                        gt_freq_dist = impact['gt_freq_distribution']

                        # é »åº¦åˆ†å¸ƒãƒ†ãƒ¼ãƒ–ãƒ«
                        gt_dist_df = pd.DataFrame([
                            {
                                'é »åº¦ç¯„å›²': label,
                                'ç”¨èªæ•°': count,
                                'å‰²åˆ': f"{count / sum(gt_freq_dist.values()) * 100:.1f}%" if sum(gt_freq_dist.values()) > 0 else "0%"
                            }
                            for label, count in gt_freq_dist.items()
                            if count > 0
                        ])
                        st.dataframe(gt_dist_df, use_container_width=True, hide_index=True)

                        st.caption(f"ğŸ’¡ åˆè¨ˆ {len(impact['gt_frequencies'])} ä»¶ã®æ­£è§£ç”¨èªãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")

                    # ã‚¹ã‚³ã‚¢åˆ†å¸ƒã®å¯è¦–åŒ–
                    with st.expander("ğŸ“Š ã‚¹ã‚³ã‚¢åˆ†å¸ƒã®è©³ç´°", expanded=False):
                        all_changes = impact['all_changes']

                        # Before/Afteræ•£å¸ƒå›³
                        import matplotlib.pyplot as plt
                        import matplotlib
                        matplotlib.use('Agg')  # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰è¨­å®š

                        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
                        import platform
                        if platform.system() == 'Windows':
                            plt.rcParams['font.family'] = 'Yu Gothic'
                        elif platform.system() == 'Darwin':  # macOS
                            plt.rcParams['font.family'] = 'Hiragino Sans'
                        else:  # Linux
                            plt.rcParams['font.family'] = 'Noto Sans CJK JP'
                        plt.rcParams['axes.unicode_minus'] = False  # ãƒã‚¤ãƒŠã‚¹è¨˜å·ã®æ–‡å­—åŒ–ã‘å¯¾ç­–

                        # 3ã¤ã®ã‚°ãƒ©ãƒ•ã‚’é…ç½®
                        fig = plt.figure(figsize=(15, 10))
                        gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)
                        ax1 = fig.add_subplot(gs[0, 0])
                        ax2 = fig.add_subplot(gs[0, 1])
                        ax3 = fig.add_subplot(gs[1, :])

                        # å·¦ä¸Š: Base Score vs Revised Score
                        base_scores = [x['base_score'] for x in all_changes]
                        revised_scores = [x['revised_score'] for x in all_changes]

                        ax1.scatter(base_scores, revised_scores, alpha=0.6)
                        max_score = max(max(base_scores), max(revised_scores))
                        ax1.plot([0, max_score], [0, max_score], 'r--', label='y=x', linewidth=1)
                        ax1.set_xlabel('æ­£è¦åŒ–ã‚¹ã‚³ã‚¢ (Before)')
                        ax1.set_ylabel('æ­£è¦åŒ–ã‚¹ã‚³ã‚¢ (After)')
                        ax1.set_title('SemReRankã«ã‚ˆã‚‹ã‚¹ã‚³ã‚¢å¤‰åŒ–')
                        ax1.legend()
                        ax1.grid(True, alpha=0.3)

                        # å³ä¸Š: é »åº¦åˆ¥ã‚¹ã‚³ã‚¢å‘ä¸Šç‡
                        freq_labels = [label for label, data in freq_impact.items() if data['count'] > 0]
                        mean_ratios = [(freq_impact[label]['mean_ratio'] - 1) * 100
                                       for label in freq_labels]

                        ax2.bar(freq_labels, mean_ratios, color='steelblue', alpha=0.7)
                        ax2.set_xlabel('å‡ºç¾é »åº¦')
                        ax2.set_ylabel('å¹³å‡ã‚¹ã‚³ã‚¢å‘ä¸Šç‡ (%)')
                        ax2.set_title('é »åº¦åˆ¥ã‚¹ã‚³ã‚¢å‘ä¸Šç‡')
                        ax2.grid(True, axis='y', alpha=0.3)
                        ax2.axhline(y=0, color='red', linestyle='--', linewidth=1)

                        # ä¸‹æ®µ: ã‚¹ã‚³ã‚¢åˆ†å¸ƒãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼ˆBefore/Afteré‡ã­åˆã‚ã›ï¼‰
                        ax3.hist(base_scores, bins=30, alpha=0.5, label='é©ç”¨å‰', color='orange', edgecolor='black')
                        ax3.hist(revised_scores, bins=30, alpha=0.5, label='é©ç”¨å¾Œ', color='blue', edgecolor='black')
                        ax3.set_xlabel('æ­£è¦åŒ–ã‚¹ã‚³ã‚¢ (0-1)')
                        ax3.set_ylabel('ç”¨èªæ•°')
                        ax3.set_title('ã‚¹ã‚³ã‚¢åˆ†å¸ƒ: SemReRanké©ç”¨å‰å¾Œ')
                        ax3.legend()
                        ax3.grid(True, alpha=0.3, axis='y')

                        plt.tight_layout()
                        st.pyplot(fig)
                        plt.close(fig)

                    # Ground Truthç”¨èªã®é »åº¦ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ 
                    if 'gt_frequencies' in impact and impact['gt_frequencies']:
                        with st.expander("ğŸ“ˆ Ground Truthç”¨èªã®é »åº¦ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ", expanded=False):
                            # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
                            import platform
                            if platform.system() == 'Windows':
                                plt.rcParams['font.family'] = 'Yu Gothic'
                            elif platform.system() == 'Darwin':  # macOS
                                plt.rcParams['font.family'] = 'Hiragino Sans'
                            else:  # Linux
                                plt.rcParams['font.family'] = 'Noto Sans CJK JP'
                            plt.rcParams['axes.unicode_minus'] = False

                            fig, ax = plt.subplots(figsize=(10, 5))

                            gt_freqs = impact['gt_frequencies']
                            ax.hist(gt_freqs, bins=range(1, max(gt_freqs) + 2), alpha=0.7, color='green', edgecolor='black')
                            ax.set_xlabel('å‡ºç¾é »åº¦')
                            ax.set_ylabel('ç”¨èªæ•°')
                            ax.set_title('Ground Truthç”¨èªã®é »åº¦åˆ†å¸ƒ')
                            ax.grid(True, alpha=0.3, axis='y')

                            # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
                            mean_freq = sum(gt_freqs) / len(gt_freqs)
                            median_freq = sorted(gt_freqs)[len(gt_freqs) // 2]
                            ax.axvline(mean_freq, color='red', linestyle='--', linewidth=2, label=f'å¹³å‡: {mean_freq:.1f}')
                            ax.axvline(median_freq, color='blue', linestyle='--', linewidth=2, label=f'ä¸­å¤®å€¤: {median_freq}')
                            ax.legend()

                            plt.tight_layout()
                            st.pyplot(fig)
                            plt.close(fig)

                # 6.5. ã‚¹ã‚³ã‚¢åˆ†å¸ƒãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼ˆå…¨å€™è£œç”¨èªï¼‰
                st.markdown("---")
                st.subheader("ğŸ“Š å€™è£œç”¨èªã‚¹ã‚³ã‚¢åˆ†å¸ƒ")
                st.caption("TF-IDFã€C-valueã€ç·åˆã‚¹ã‚³ã‚¢ã®åˆ†å¸ƒã‚’å¯è¦–åŒ–")

                # å€™è£œç”¨èªãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º
                tfidf_scores = [t.get('tfidf_score', 0) for t in candidate_terms if t.get('tfidf_score', 0) > 0]
                cvalue_scores = [t.get('cvalue_score', 0) for t in candidate_terms if t.get('cvalue_score', 0) > 0]
                base_scores_all = [t.get('base_score', 0) for t in candidate_terms if t.get('base_score', 0) > 0]
                revised_scores_all = [t.get('revised_score', 0) for t in candidate_terms if t.get('revised_score', 0) > 0]

                if tfidf_scores or cvalue_scores or base_scores_all:
                    import matplotlib.pyplot as plt
                    import matplotlib
                    matplotlib.use('Agg')

                    # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
                    import platform
                    if platform.system() == 'Windows':
                        plt.rcParams['font.family'] = 'Yu Gothic'
                    elif platform.system() == 'Darwin':
                        plt.rcParams['font.family'] = 'Hiragino Sans'
                    else:
                        plt.rcParams['font.family'] = 'Noto Sans CJK JP'
                    plt.rcParams['axes.unicode_minus'] = False

                    # 2x2ã‚°ãƒªãƒƒãƒ‰ã§ã‚¹ã‚³ã‚¢åˆ†å¸ƒã‚’è¡¨ç¤º
                    fig, axes = plt.subplots(2, 2, figsize=(14, 10))

                    # TF-IDFã‚¹ã‚³ã‚¢åˆ†å¸ƒ
                    if tfidf_scores:
                        axes[0, 0].hist(tfidf_scores, bins=30, alpha=0.7, color='steelblue', edgecolor='black')
                        axes[0, 0].set_xlabel('TF-IDFã‚¹ã‚³ã‚¢')
                        axes[0, 0].set_ylabel('ç”¨èªæ•°')
                        axes[0, 0].set_title(f'TF-IDFã‚¹ã‚³ã‚¢åˆ†å¸ƒ (n={len(tfidf_scores)})')
                        axes[0, 0].grid(True, alpha=0.3, axis='y')
                        # çµ±è¨ˆæƒ…å ±
                        mean_tfidf = sum(tfidf_scores) / len(tfidf_scores)
                        axes[0, 0].axvline(mean_tfidf, color='red', linestyle='--', linewidth=2, label=f'å¹³å‡: {mean_tfidf:.2f}')
                        axes[0, 0].legend()

                    # C-valueã‚¹ã‚³ã‚¢åˆ†å¸ƒ
                    if cvalue_scores:
                        axes[0, 1].hist(cvalue_scores, bins=30, alpha=0.7, color='green', edgecolor='black')
                        axes[0, 1].set_xlabel('C-valueã‚¹ã‚³ã‚¢')
                        axes[0, 1].set_ylabel('ç”¨èªæ•°')
                        axes[0, 1].set_title(f'C-valueã‚¹ã‚³ã‚¢åˆ†å¸ƒ (n={len(cvalue_scores)})')
                        axes[0, 1].grid(True, alpha=0.3, axis='y')
                        # çµ±è¨ˆæƒ…å ±
                        mean_cvalue = sum(cvalue_scores) / len(cvalue_scores)
                        axes[0, 1].axvline(mean_cvalue, color='red', linestyle='--', linewidth=2, label=f'å¹³å‡: {mean_cvalue:.2f}')
                        axes[0, 1].legend()

                    # Base Scoreåˆ†å¸ƒï¼ˆæ­£è¦åŒ–å‰ï¼‰
                    if base_scores_all:
                        axes[1, 0].hist(base_scores_all, bins=30, alpha=0.7, color='orange', edgecolor='black')
                        axes[1, 0].set_xlabel('Base Score')
                        axes[1, 0].set_ylabel('ç”¨èªæ•°')
                        axes[1, 0].set_title(f'Base Scoreåˆ†å¸ƒ (n={len(base_scores_all)})')
                        axes[1, 0].grid(True, alpha=0.3, axis='y')
                        # çµ±è¨ˆæƒ…å ±
                        mean_base = sum(base_scores_all) / len(base_scores_all)
                        axes[1, 0].axvline(mean_base, color='red', linestyle='--', linewidth=2, label=f'å¹³å‡: {mean_base:.2f}')
                        axes[1, 0].legend()

                    # Revised Scoreåˆ†å¸ƒï¼ˆSemReRanké©ç”¨å¾Œï¼‰
                    if revised_scores_all:
                        axes[1, 1].hist(revised_scores_all, bins=30, alpha=0.7, color='purple', edgecolor='black')
                        axes[1, 1].set_xlabel('Revised Score')
                        axes[1, 1].set_ylabel('ç”¨èªæ•°')
                        axes[1, 1].set_title(f'Revised Scoreåˆ†å¸ƒ (SemReRanké©ç”¨å¾Œ, n={len(revised_scores_all)})')
                        axes[1, 1].grid(True, alpha=0.3, axis='y')
                        # çµ±è¨ˆæƒ…å ±
                        mean_revised = sum(revised_scores_all) / len(revised_scores_all)
                        axes[1, 1].axvline(mean_revised, color='red', linestyle='--', linewidth=2, label=f'å¹³å‡: {mean_revised:.2f}')
                        axes[1, 1].legend()

                    plt.tight_layout()
                    st.pyplot(fig)
                    plt.close(fig)

                    # Ground Truth vs å…¨å€™è£œã®æ¯”è¼ƒ
                    with st.expander("ğŸ“Š Ground Truth vs å…¨å€™è£œã®æ¯”è¼ƒ", expanded=False):
                        st.caption("Ground Truthç”¨èªã¨å…¨å€™è£œç”¨èªã®ã‚¹ã‚³ã‚¢åˆ†å¸ƒã‚’æ¯”è¼ƒ")

                        # Ground Truthç”¨èªã®ã‚¹ã‚³ã‚¢ã‚’æŠ½å‡º
                        gt_terms_set = set(ground_truth.get("all_documents", []))
                        if not gt_terms_set:
                            # all_documentsã‚­ãƒ¼ãŒãªã„å ´åˆã¯ã€å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ç”¨èªã‚’çµ±åˆ
                            for key, doc_terms in ground_truth.items():
                                if isinstance(doc_terms, list):
                                    gt_terms_set.update(doc_terms)

                        gt_tfidf = []
                        gt_cvalue = []
                        gt_base = []
                        gt_revised = []

                        for term_data in candidate_terms:
                            term_name = term_data.get('term') or term_data.get('headword')
                            if term_name in gt_terms_set:
                                if term_data.get('tfidf_score', 0) > 0:
                                    gt_tfidf.append(term_data['tfidf_score'])
                                if term_data.get('cvalue_score', 0) > 0:
                                    gt_cvalue.append(term_data['cvalue_score'])
                                if term_data.get('base_score', 0) > 0:
                                    gt_base.append(term_data['base_score'])
                                if term_data.get('revised_score', 0) > 0:
                                    gt_revised.append(term_data['revised_score'])

                        if gt_tfidf or gt_cvalue or gt_base:
                            fig2, axes2 = plt.subplots(2, 2, figsize=(14, 10))

                            # TF-IDFæ¯”è¼ƒ
                            if tfidf_scores and gt_tfidf:
                                axes2[0, 0].hist(tfidf_scores, bins=30, alpha=0.5, label='å…¨å€™è£œ', color='gray', edgecolor='black')
                                axes2[0, 0].hist(gt_tfidf, bins=30, alpha=0.7, label='Ground Truth', color='blue', edgecolor='black')
                                axes2[0, 0].set_xlabel('TF-IDFã‚¹ã‚³ã‚¢')
                                axes2[0, 0].set_ylabel('ç”¨èªæ•°')
                                axes2[0, 0].set_title('TF-IDFã‚¹ã‚³ã‚¢: Ground Truth vs å…¨å€™è£œ')
                                axes2[0, 0].legend()
                                axes2[0, 0].grid(True, alpha=0.3, axis='y')

                            # C-valueæ¯”è¼ƒ
                            if cvalue_scores and gt_cvalue:
                                axes2[0, 1].hist(cvalue_scores, bins=30, alpha=0.5, label='å…¨å€™è£œ', color='gray', edgecolor='black')
                                axes2[0, 1].hist(gt_cvalue, bins=30, alpha=0.7, label='Ground Truth', color='green', edgecolor='black')
                                axes2[0, 1].set_xlabel('C-valueã‚¹ã‚³ã‚¢')
                                axes2[0, 1].set_ylabel('ç”¨èªæ•°')
                                axes2[0, 1].set_title('C-valueã‚¹ã‚³ã‚¢: Ground Truth vs å…¨å€™è£œ')
                                axes2[0, 1].legend()
                                axes2[0, 1].grid(True, alpha=0.3, axis='y')

                            # Base Scoreæ¯”è¼ƒ
                            if base_scores_all and gt_base:
                                axes2[1, 0].hist(base_scores_all, bins=30, alpha=0.5, label='å…¨å€™è£œ', color='gray', edgecolor='black')
                                axes2[1, 0].hist(gt_base, bins=30, alpha=0.7, label='Ground Truth', color='orange', edgecolor='black')
                                axes2[1, 0].set_xlabel('Base Score')
                                axes2[1, 0].set_ylabel('ç”¨èªæ•°')
                                axes2[1, 0].set_title('Base Score: Ground Truth vs å…¨å€™è£œ')
                                axes2[1, 0].legend()
                                axes2[1, 0].grid(True, alpha=0.3, axis='y')

                            # Revised Scoreæ¯”è¼ƒ
                            if revised_scores_all and gt_revised:
                                axes2[1, 1].hist(revised_scores_all, bins=30, alpha=0.5, label='å…¨å€™è£œ', color='gray', edgecolor='black')
                                axes2[1, 1].hist(gt_revised, bins=30, alpha=0.7, label='Ground Truth', color='purple', edgecolor='black')
                                axes2[1, 1].set_xlabel('Revised Score')
                                axes2[1, 1].set_ylabel('ç”¨èªæ•°')
                                axes2[1, 1].set_title('Revised Score: Ground Truth vs å…¨å€™è£œ')
                                axes2[1, 1].legend()
                                axes2[1, 1].grid(True, alpha=0.3, axis='y')

                            plt.tight_layout()
                            st.pyplot(fig2)
                            plt.close(fig2)

                            # çµ±è¨ˆæƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«
                            st.markdown("#### ğŸ“ˆ çµ±è¨ˆæ¯”è¼ƒ")
                            stats_data = []

                            if gt_tfidf and tfidf_scores:
                                stats_data.append({
                                    "ã‚¹ã‚³ã‚¢ç¨®åˆ¥": "TF-IDF",
                                    "GTå¹³å‡": f"{sum(gt_tfidf)/len(gt_tfidf):.3f}",
                                    "å…¨ä½“å¹³å‡": f"{sum(tfidf_scores)/len(tfidf_scores):.3f}",
                                    "GTä¸­å¤®å€¤": f"{sorted(gt_tfidf)[len(gt_tfidf)//2]:.3f}",
                                    "å…¨ä½“ä¸­å¤®å€¤": f"{sorted(tfidf_scores)[len(tfidf_scores)//2]:.3f}"
                                })

                            if gt_cvalue and cvalue_scores:
                                stats_data.append({
                                    "ã‚¹ã‚³ã‚¢ç¨®åˆ¥": "C-value",
                                    "GTå¹³å‡": f"{sum(gt_cvalue)/len(gt_cvalue):.3f}",
                                    "å…¨ä½“å¹³å‡": f"{sum(cvalue_scores)/len(cvalue_scores):.3f}",
                                    "GTä¸­å¤®å€¤": f"{sorted(gt_cvalue)[len(gt_cvalue)//2]:.3f}",
                                    "å…¨ä½“ä¸­å¤®å€¤": f"{sorted(cvalue_scores)[len(cvalue_scores)//2]:.3f}"
                                })

                            if gt_base and base_scores_all:
                                stats_data.append({
                                    "ã‚¹ã‚³ã‚¢ç¨®åˆ¥": "Base Score",
                                    "GTå¹³å‡": f"{sum(gt_base)/len(gt_base):.3f}",
                                    "å…¨ä½“å¹³å‡": f"{sum(base_scores_all)/len(base_scores_all):.3f}",
                                    "GTä¸­å¤®å€¤": f"{sorted(gt_base)[len(gt_base)//2]:.3f}",
                                    "å…¨ä½“ä¸­å¤®å€¤": f"{sorted(base_scores_all)[len(base_scores_all)//2]:.3f}"
                                })

                            if gt_revised and revised_scores_all:
                                stats_data.append({
                                    "ã‚¹ã‚³ã‚¢ç¨®åˆ¥": "Revised Score",
                                    "GTå¹³å‡": f"{sum(gt_revised)/len(gt_revised):.3f}",
                                    "å…¨ä½“å¹³å‡": f"{sum(revised_scores_all)/len(revised_scores_all):.3f}",
                                    "GTä¸­å¤®å€¤": f"{sorted(gt_revised)[len(gt_revised)//2]:.3f}",
                                    "å…¨ä½“ä¸­å¤®å€¤": f"{sorted(revised_scores_all)[len(revised_scores_all)//2]:.3f}"
                                })

                            if stats_data:
                                stats_df = pd.DataFrame(stats_data)
                                st.dataframe(stats_df, use_container_width=True, hide_index=True)
                                st.caption("ğŸ’¡ Ground Truthç”¨èªã®å¹³å‡ã‚¹ã‚³ã‚¢ãŒå…¨ä½“ã‚ˆã‚Šé«˜ã„å ´åˆã€ãã®ã‚¹ã‚³ã‚¢ã¯å°‚é–€ç”¨èªæŠ½å‡ºã«æœ‰åŠ¹")

                        else:
                            st.info("Ground Truthç”¨èªãŒå€™è£œç”¨èªãƒ‡ãƒ¼ã‚¿ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

                else:
                    st.info("å€™è£œç”¨èªãƒ‡ãƒ¼ã‚¿ã«ã‚¹ã‚³ã‚¢æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")

                # 7. Ground Truthè¿½è·¡åˆ†æï¼ˆdropout_report.jsonãŒã‚ã‚‹å ´åˆï¼‰
                dropout_report_path = Path("output").glob("dropout_report_*.json")
                dropout_report_files = sorted(dropout_report_path, key=lambda p: p.stat().st_mtime, reverse=True)

                if dropout_report_files:
                    st.markdown("---")
                    st.subheader("ğŸ“Š Ground Truthè¿½è·¡ãƒ¬ãƒãƒ¼ãƒˆ")
                    st.caption("å„ç”¨èªãŒæŠ½å‡ºãƒ—ãƒ­ã‚»ã‚¹ã®ã©ã®æ®µéšã§è„±è½ã—ãŸã‹ã‚’åˆ†æ")

                    # æœ€æ–°ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿
                    latest_dropout_report = dropout_report_files[0]

                    try:
                        with open(latest_dropout_report, 'r', encoding='utf-8') as f:
                            dropout_data = json.load(f)

                        summary = dropout_data.get("summary", {})
                        dropout_by_stage = dropout_data.get("dropout_by_stage", {})
                        extraction_funnel = dropout_data.get("extraction_funnel", [])
                        missed_terms = dropout_data.get("missed_terms", [])

                        # ã‚µãƒãƒªãƒ¼ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                        col1, col2, col3 = st.columns(3)
                        col1.metric("æŠ½å‡ºæˆåŠŸ", f"{summary.get('extracted', 0)}ä»¶")
                        col2.metric("è„±è½", f"{summary.get('missed', 0)}ä»¶")
                        col3.metric("Recall", f"{summary.get('recall', 0):.1%}")

                        # æŠ½å‡ºãƒ•ã‚¡ãƒãƒ«ï¼ˆæ®µéšåˆ¥æ®‹å­˜æ•°ï¼‰ã®å¯è¦–åŒ–
                        if extraction_funnel:
                            st.markdown("#### ğŸ“‰ æŠ½å‡ºãƒ•ã‚¡ãƒãƒ«ï¼ˆæ®µéšåˆ¥æ®‹å­˜æ•°ï¼‰")

                            import matplotlib.pyplot as plt
                            import matplotlib
                            matplotlib.use('Agg')

                            # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
                            import platform
                            if platform.system() == 'Windows':
                                plt.rcParams['font.family'] = 'Yu Gothic'
                            elif platform.system() == 'Darwin':
                                plt.rcParams['font.family'] = 'Hiragino Sans'
                            else:
                                plt.rcParams['font.family'] = 'Noto Sans CJK JP'
                            plt.rcParams['axes.unicode_minus'] = False

                            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

                            # å·¦: æ®‹å­˜æ•°ã®æ¨ç§»ï¼ˆæŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•ï¼‰
                            stages = [entry['stage'] for entry in extraction_funnel]
                            remaining = [entry['remaining'] for entry in extraction_funnel]

                            ax1.plot(stages, remaining, marker='o', linewidth=2, markersize=8, color='steelblue')
                            ax1.set_xlabel('æŠ½å‡ºæ®µéš')
                            ax1.set_ylabel('æ®‹å­˜ç”¨èªæ•°')
                            ax1.set_title('æŠ½å‡ºãƒ•ã‚¡ãƒãƒ«: Ground Truthç”¨èªã®æ®‹å­˜æ•°')
                            ax1.grid(True, alpha=0.3)
                            ax1.tick_params(axis='x', rotation=45)

                            # å³: æ®µéšåˆ¥è„±è½æ•°ï¼ˆæ£’ã‚°ãƒ©ãƒ•ï¼‰
                            dropout_counts = [entry['dropout'] for entry in extraction_funnel]
                            colors = ['red' if d > 0 else 'lightgray' for d in dropout_counts]

                            ax2.bar(stages, dropout_counts, color=colors, alpha=0.7)
                            ax2.set_xlabel('æŠ½å‡ºæ®µéš')
                            ax2.set_ylabel('è„±è½ç”¨èªæ•°')
                            ax2.set_title('æ®µéšåˆ¥è„±è½æ•°')
                            ax2.grid(True, alpha=0.3, axis='y')
                            ax2.tick_params(axis='x', rotation=45)

                            plt.tight_layout()
                            st.pyplot(fig)
                            plt.close(fig)

                        # æ®µéšåˆ¥è„±è½è©³ç´°
                        if dropout_by_stage:
                            st.markdown("#### ğŸ“‹ æ®µéšåˆ¥è„±è½è©³ç´°")

                            dropout_df = pd.DataFrame([
                                {
                                    'æ®µéš': stage,
                                    'è„±è½æ•°': count,
                                    'å‰²åˆ': f"{count / summary['missed'] * 100:.1f}%" if summary['missed'] > 0 else "0%"
                                }
                                for stage, count in sorted(dropout_by_stage.items(), key=lambda x: x[1], reverse=True)
                                if count > 0
                            ])
                            st.dataframe(dropout_df, use_container_width=True, hide_index=True)

                        # è„±è½ã—ãŸç”¨èªã®è©³ç´°
                        with st.expander("âŒ è„±è½ã—ãŸç”¨èªã®è©³ç´°", expanded=False):
                            if missed_terms:
                                # è„±è½æ®µéšã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
                                from collections import defaultdict
                                by_dropout_stage = defaultdict(list)

                                for term_info in missed_terms:
                                    stage = term_info.get("dropout_stage", "unknown")
                                    by_dropout_stage[stage].append(term_info["term"])

                                for stage, terms in sorted(by_dropout_stage.items()):
                                    st.markdown(f"**{stage}ã§è„±è½ï¼ˆ{len(terms)}ä»¶ï¼‰:**")
                                    st.write(", ".join(terms[:20]))
                                    if len(terms) > 20:
                                        st.caption(f"...ä»– {len(terms) - 20}ä»¶")
                            else:
                                st.info("ã™ã¹ã¦ã®Ground Truthç”¨èªãŒæŠ½å‡ºã•ã‚Œã¾ã—ãŸ")

                        # ãƒ¬ãƒãƒ¼ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                        dropout_json = json.dumps(dropout_data, ensure_ascii=False, indent=2)
                        st.download_button(
                            "ğŸ“¥ Ground Truthè¿½è·¡ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (JSON)",
                            data=dropout_json,
                            file_name="ground_truth_dropout_report.json",
                            mime="application/json",
                            use_container_width=True
                        )

                    except Exception as e:
                        st.error(f"Ground Truthè¿½è·¡ãƒ¬ãƒãƒ¼ãƒˆã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                        import traceback
                        st.code(traceback.format_exc())

                # 8. é€šå¸¸ãƒ¬ãƒãƒ¼ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                st.markdown("---")
                md_report = analyzer.generate_markdown_report(results)
                st.download_button(
                    "ğŸ“¥ è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (Markdown)",
                    data=md_report,
                    file_name="term_analysis_report.md",
                    mime="text/markdown",
                    use_container_width=True
                )

            except Exception as e:
                st.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                import traceback
                st.code(traceback.format_exc())
