"""
evaluation_tab.py - RAG System Evaluation Results Dashboard
==========================================================
This module provides a Streamlit interface for viewing and analyzing
RAG evaluation results from CSV files.
"""

import streamlit as st
import pandas as pd
import os
import asyncio
import math
from typing import Dict, Any, List, Optional
import plotly.graph_objects as go
import glob
from datetime import datetime
import numpy as np
from langchain_core.documents import Document
from sklearn.metrics.pairwise import cosine_similarity


def render_evaluation_tab(rag_system):
    """Render the evaluation results dashboard"""
    
    st.markdown("## ğŸ¯ RAGè©•ä¾¡çµæœãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    st.markdown("---")
    
    # èª¬æ˜ãƒ†ã‚­ã‚¹ãƒˆ
    st.info("""
    ğŸ“‹ **æ–°ã—ã„è©•ä¾¡ãƒ•ãƒ­ãƒ¼**: 
    1. Chatã‚¿ãƒ–ã§ä¸€æ‹¬è³ªå•å®Ÿè¡Œï¼ˆè³ªå•+æƒ³å®šå¼•ç”¨å…ƒã®CSVï¼‰â†’ çµæœCSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    2. ã“ã®ã‚¿ãƒ–ã§çµæœCSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ â†’ è©•ä¾¡è¨ˆç®—å®Ÿè¡Œ â†’ å³åº§ã«å¯è¦–åŒ–
    
    ğŸ“Š **ã“ã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**: ä¸€æ‹¬è³ªå•çµæœCSVã‹ã‚‰è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã€è©³ç´°ãªåˆ†æã¨å¯è¦–åŒ–ã‚’æä¾›ã—ã¾ã™ã€‚
    """)
    
    # ã‚¿ãƒ–åˆ†å‰²
    eval_tabs = st.tabs(["ğŸš€ è©•ä¾¡å®Ÿè¡Œ", "ğŸ“ˆ çµæœãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰", "ğŸ“Š æ¯”è¼ƒåˆ†æ", "ğŸ“ ãƒ¬ãƒãƒ¼ãƒˆ"])
    
    with eval_tabs[0]:
        render_evaluation_execution(rag_system)
    
    with eval_tabs[1]:
        render_results_dashboard()
    
    with eval_tabs[2]:
        render_comparison_analysis()
        
    with eval_tabs[3]:
        render_report_generation()


def render_results_dashboard():
    """çµæœãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®è¡¨ç¤º"""
    
    st.subheader("ğŸ“ˆ è©•ä¾¡çµæœãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    
    # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«é¸æŠUIä»˜ãï¼‰
    results_df = load_evaluation_results_with_selection()
    
    if results_df is not None and not results_df.empty:
        display_results_summary(results_df)
        display_detailed_analysis(results_df)
        display_question_breakdown(results_df)
    else:
        st.info("è©•ä¾¡çµæœCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        st.markdown("""
        **è©•ä¾¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ä¾‹**ï¼š
        - `evaluation_results_*.csv` 
        - `2025-08-28T15-26_export.csv`
        - ãã®ä»–ã®è©•ä¾¡å®Ÿè¡Œã§ç”Ÿæˆã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«
        """)


def load_evaluation_results_with_selection() -> Optional[pd.DataFrame]:
    """è©•ä¾¡çµæœCSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã®ã¿ï¼‰"""
    
    st.write("**è©•ä¾¡çµæœCSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„**")
    
    uploaded_file = st.file_uploader(
        "è©•ä¾¡çµæœCSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=['csv'],
        help="è©•ä¾¡å®Ÿè¡Œã§ç”Ÿæˆã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¾‹: 2025-08-28T15-26_export.csvï¼‰",
        key="results_dashboard_uploader_v3"
    )
    
    if uploaded_file:
        try:
            df = pd.read_csv(uploaded_file)
            st.success(f"âœ… {uploaded_file.name} ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{len(df)}è¡Œï¼‰")
            return df
        except Exception as e:
            st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    return None


def load_evaluation_results() -> Optional[pd.DataFrame]:
    """è©•ä¾¡çµæœCSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.write("**çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®é¸æŠ**")
        
        # è‡ªå‹•æ¤œå‡ºã•ã‚ŒãŸçµæœãƒ•ã‚¡ã‚¤ãƒ«
        result_files = glob.glob("evaluation_results*.csv")
        result_files.extend(glob.glob("*evaluation*.csv"))
        result_files = list(set(result_files))  # é‡è¤‡é™¤å»
        
        if result_files:
            # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å…ˆé ­ã«
            result_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
            
            selected_file = st.selectbox(
                "æ¤œå‡ºã•ã‚ŒãŸçµæœãƒ•ã‚¡ã‚¤ãƒ«",
                result_files,
                help="æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¸Šéƒ¨ã«è¡¨ç¤ºã•ã‚Œã¾ã™"
            )
            
            if selected_file:
                try:
                    df = pd.read_csv(selected_file)
                    file_info = os.stat(selected_file)
                    st.success(f"âœ… {selected_file} ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{len(df)}è¡Œã€{file_info.st_size:,} bytesï¼‰")
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®è¡¨ç¤º
                    mod_time = datetime.fromtimestamp(file_info.st_mtime)
                    st.caption(f"æœ€çµ‚æ›´æ–°: {mod_time.strftime('%Y-%m-%d %H:%M:%S')}")
                    
                    return df
                except Exception as e:
                    st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            st.warning("è©•ä¾¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    with col2:
        st.write("**æ‰‹å‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰**")
        uploaded_file = st.file_uploader(
            "çµæœCSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
            type=['csv'],
            help="evaluate_rag.pyã§ç”Ÿæˆã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«"
        )
        
        if uploaded_file:
            try:
                df = pd.read_csv(uploaded_file)
                st.success(f"âœ… {len(df)}è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
                return df
            except Exception as e:
                st.error(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
    
    return None


def display_results_summary(df: pd.DataFrame):
    """è©•ä¾¡çµæœã®ã‚µãƒãƒªãƒ¼è¡¨ç¤º"""
    
    st.markdown("### ğŸ“Š è©•ä¾¡ã‚µãƒãƒªãƒ¼")
    
    # åŸºæœ¬çµ±è¨ˆã®è¨ˆç®—
    total_questions = len(df)
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å–å¾—ï¼ˆã‚«ãƒ©ãƒ åã‚’æŸ”è»Ÿã«å¯¾å¿œï¼‰
    metrics = {}
    
    # MRRã®å–å¾—
    if 'MRR' in df.columns:
        metrics['MRR'] = df['MRR'].mean()
    elif 'mrr' in df.columns:
        metrics['MRR'] = df['mrr'].mean()
    
    # Recall@Kã®å–å¾—
    recall_cols = [col for col in df.columns if 'recall' in col.lower() and ('5' in col or '@5' in col)]
    if recall_cols:
        metrics['Recall@5'] = df[recall_cols[0]].mean()
    
    # Precision@Kã®å–å¾—
    precision_cols = [col for col in df.columns if 'precision' in col.lower() and ('5' in col or '@5' in col)]
    if precision_cols:
        metrics['Precision@5'] = df[precision_cols[0]].mean()
    
    # nDCG@Kã®å–å¾—
    ndcg_cols = [col for col in df.columns if 'ndcg' in col.lower() and ('5' in col or '@5' in col)]
    if ndcg_cols:
        metrics['nDCG@5'] = df[ndcg_cols[0]].mean()
    
    # Hit Rate@Kã®å–å¾—
    hit_rate_cols = [col for col in df.columns if 'hit' in col.lower() and ('5' in col or '@5' in col)]
    if hit_rate_cols:
        metrics['Hit Rate@5'] = df[hit_rate_cols[0]].mean()
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
    cols = st.columns(len(metrics) + 1)
    
    with cols[0]:
        st.metric("ç·è³ªå•æ•°", total_questions)
    
    for i, (metric_name, value) in enumerate(metrics.items(), 1):
        if i < len(cols):
            with cols[i]:
                st.metric(metric_name, f"{value:.3f}")
    
    # æ€§èƒ½ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
    if metrics:
        avg_score = sum(metrics.values()) / len(metrics)
        rating = get_performance_rating(avg_score)
        st.markdown(f"**ç·åˆè©•ä¾¡**: {rating}")


def get_performance_rating(score: float) -> str:
    """æ€§èƒ½ã‚¹ã‚³ã‚¢ã«åŸºã¥ããƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°"""
    if score >= 0.9:
        return "ğŸŒŸ å„ªç§€ (Excellent)"
    elif score >= 0.8:
        return "ğŸš€ è‰¯å¥½ (Good)"  
    elif score >= 0.7:
        return "ğŸ‘ æ™®é€š (Fair)"
    elif score >= 0.6:
        return "âš ï¸ è¦æ”¹å–„ (Needs Improvement)"
    else:
        return "âŒ ä¸è‰¯ (Poor)"


def display_detailed_analysis(df: pd.DataFrame):
    """è©³ç´°åˆ†æã®è¡¨ç¤º"""
    
    st.markdown("### ğŸ“ˆ è©³ç´°åˆ†æ")
    
    # æŒ‡æ¨™ã®åˆ†å¸ƒã‚°ãƒ©ãƒ•
    metric_cols = [col for col in df.columns if any(metric in col.lower() 
                   for metric in ['mrr', 'recall', 'precision', 'ndcg', 'hit'])]
    
    if metric_cols:
        col1, col2 = st.columns(2)
        
        with col1:
            # ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
            radar_metrics = {}
            for col in metric_cols[:6]:  # æœ€å¤§6ã¤ã®æŒ‡æ¨™
                if df[col].notna().any():
                    radar_metrics[col] = df[col].mean()
            
            if radar_metrics:
                # ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ
                fig_radar = create_radar_chart(radar_metrics, "è©•ä¾¡æŒ‡æ¨™ã®ç·åˆæ¯”è¼ƒ")
                st.plotly_chart(fig_radar, use_container_width=True)
        
        with col2:
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒæ£’ã‚°ãƒ©ãƒ•
            if len(metric_cols) > 1:
                fig_bar = create_metrics_bar_chart(df, metric_cols[:6])
                st.plotly_chart(fig_bar, use_container_width=True)
        
        # 2è¡Œç›®ã®ã‚°ãƒ©ãƒ•
        col3, col4 = st.columns(2)
        
        with col3:
            # è³ªå•åˆ¥æˆç¸¾ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
            if len(df) > 1 and len(metric_cols) > 3:
                fig_heatmap = create_question_performance_heatmap(df, metric_cols[:4])
                st.plotly_chart(fig_heatmap, use_container_width=True)
        
        with col4:
            # Kå€¤åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
            k_metrics = [col for col in metric_cols if '@' in col]
            if k_metrics:
                fig_k_comparison = create_k_value_comparison(df, k_metrics)
                st.plotly_chart(fig_k_comparison, use_container_width=True)
        
        # 3è¡Œç›®ï¼šåˆ†å¸ƒã¨ãƒˆãƒ¬ãƒ³ãƒ‰
        st.markdown("#### ğŸ“Š æŒ‡æ¨™åˆ†å¸ƒã¨ãƒˆãƒ¬ãƒ³ãƒ‰")
        
        col5, col6 = st.columns(2)
        
        with col5:
            # è©•ä¾¡æŒ‡æ¨™ã®åˆ†å¸ƒãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼ˆæ”¹å–„ç‰ˆï¼‰
            selected_metric = st.selectbox(
                "åˆ†å¸ƒã‚’è¡¨ç¤ºã™ã‚‹æŒ‡æ¨™",
                metric_cols,
                key="distribution_metric_selector"
            )
            if selected_metric:
                fig_dist = create_metric_distribution(df, selected_metric)
                st.plotly_chart(fig_dist, use_container_width=True)
        
        with col6:
            # ç´¯ç©ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆè³ªå•æ•°ã”ã¨ã®æˆç¸¾æ¨ç§»ï¼‰
            if len(df) > 1:
                fig_cumulative = create_cumulative_performance(df, metric_cols[:3])
                st.plotly_chart(fig_cumulative, use_container_width=True)
        


def display_question_breakdown(df: pd.DataFrame):
    """è³ªå•åˆ¥è©³ç´°çµæœã®è¡¨ç¤º"""
    
    st.markdown("### ğŸ“ è³ªå•åˆ¥è©³ç´°çµæœ")
    
    # è³ªå•ã‚«ãƒ©ãƒ ã®æ¤œå‡º
    question_col = None
    for col in ['question', 'Question', 'è³ªå•']:
        if col in df.columns:
            question_col = col
            break
    
    if question_col:
        # è³ªå•é¸æŠ
        questions = df[question_col].tolist()
        selected_idx = st.selectbox(
            "è©³ç´°ã‚’è¡¨ç¤ºã™ã‚‹è³ªå•",
            range(len(questions)),
            format_func=lambda x: f"Q{x+1}: {questions[x][:50]}..."
        )
        
        # é¸æŠã•ã‚ŒãŸè³ªå•ã®è©³ç´°è¡¨ç¤º
        selected_row = df.iloc[selected_idx]
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.write("**è³ªå•**")
            st.write(selected_row[question_col])
            
            # è©•ä¾¡æŒ‡æ¨™
            st.write("**è©•ä¾¡æŒ‡æ¨™**")
            metrics_data = {}
            for col in df.columns:
                if col != question_col and pd.notna(selected_row[col]):
                    try:
                        if isinstance(selected_row[col], (int, float)):
                            metrics_data[col] = selected_row[col]
                    except:
                        pass
            
            if metrics_data:
                metrics_df = pd.DataFrame(list(metrics_data.items()), 
                                        columns=['æŒ‡æ¨™', 'å€¤'])
                st.dataframe(metrics_df, hide_index=True)
        
        with col2:
            # æœŸå¾…ã•ã‚Œã‚‹å¼•ç”¨å…ƒãŒã‚ã‚Œã°è¡¨ç¤º
            expected_cols = [col for col in df.columns if 'å¼•ç”¨' in col or 'expected' in col.lower()]
            if expected_cols:
                st.write("**æœŸå¾…ã•ã‚Œã‚‹å¼•ç”¨å…ƒ**")
                for col in expected_cols:
                    if pd.notna(selected_row[col]):
                        st.write(f"- {selected_row[col]}")
            
            # å–å¾—ã•ã‚ŒãŸæ–‡æ›¸ãŒã‚ã‚Œã°è¡¨ç¤º
            retrieved_cols = [col for col in df.columns if 'retrieved' in col.lower() or 'å–å¾—' in col]
            if retrieved_cols:
                st.write("**å–å¾—ã•ã‚ŒãŸæ–‡æ›¸**")
                for col in retrieved_cols:
                    if pd.notna(selected_row[col]):
                        st.write(f"- {selected_row[col]}")
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«
    with st.expander("ğŸ“‹ å…¨ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º", expanded=False):
        st.dataframe(df, use_container_width=True)


def render_comparison_analysis():
    """æ¯”è¼ƒåˆ†æã®è¡¨ç¤º"""
    
    st.subheader("ğŸ“Š æ¯”è¼ƒåˆ†æ")
    
    # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®é¸æŠ
    result_files = glob.glob("evaluation_results*.csv")
    result_files.extend(glob.glob("*evaluation*.csv"))
    result_files = list(set(result_files))
    
    if len(result_files) < 2:
        st.warning("æ¯”è¼ƒåˆ†æã«ã¯2ã¤ä»¥ä¸Šã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ã§ã™ã€‚")
        return
    
    selected_files = st.multiselect(
        "æ¯”è¼ƒã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
        result_files,
        default=result_files[:min(3, len(result_files))]  # æœ€å¤§3ãƒ•ã‚¡ã‚¤ãƒ«
    )
    
    if len(selected_files) >= 2:
        comparison_data = load_multiple_results(selected_files)
        
        if comparison_data:
            display_comparison_charts(comparison_data)
            display_improvement_analysis(comparison_data)


def load_multiple_results(file_paths: List[str]) -> Dict[str, pd.DataFrame]:
    """è¤‡æ•°ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    
    results = {}
    for file_path in file_paths:
        try:
            df = pd.read_csv(file_path)
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰è­˜åˆ¥å­ã‚’ä½œæˆ
            name = os.path.basename(file_path).replace('.csv', '')
            results[name] = df
        except Exception as e:
            st.error(f"{file_path} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
    
    return results


def display_comparison_charts(comparison_data: Dict[str, pd.DataFrame]):
    """æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆã®è¡¨ç¤º"""
    
    st.markdown("#### ğŸ“ˆ æ€§èƒ½æ¯”è¼ƒ")
    
    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®å¹³å‡æŒ‡æ¨™ã‚’è¨ˆç®—
    comparison_metrics = {}
    
    for name, df in comparison_data.items():
        metrics = {}
        
        # ä¸»è¦æŒ‡æ¨™ã®å¹³å‡å€¤ã‚’è¨ˆç®—
        for col in df.columns:
            if any(metric in col.lower() for metric in ['mrr', 'recall', 'precision', 'ndcg', 'hit']):
                if df[col].dtype in ['float64', 'int64'] and df[col].notna().any():
                    metrics[col] = df[col].mean()
        
        comparison_metrics[name] = metrics
    
    if comparison_metrics:
        # æ¯”è¼ƒæ£’ã‚°ãƒ©ãƒ•
        fig_comparison = create_comparison_bar_chart(comparison_metrics)
        st.plotly_chart(fig_comparison, use_container_width=True)
        
        # æ¯”è¼ƒãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ
        fig_radar = create_comparison_radar_chart(comparison_metrics)
        st.plotly_chart(fig_radar, use_container_width=True)


def display_improvement_analysis(comparison_data: Dict[str, pd.DataFrame]):
    """æ”¹å–„åˆ†æã®è¡¨ç¤º"""
    
    st.markdown("#### ğŸ“Š æ”¹å–„åˆ†æ")
    
    if len(comparison_data) >= 2:
        file_names = list(comparison_data.keys())
        
        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®é¸æŠ
        col1, col2 = st.columns(2)
        
        with col1:
            baseline = st.selectbox("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³", file_names, index=0)
        
        with col2:
            target = st.selectbox("æ¯”è¼ƒå¯¾è±¡", file_names, index=1)
        
        if baseline != target:
            improvement_analysis = calculate_improvement(
                comparison_data[baseline], 
                comparison_data[target]
            )
            
            if improvement_analysis:
                display_improvement_metrics(improvement_analysis)


def calculate_improvement(baseline_df: pd.DataFrame, target_df: pd.DataFrame) -> Dict[str, float]:
    """æ”¹å–„ç‡ã®è¨ˆç®—"""
    
    improvements = {}
    
    for col in baseline_df.columns:
        if (col in target_df.columns and 
            baseline_df[col].dtype in ['float64', 'int64'] and 
            baseline_df[col].notna().any() and target_df[col].notna().any()):
            
            baseline_mean = baseline_df[col].mean()
            target_mean = target_df[col].mean()
            
            if baseline_mean != 0:
                improvement = ((target_mean - baseline_mean) / baseline_mean) * 100
                improvements[col] = improvement
    
    return improvements


def display_improvement_metrics(improvements: Dict[str, float]):
    """æ”¹å–„æŒ‡æ¨™ã®è¡¨ç¤º"""
    
    st.markdown("**æ”¹å–„ç‡ (%)**")
    
    # æ”¹å–„ç‡ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
    metrics_cols = st.columns(min(len(improvements), 4))
    
    for i, (metric, improvement) in enumerate(improvements.items()):
        if i < len(metrics_cols):
            with metrics_cols[i]:
                delta_color = "normal" if improvement >= 0 else "inverse"
                st.metric(
                    metric, 
                    f"{improvement:+.1f}%",
                    delta=f"{improvement:+.1f}%",
                    delta_color=delta_color
                )


def render_evaluation_execution(rag_system):
    """è©•ä¾¡å®Ÿè¡Œã‚»ã‚¯ã‚·ãƒ§ãƒ³"""
    
    st.subheader("ğŸš€ è©•ä¾¡å®Ÿè¡Œ")
    
    st.markdown("""
    ä¸€æ‹¬è³ªå•çµæœCSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€è©•ä¾¡æŒ‡æ¨™ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
    
    **å¿…è¦ãªCSVãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ**ï¼š
    - è³ªå•, æƒ³å®šã®å¼•ç”¨å…ƒ1, æƒ³å®šã®å¼•ç”¨å…ƒ2, ..., å›ç­”, å‚ç…§ã‚½ãƒ¼ã‚¹, ãƒãƒ£ãƒ³ã‚¯1, ãƒãƒ£ãƒ³ã‚¯2, ...
    """)
    
    # è¨­å®šã‚»ã‚¯ã‚·ãƒ§ãƒ³
    with st.expander("âš™ï¸ è©•ä¾¡è¨­å®š", expanded=True):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            similarity_method = st.selectbox(
                "é¡ä¼¼åº¦è¨ˆç®—æ‰‹æ³•",
                ["azure_embedding", "text_overlap", "azure_llm"],
                help="è©•ä¾¡ã«ä½¿ç”¨ã™ã‚‹é¡ä¼¼åº¦è¨ˆç®—æ‰‹æ³•ã‚’é¸æŠ"
            )
        
        with col2:
            similarity_threshold = st.slider(
                "é¡ä¼¼åº¦é–¾å€¤",
                min_value=0.0,
                max_value=1.0,
                value=0.7,
                step=0.1,
                help="é–¢é€£æ€§åˆ¤å®šã®é–¾å€¤"
            )
        
        with col3:
            k_values = st.multiselect(
                "è©•ä¾¡Kå€¤",
                [1, 3, 5, 10],
                default=[1, 3, 5],
                help="Recall@K, Precision@Kãªã©ã§ä½¿ç”¨ã™ã‚‹Kå€¤"
            )
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.file_uploader(
        "ä¸€æ‹¬è³ªå•çµæœCSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=['csv'],
        help="Chatã‚¿ãƒ–ã®ä¸€æ‹¬è³ªå•ã§ç”Ÿæˆã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«",
        key="evaluation_execution_uploader"
    )
    
    if uploaded_file:
        try:
            df = pd.read_csv(uploaded_file)
            st.success(f"âœ… CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆ{len(df)}è¡Œï¼‰")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            with st.expander("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=False):
                st.dataframe(df.head(), use_container_width=True)
            
            # è©•ä¾¡å®Ÿè¡Œãƒœã‚¿ãƒ³
            if st.button("ğŸ¯ è©•ä¾¡ã‚’å®Ÿè¡Œ", type="primary", use_container_width=True):
                run_evaluation(df, rag_system, similarity_method, similarity_threshold, k_values)
                
        except Exception as e:
            st.error(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


def run_evaluation(df: pd.DataFrame, rag_system, similarity_method: str, similarity_threshold: float, k_values: list):
    """è©•ä¾¡å®Ÿè¡Œå‡¦ç†"""
    
    with st.spinner("è©•ä¾¡è¨ˆç®—ã‚’å®Ÿè¡Œä¸­..."):
        try:
            # RAGEvaluatorã®åˆæœŸåŒ–
            from rag.evaluator import RAGEvaluator
            evaluator = RAGEvaluator(
                config=rag_system.config,
                similarity_method=similarity_method,
                similarity_threshold=similarity_threshold,
                k_values=k_values
            )
            
            # è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
            evaluation_results = []
            progress_bar = st.progress(0)
            total_rows = len(df)
            
            for i, row in df.iterrows():
                # è³ªå•ã®å–å¾—
                question = row.get('è³ªå•', '')
                if not question:
                    continue
                
                # æƒ³å®šå¼•ç”¨å…ƒã®å–å¾—
                expected_sources = []
                for col in df.columns:
                    if 'æƒ³å®šã®å¼•ç”¨å…ƒ' in col and pd.notna(row[col]):
                        expected_sources.append(str(row[col]))
                
                # å–å¾—ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ã®å–å¾—
                retrieved_chunks = []
                for col in df.columns:
                    if 'ãƒãƒ£ãƒ³ã‚¯' in col and pd.notna(row[col]):
                        chunk_content = str(row[col])
                        # ãƒãƒ£ãƒ³ã‚¯å†…å®¹ã‹ã‚‰ãƒ¡ã‚¿æƒ…å ±ã‚’å®Œå…¨ã«é™¤å»
                        if '---\n' in chunk_content:
                            text_content = chunk_content.split('---\n', 1)[1]
                        elif '\n---\n' in chunk_content:
                            text_content = chunk_content.split('\n---\n', 1)[1]
                        else:
                            # ãƒ¡ã‚¿æƒ…å ±ãŒãªã„å ´åˆã€æœ€åˆã®è¡Œï¼ˆSource: xxxï¼‰ã‚’é™¤å»
                            lines = chunk_content.split('\n')
                            if lines[0].startswith('Source:'):
                                text_content = '\n'.join(lines[1:])
                            else:
                                text_content = chunk_content
                        
                        # ç©ºç™½è¡Œã‚’å‰Šé™¤ã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                        text_content = text_content.strip()
                        if text_content:  # ç©ºã§ãªã„å ´åˆã®ã¿è¿½åŠ 
                            retrieved_chunks.append(text_content)
                
                if expected_sources and retrieved_chunks:
                    # è©•ä¾¡è¨ˆç®—
                    result = calculate_single_question_metrics(
                        evaluator=evaluator,
                        question=question,
                        retrieved_documents=retrieved_chunks,
                        expected_sources=expected_sources
                    )
                    evaluation_results.append(result)
                
                progress_bar.progress((i + 1) / total_rows)
            
            # çµæœã®é›†ç´„
            if evaluation_results:
                aggregated_metrics = evaluator.create_evaluation_report(evaluation_results)
                
                # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
                st.session_state.evaluation_results = evaluation_results
                st.session_state.evaluation_metrics = aggregated_metrics
                st.session_state.evaluation_method = similarity_method
                
                # çµæœã®è¡¨ç¤º
                display_evaluation_results(aggregated_metrics, evaluation_results)
                
                # CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
                export_evaluation_results(evaluation_results, similarity_method)
            else:
                st.error("è©•ä¾¡ã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚CSVã®å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                
        except Exception as e:
            st.error(f"è©•ä¾¡å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


def calculate_single_question_metrics(evaluator, question: str, retrieved_documents: List[str], expected_sources: List[str]):
    """å˜ä¸€è³ªå•ã«å¯¾ã™ã‚‹è©•ä¾¡æŒ‡æ¨™è¨ˆç®—ï¼ˆåŒæœŸç‰ˆï¼‰"""
    
    # Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
    retrieved_docs = [Document(page_content=doc) for doc in retrieved_documents]
    
    # é¡ä¼¼åº¦è¡Œåˆ—ã®è¨ˆç®—
    relevance_matrix = []
    
    for doc in retrieved_docs:
        chunk_content = doc.page_content
        chunk_relevances = []
        
        for expected_source in expected_sources:
            score = 0.0
            
            if evaluator.similarity_method == 'azure_embedding':
                # Azure Embeddingã‚’åŒæœŸçš„ã«å‡¦ç†
                try:
                    embedding1 = asyncio.run(evaluator.get_azure_embedding(chunk_content))
                    embedding2 = asyncio.run(evaluator.get_azure_embedding(expected_source))
                    if embedding1 is not None and embedding2 is not None:
                        score = cosine_similarity(
                            embedding1.reshape(1, -1), 
                            embedding2.reshape(1, -1)
                        )[0][0]
                except Exception as e:
                    st.warning(f"Embeddingè¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                    score = 0.0
                        
            elif evaluator.similarity_method == 'azure_llm':
                # Azure LLMã‚’åŒæœŸçš„ã«å‡¦ç†
                try:
                    score = asyncio.run(evaluator.calculate_azure_llm_similarity(
                        question, expected_source, chunk_content
                    ))
                except Exception as e:
                    st.warning(f"LLMé¡ä¼¼åº¦è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                    score = 0.0
                    
            elif evaluator.similarity_method == 'text_overlap':
                score = evaluator.calculate_text_overlap(expected_source, chunk_content)
                    
            elif evaluator.similarity_method == 'hybrid':
                # ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è¨ˆç®—
                try:
                    embedding1 = asyncio.run(evaluator.get_azure_embedding(chunk_content))
                    embedding2 = asyncio.run(evaluator.get_azure_embedding(expected_source))
                    embed_score = 0.0
                    if embedding1 is not None and embedding2 is not None:
                        embed_score = cosine_similarity(
                            embedding1.reshape(1, -1), 
                            embedding2.reshape(1, -1)
                        )[0][0]
                    overlap_score = evaluator.calculate_text_overlap(expected_source, chunk_content)
                    score = 0.7 * embed_score + 0.3 * overlap_score
                except Exception as e:
                    st.warning(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
                    score = evaluator.calculate_text_overlap(expected_source, chunk_content)
            
            is_relevant = score >= evaluator.similarity_threshold
            chunk_relevances.append((is_relevant, score))
        
        relevance_matrix.append(chunk_relevances)
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
    recall_at_k, precision_at_k, ndcg_at_k, hit_rate_at_k = {}, {}, {}, {}
    
    for k in evaluator.k_values:
        k_chunks = min(k, len(retrieved_docs))
        k_relevance_matrix = relevance_matrix[:k_chunks]
        
        # Recall@K
        found_sources = sum(
            1 for i in range(len(expected_sources)) 
            if any(k_relevance_matrix[j][i][0] for j in range(k_chunks))
        )
        recall_at_k[k] = found_sources / len(expected_sources) if expected_sources else 0.0
        
        # Precision@K
        relevant_chunks = sum(
            1 for i in range(k_chunks) 
            if any(is_rel for is_rel, _ in k_relevance_matrix[i])
        )
        precision_at_k[k] = relevant_chunks / k_chunks if k_chunks > 0 else 0.0
        
        # Hit Rate@K
        hit_rate_at_k[k] = 1.0 if relevant_chunks > 0 else 0.0
        
        # nDCG@K: æ­£è¦åŒ–ã•ã‚ŒãŸç´¯ç©åˆ©å¾—
        dcg = 0.0
        for i in range(k_chunks):
            if i < len(k_relevance_matrix):
                # å„ä½ç½®ã§ã®é–¢é€£æ€§åˆ¤å®šï¼ˆãƒã‚¤ãƒŠãƒª: 1 if relevant, 0 if notï¼‰
                is_relevant = any(is_rel for is_rel, _ in k_relevance_matrix[i])
                relevance_score = 1.0 if is_relevant else 0.0
                dcg += relevance_score / math.log2(i + 2)
        
        # IDCG: ç†æƒ³çš„ãªãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼ˆé–¢é€£æ–‡æ›¸æ•°ã®ä¸Šä½kå€‹ï¼‰
        num_relevant = sum(
            1 for r in relevance_matrix 
            if any(is_rel for is_rel, _ in r)
        )
        ideal_relevant_count = min(k_chunks, num_relevant, len(expected_sources))
        
        idcg = sum(
            1.0 / math.log2(i + 2) 
            for i in range(ideal_relevant_count)
        )
        
        ndcg_at_k[k] = dcg / idcg if idcg > 0 else 0.0
    
    # MRR
    mrr = next(
        (1.0 / (i + 1) for i, r in enumerate(relevance_matrix) 
         if any(is_rel for is_rel, _ in r)), 
        0.0
    )
    
    # é–¢é€£æ€§ã‚¹ã‚³ã‚¢
    relevance_scores = [
        max(score for _, score in chunk_relevances) 
        for chunk_relevances in relevance_matrix
    ]
    
    # EvaluationResultsã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
    from rag.evaluator import EvaluationResults
    return EvaluationResults(
        question=question,
        recall_at_k=recall_at_k,
        precision_at_k=precision_at_k,
        mrr=mrr,
        ndcg_at_k=ndcg_at_k,
        hit_rate_at_k=hit_rate_at_k,
        retrieved_docs=retrieved_docs,
        expected_sources=expected_sources,
        relevance_scores=relevance_scores
    )


def display_evaluation_results(metrics, results):
    """è©•ä¾¡çµæœã®è¡¨ç¤º"""
    
    st.success("ğŸ‰ è©•ä¾¡ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
    st.markdown("### ğŸ“Š è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼")
    
    cols = st.columns(5)
    with cols[0]:
        st.metric("ç·è³ªå•æ•°", metrics.num_questions)
    with cols[1]:
        st.metric("MRR", f"{metrics.mrr:.3f}")
    with cols[2]:
        st.metric("Recall@5", f"{metrics.recall_at_k.get(5, 0):.3f}")
    with cols[3]:
        st.metric("Precision@5", f"{metrics.precision_at_k.get(5, 0):.3f}")
    with cols[4]:
        st.metric("nDCG@5", f"{metrics.ndcg_at_k.get(5, 0):.3f}")
    
    # ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ
    radar_metrics = {
        "MRR": metrics.mrr,
        "Recall@3": metrics.recall_at_k.get(3, 0),
        "Precision@3": metrics.precision_at_k.get(3, 0),
        "nDCG@3": metrics.ndcg_at_k.get(3, 0),
        "Hit Rate@3": metrics.hit_rate_at_k.get(3, 0)
    }
    
    fig_radar = create_radar_chart(radar_metrics, "è©•ä¾¡æŒ‡æ¨™çµæœ")
    st.plotly_chart(fig_radar, use_container_width=True)


def export_evaluation_results(results, method):
    """è©•ä¾¡çµæœã®CSVã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    
    st.markdown("### ğŸ’¾ çµæœä¿å­˜")
    
    # çµæœã‚’DataFrameã«å¤‰æ›
    export_data = []
    for result in results:
        row_data = {
            "question": result.question,
            "mrr": result.mrr
        }
        
        # Kå€¤åˆ¥ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        for k in [1, 3, 5]:
            row_data[f"recall@{k}"] = result.recall_at_k.get(k, 0)
            row_data[f"precision@{k}"] = result.precision_at_k.get(k, 0)
            row_data[f"ndcg@{k}"] = result.ndcg_at_k.get(k, 0)
            row_data[f"hit_rate@{k}"] = result.hit_rate_at_k.get(k, 0)
        
        export_data.append(row_data)
    
    if export_data:
        export_df = pd.DataFrame(export_data)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"evaluation_results_{method}_{timestamp}.csv"
        
        csv_data = export_df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label=f"ğŸ“¥ çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ ({filename})",
            data=csv_data,
            file_name=filename,
            mime="text/csv"
        )


def render_report_generation():
    """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ©Ÿèƒ½"""
    
    st.subheader("ğŸ“ è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
    
    # ãƒ¬ãƒãƒ¼ãƒˆè¨­å®š
    col1, col2 = st.columns([1, 1])
    
    with col1:
        report_type = st.selectbox(
            "ãƒ¬ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—",
            ["æ¨™æº–ãƒ¬ãƒãƒ¼ãƒˆ", "è©³ç´°åˆ†æãƒ¬ãƒãƒ¼ãƒˆ", "æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ"]
        )
        
        include_charts = st.checkbox("ã‚°ãƒ©ãƒ•ã‚’å«ã‚ã‚‹", value=True)
        include_raw_data = st.checkbox("ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚ã‚‹", value=False)
    
    with col2:
        export_format = st.selectbox(
            "å‡ºåŠ›å½¢å¼", 
            ["CSV", "HTML", "Markdown"]
        )
        
        report_filename = st.text_input(
            "ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å",
            value=f"rag_evaluation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        )
    
    if st.button("ğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ", type="primary"):
        # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        results_df = load_evaluation_results()
        
        if results_df is not None:
            generate_evaluation_report(
                results_df, 
                report_type, 
                export_format,
                report_filename,
                include_charts,
                include_raw_data
            )
        else:
            st.error("ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")


def generate_evaluation_report(df: pd.DataFrame, report_type: str, export_format: str, 
                             filename: str, include_charts: bool, include_raw_data: bool):
    """è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
    
    try:
        # ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ã®ç”Ÿæˆ
        report_content = create_report_content(df, report_type, include_charts, include_raw_data)
        
        if export_format == "CSV":
            # CSVå½¢å¼ã§ã®å‡ºåŠ›
            output_filename = f"{filename}.csv"
            df.to_csv(output_filename, index=False, encoding='utf-8-sig')
            
        elif export_format == "HTML":
            # HTMLå½¢å¼ã§ã®å‡ºåŠ›
            output_filename = f"{filename}.html"
            with open(output_filename, 'w', encoding='utf-8') as f:
                f.write(report_content)
                
        elif export_format == "Markdown":
            # Markdownå½¢å¼ã§ã®å‡ºåŠ›
            output_filename = f"{filename}.md"
            with open(output_filename, 'w', encoding='utf-8') as f:
                f.write(report_content)
        
        st.success(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {output_filename}")
        
        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯
        with open(output_filename, 'r', encoding='utf-8') as f:
            content = f.read()
            
        st.download_button(
            label=f"ğŸ“¥ {output_filename} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            data=content,
            file_name=output_filename,
            mime="text/plain" if export_format == "Markdown" else "text/html"
        )
        
    except Exception as e:
        st.error(f"ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")


def create_report_content(df: pd.DataFrame, report_type: str, include_charts: bool, include_raw_data: bool) -> str:
    """ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ã®ä½œæˆ"""
    
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    content = f"""
# RAGã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

**ç”Ÿæˆæ—¥æ™‚**: {timestamp}  
**ãƒ¬ãƒãƒ¼ãƒˆã‚¿ã‚¤ãƒ—**: {report_type}  
**è©•ä¾¡å¯¾è±¡**: {len(df)}ä»¶ã®è³ªå•

## ğŸ“Š è©•ä¾¡ã‚µãƒãƒªãƒ¼

"""
    
    # åŸºæœ¬çµ±è¨ˆã®è¿½åŠ 
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        content += "### ä¸»è¦æŒ‡æ¨™\n\n"
        for col in numeric_cols:
            mean_val = df[col].mean()
            content += f"- **{col}**: {mean_val:.3f}\n"
    
    content += f"\n### ãƒ‡ãƒ¼ã‚¿æ¦‚è¦\n\n"
    content += f"- ç·è³ªå•æ•°: {len(df)}\n"
    content += f"- ãƒ‡ãƒ¼ã‚¿ã‚«ãƒ©ãƒ æ•°: {len(df.columns)}\n"
    
    # ç”Ÿãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ 
    if include_raw_data:
        content += f"\n## ğŸ“‹ è©³ç´°ãƒ‡ãƒ¼ã‚¿\n\n"
        content += df.to_markdown(index=False)
    
    content += f"\n---\n*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ*"
    
    return content


def show_evaluation_instructions():
    """è©•ä¾¡å®Ÿè¡Œæ‰‹é †ã®è¡¨ç¤º"""
    
    st.markdown("### ğŸ”§ è©•ä¾¡å®Ÿè¡Œæ‰‹é †")
    
    st.code("""
# 1. è©•ä¾¡ãƒ‡ãƒ¼ã‚¿CSVã®æº–å‚™
# å½¢å¼: è³ªå•,æƒ³å®šã®å¼•ç”¨å…ƒ1,æƒ³å®šã®å¼•ç”¨å…ƒ2,æƒ³å®šã®å¼•ç”¨å…ƒ3

# 2. ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã§è©•ä¾¡å®Ÿè¡Œ
python evaluate_rag.py

# 3. ç”Ÿæˆã•ã‚ŒãŸçµæœCSVã‚’ã“ã®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ç¢ºèª
    """, language="bash")
    
    st.markdown("è©³ç´°ãªæ‰‹é †ã¯ [è©•ä¾¡ã‚¬ã‚¤ãƒ‰](../docs/evaluation_ui_guide.md) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚")


# ===== ãƒãƒ£ãƒ¼ãƒˆä½œæˆé–¢æ•° =====

def create_metrics_bar_chart(df: pd.DataFrame, metric_cols: List[str]) -> go.Figure:
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒæ£’ã‚°ãƒ©ãƒ•ã®ä½œæˆ"""
    
    fig = go.Figure()
    
    metrics_data = {}
    for col in metric_cols:
        if col in df.columns and df[col].notna().any():
            metrics_data[col] = df[col].mean()
    
    if metrics_data:
        metrics = list(metrics_data.keys())
        values = list(metrics_data.values())
        
        # ã‚«ãƒ©ãƒ¼ã‚°ãƒ©ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7', '#DDA0DD']
        
        fig.add_trace(go.Bar(
            x=metrics,
            y=values,
            marker=dict(
                color=values,
                colorscale='Viridis',
                showscale=True,
                colorbar=dict(title="ã‚¹ã‚³ã‚¢", thickness=15)
            ),
            text=[f"{v:.3f}" for v in values],
            textposition='outside',
            textfont=dict(size=12, color='#333')
        ))
    
    fig.update_layout(
        title={
            'text': "ğŸ“Š è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¯”è¼ƒ",
            'x': 0.5,
            'xanchor': 'center',
            'font': dict(size=16)
        },
        xaxis_title="",
        yaxis_title="å¹³å‡ã‚¹ã‚³ã‚¢",
        showlegend=False,
        plot_bgcolor='rgba(240, 240, 240, 0.1)',
        paper_bgcolor='white',
        yaxis=dict(
            gridcolor='rgba(200, 200, 200, 0.3)',
            range=[0, max(values) * 1.2] if values else [0, 1]
        ),
        xaxis=dict(tickangle=-45),
        margin=dict(t=60, b=100),
        height=400
    )
    
    return fig


def create_question_performance_heatmap(df: pd.DataFrame, metric_cols: List[str]) -> go.Figure:
    """è³ªå•åˆ¥æˆç¸¾ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®ä½œæˆ"""
    
    # è³ªå•åˆ—ã‚’å–å¾—
    question_col = None
    for col in ['question', 'Question', 'è³ªå•']:
        if col in df.columns:
            question_col = col
            break
    
    if not question_col:
        # è³ªå•åˆ—ãŒãªã„å ´åˆã¯è¡Œç•ªå·ã‚’ä½¿ç”¨
        questions = [f"Q{i+1}" for i in range(len(df))]
    else:
        questions = [q[:30] + "..." if len(str(q)) > 30 else str(q) for q in df[question_col]]
    
    # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
    z_data = []
    for col in metric_cols:
        if col in df.columns:
            z_data.append(df[col].fillna(0).tolist())
    
    fig = go.Figure(data=go.Heatmap(
        z=z_data,
        x=questions,
        y=metric_cols,
        colorscale='RdBu',
        colorbar=dict(
            title="ã‚¹ã‚³ã‚¢",
            thickness=15,
            len=0.7
        ),
        hoverongaps=False,
        text=[[f"{val:.3f}" for val in row] for row in z_data],
        texttemplate="%{text}",
        textfont={"size": 10}
    ))
    
    fig.update_layout(
        title={
            'text': "ğŸ”¥ è³ªå•åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—",
            'x': 0.5,
            'xanchor': 'center',
            'font': dict(size=16)
        },
        xaxis_title="",
        yaxis_title="",
        xaxis={'side': 'bottom'},
        plot_bgcolor='rgba(240, 240, 240, 0.1)',
        paper_bgcolor='white',
        height=400,
        margin=dict(t=60, b=100)
    )
    
    return fig


def create_k_value_comparison(df: pd.DataFrame, k_metrics: List[str]) -> go.Figure:
    """Kå€¤åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã‚°ãƒ©ãƒ•ã®ä½œæˆ"""
    
    fig = go.Figure()
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç¨®åˆ¥ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    metric_types = {}
    for col in k_metrics:
        if '@' in col:
            base_metric = col.split('@')[0].lower()
            if base_metric not in metric_types:
                metric_types[base_metric] = []
            metric_types[base_metric].append(col)
    
    colors = ['rgb(31, 119, 180)', 'rgb(255, 127, 14)', 'rgb(44, 160, 44)', 'rgb(214, 39, 40)']
    
    for i, (metric_type, cols) in enumerate(metric_types.items()):
        k_values = []
        avg_values = []
        
        for col in sorted(cols):
            if col in df.columns:
                k_val = col.split('@')[1] if '@' in col else col
                k_values.append(f"K={k_val}")
                avg_values.append(df[col].mean())
        
        fig.add_trace(go.Scatter(
            x=k_values,
            y=avg_values,
            mode='lines+markers',
            name=metric_type.capitalize(),
            line=dict(color=colors[i % len(colors)], width=3),
            marker=dict(size=10, symbol='circle'),
            hovertemplate='%{y:.3f}<extra></extra>'
        ))
    
    fig.update_layout(
        title={
            'text': "ğŸ“ˆ Kå€¤åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ",
            'x': 0.5,
            'xanchor': 'center',
            'font': dict(size=16)
        },
        xaxis_title="Kå€¤",
        yaxis_title="å¹³å‡ã‚¹ã‚³ã‚¢",
        showlegend=True,
        plot_bgcolor='rgba(240, 240, 240, 0.1)',
        paper_bgcolor='white',
        yaxis=dict(
            gridcolor='rgba(200, 200, 200, 0.3)',
            range=[0, 1.05]
        ),
        xaxis=dict(gridcolor='rgba(200, 200, 200, 0.3)'),
        legend=dict(
            bgcolor='rgba(255, 255, 255, 0.8)',
            bordercolor='rgba(200, 200, 200, 0.5)',
            borderwidth=1
        ),
        height=400,
        margin=dict(t=60)
    )
    
    return fig


def create_metric_distribution(df: pd.DataFrame, metric: str) -> go.Figure:
    """æŒ‡æ¨™åˆ†å¸ƒãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã®ä½œæˆ"""
    
    if metric not in df.columns:
        return go.Figure()
    
    values = df[metric].dropna()
    
    fig = go.Figure()
    fig.add_trace(go.Histogram(
        x=values,
        nbinsx=min(20, len(values)),
        marker=dict(
            color='#45B7D1',
            line=dict(color='#2C3E50', width=1.5)
        ),
        opacity=0.75,
        name=metric,
        hovertemplate='%{x:.3f}<br>é »åº¦: %{y}<extra></extra>'
    ))
    
    # çµ±è¨ˆæƒ…å ±ã‚’è¿½åŠ 
    mean_val = values.mean()
    median_val = values.median()
    
    fig.add_vline(
        x=mean_val, 
        line_dash="dash", 
        line_color="#E74C3C",
        line_width=2,
        annotation_text=f"å¹³å‡: {mean_val:.3f}",
        annotation_position="top right"
    )
    fig.add_vline(
        x=median_val, 
        line_dash="dash", 
        line_color="#27AE60",
        line_width=2,
        annotation_text=f"ä¸­å¤®å€¤: {median_val:.3f}",
        annotation_position="top left"
    )
    
    fig.update_layout(
        title={
            'text': f"ğŸ“Š {metric} ã®åˆ†å¸ƒ",
            'x': 0.5,
            'xanchor': 'center',
            'font': dict(size=16)
        },
        xaxis_title=metric,
        yaxis_title="é »åº¦",
        showlegend=False,
        plot_bgcolor='rgba(240, 240, 240, 0.1)',
        paper_bgcolor='white',
        bargap=0.1,
        yaxis=dict(gridcolor='rgba(200, 200, 200, 0.3)'),
        xaxis=dict(gridcolor='rgba(200, 200, 200, 0.3)'),
        height=400,
        margin=dict(t=60)
    )
    
    return fig


def create_cumulative_performance(df: pd.DataFrame, metric_cols: List[str]) -> go.Figure:
    """ç´¯ç©ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ä½œæˆ"""
    
    fig = go.Figure()
    
    # 16é€²æ•°ã‚«ãƒ©ãƒ¼ã‚³ãƒ¼ãƒ‰ã«å¤‰æ›´
    colors = ['#1F77B4', '#FF7F0E', '#2CA02C', '#D62728', '#9467BD', '#8C564B']
    
    for i, col in enumerate(metric_cols):
        if col in df.columns:
            # ç´¯ç©å¹³å‡ã‚’è¨ˆç®—
            cumulative_avg = df[col].expanding().mean()
            
            fig.add_trace(go.Scatter(
                x=list(range(1, len(cumulative_avg) + 1)),
                y=cumulative_avg,
                mode='lines+markers',
                name=col,
                line=dict(
                    color=colors[i % len(colors)], 
                    width=3,
                    shape='spline'
                ),
                marker=dict(
                    size=8,
                    symbol='circle',
                    line=dict(width=2, color='white')
                ),
                hovertemplate='%{y:.3f}<extra></extra>'
            ))
    
    fig.update_layout(
        title={
            'text': "ğŸ“Š ç´¯ç©ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¨ç§»",
            'x': 0.5,
            'xanchor': 'center',
            'font': dict(size=16)
        },
        xaxis_title="è³ªå•æ•°",
        yaxis_title="ç´¯ç©å¹³å‡ã‚¹ã‚³ã‚¢",
        showlegend=True,
        plot_bgcolor='rgba(240, 240, 240, 0.1)',
        paper_bgcolor='white',
        yaxis=dict(
            gridcolor='rgba(200, 200, 200, 0.3)',
            range=[0, 1.05]
        ),
        xaxis=dict(gridcolor='rgba(200, 200, 200, 0.3)'),
        legend=dict(
            bgcolor='rgba(255, 255, 255, 0.8)',
            bordercolor='rgba(200, 200, 200, 0.5)',
            borderwidth=1,
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="center",
            x=0.5
        ),
        height=400,
        margin=dict(t=100)
    )
    
    return fig


def create_radar_chart(metrics: Dict[str, float], title: str) -> go.Figure:
    """ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®ä½œæˆ"""
    
    categories = list(metrics.keys())
    values = list(metrics.values())
    
    # é–‰ã˜ãŸå½¢ã«ã™ã‚‹ãŸã‚æœ€åˆã®å€¤ã‚’è¿½åŠ 
    categories_closed = categories + [categories[0]]
    values_closed = values + [values[0]]
    
    fig = go.Figure()
    
    # èƒŒæ™¯ã®ã‚°ãƒªãƒƒãƒ‰ãƒ¬ã‚¤ãƒ¤ãƒ¼
    for level in [0.2, 0.4, 0.6, 0.8, 1.0]:
        fig.add_trace(go.Scatterpolar(
            r=[level] * len(categories_closed),
            theta=categories_closed,
            fill=None,
            line=dict(color='rgba(200, 200, 200, 0.3)', width=1),
            showlegend=False,
            hoverinfo='skip'
        ))
    
    # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿
    fig.add_trace(go.Scatterpolar(
        r=values_closed,
        theta=categories_closed,
        fill='toself',
        name='è©•ä¾¡æŒ‡æ¨™',
        line=dict(color='#4ECDC4', width=3),
        fillcolor='rgba(78, 205, 196, 0.3)',
        marker=dict(size=10, color='#4ECDC4', symbol='circle'),
        hovertemplate='%{theta}: %{r:.3f}<extra></extra>'
    ))
    
    fig.update_layout(
        polar=dict(
            bgcolor='rgba(240, 240, 240, 0.1)',
            radialaxis=dict(
                visible=True,
                range=[0, 1],
                showline=False,
                gridcolor='rgba(200, 200, 200, 0.3)',
                tickfont=dict(size=10)
            ),
            angularaxis=dict(
                gridcolor='rgba(200, 200, 200, 0.3)',
                tickfont=dict(size=12)
            )
        ),
        showlegend=False,
        title={
            'text': f"ğŸ¯ {title}",
            'x': 0.5,
            'xanchor': 'center',
            'font': dict(size=16)
        },
        paper_bgcolor='white',
        height=400,
        margin=dict(t=60, b=40, l=40, r=40)
    )
    
    return fig




def create_comparison_bar_chart(comparison_metrics: Dict[str, Dict[str, float]]) -> go.Figure:
    """æ¯”è¼ƒæ£’ã‚°ãƒ©ãƒ•ã®ä½œæˆ"""
    
    # å…±é€šæŒ‡æ¨™ã®æŠ½å‡º
    all_metrics = set()
    for metrics in comparison_metrics.values():
        all_metrics.update(metrics.keys())
    
    common_metrics = list(all_metrics)[:6]  # æœ€å¤§6ã¤
    
    fig = go.Figure()
    
    for name, metrics in comparison_metrics.items():
        values = [metrics.get(metric, 0) for metric in common_metrics]
        fig.add_trace(go.Bar(
            x=common_metrics,
            y=values,
            name=name
        ))
    
    fig.update_layout(
        title="è©•ä¾¡æŒ‡æ¨™æ¯”è¼ƒ",
        xaxis_title="æŒ‡æ¨™",
        yaxis_title="å€¤",
        barmode='group'
    )
    
    return fig


def create_comparison_radar_chart(comparison_metrics: Dict[str, Dict[str, float]]) -> go.Figure:
    """æ¯”è¼ƒãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆã®ä½œæˆ"""
    
    fig = go.Figure()
    
    # å…±é€šæŒ‡æ¨™ã®æŠ½å‡º
    all_metrics = set()
    for metrics in comparison_metrics.values():
        all_metrics.update(metrics.keys())
    
    common_metrics = list(all_metrics)[:6]  # æœ€å¤§6ã¤
    
    colors = ['rgb(30, 144, 255)', 'rgb(255, 99, 71)', 'rgb(50, 205, 50)', 
              'rgb(255, 165, 0)', 'rgb(138, 43, 226)', 'rgb(255, 20, 147)']
    
    for i, (name, metrics) in enumerate(comparison_metrics.items()):
        values = [metrics.get(metric, 0) for metric in common_metrics]
        
        # é–‰ã˜ãŸå½¢ã«ã™ã‚‹ãŸã‚æœ€åˆã®å€¤ã‚’è¿½åŠ 
        values_closed = values + [values[0]]
        categories_closed = common_metrics + [common_metrics[0]]
        
        fig.add_trace(go.Scatterpolar(
            r=values_closed,
            theta=categories_closed,
            fill='toself',
            name=name,
            line=dict(color=colors[i % len(colors)])
        ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 1]
            )
        ),
        showlegend=True,
        title="è©•ä¾¡æŒ‡æ¨™æ¯”è¼ƒ (ãƒ¬ãƒ¼ãƒ€ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ)"
    )
    
    return fig