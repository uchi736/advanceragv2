#!/usr/bin/env python3
"""term_clustering_analyzer.py
å°‚é–€ç”¨èªã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æãƒ„ãƒ¼ãƒ«
------------------------------------------
HDBSCANã‚’ä½¿ç”¨ã—ã¦å°‚é–€ç”¨èªã‚’è‡ªå‹•çš„ã«ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
"""

from __future__ import annotations

import json
import logging
import sys
from pathlib import Path
from typing import Dict, List, Optional, Any
import numpy as np
from datetime import datetime

import hdbscan
import umap
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import normalize
from dotenv import load_dotenv
from langchain_openai import AzureOpenAIEmbeddings
from langchain_openai import AzureChatOpenAI
from sqlalchemy import create_engine, text

# Project imports
sys.path.append(str(Path(__file__).resolve().parents[1]))
from rag.config import Config
from rag.prompts import get_synonym_judgment_single_definition_prompt, get_synonym_judgment_with_definitions_prompt

# â”€â”€ ENV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
load_dotenv()
cfg = Config()

PG_URL = f"postgresql+psycopg://{cfg.db_user}:{cfg.db_password}@{cfg.db_host}:{cfg.db_port}/{cfg.db_name}"
JARGON_TABLE_NAME = cfg.jargon_table_name

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")
logger = logging.getLogger(__name__)

# Azure OpenAI Embeddings
embeddings = AzureOpenAIEmbeddings(
    azure_deployment=cfg.azure_openai_embedding_deployment_name,
    api_version=cfg.azure_openai_api_version,
    azure_endpoint=cfg.azure_openai_endpoint,
    api_key=cfg.azure_openai_api_key
)

# Azure OpenAI LLM for naming (4.1-miniä½¿ç”¨)
llm = AzureChatOpenAI(
    azure_deployment=cfg.azure_openai_chat_mini_deployment_name,
    api_version=cfg.azure_openai_api_version,
    azure_endpoint=cfg.azure_openai_endpoint,
    api_key=cfg.azure_openai_api_key,
    temperature=0.1
)

# â”€â”€ Term Clustering Analyzer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
class TermClusteringAnalyzer:
    """å°‚é–€ç”¨èªã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°åˆ†æã‚¯ãƒ©ã‚¹"""

    def __init__(self, connection_string: str, min_terms: int = 3, jargon_table_name: str = None, embeddings=None):
        """
        Args:
            connection_string: PostgreSQLæ¥ç¶šæ–‡å­—åˆ—
            min_terms: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹æœ€å°ç”¨èªæ•°
            jargon_table_name: å°‚é–€ç”¨èªãƒ†ãƒ¼ãƒ–ãƒ«åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯JARGON_TABLE_NAMEï¼‰
            embeddings: AzureOpenAIEmbeddings instanceï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ï¼‰
        """
        self.connection_string = connection_string
        self.min_terms = min_terms
        self.jargon_table_name = jargon_table_name or JARGON_TABLE_NAME
        self.embeddings = embeddings or globals()['embeddings']
        self.terms_data = []
        self.embeddings_matrix = None
        self.clusters = None
        self.clusterer = None
        
    def load_terms_from_db(self) -> List[Dict[str, Any]]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å°‚é–€ç”¨èªã‚’èª­ã¿è¾¼ã¿"""
        engine = create_engine(self.connection_string)
        query = text(f"""
            SELECT term, definition, domain, aliases, related_terms
            FROM {self.jargon_table_name}
            ORDER BY term
        """)

        terms = []
        try:
            with engine.connect() as conn:
                result = conn.execute(query)
                for row in result:
                    terms.append({
                        'term': row.term,
                        'definition': row.definition,
                        'domain': row.domain,
                        'aliases': row.aliases or [],
                        'related_terms': row.related_terms or [],
                        'text_for_embedding': f"{row.term}: {row.definition}"
                    })
            logger.info(f"Loaded {len(terms)} terms from database")
        except Exception as e:
            logger.error(f"Error loading terms: {e}")
            
        self.terms_data = terms
        return terms
    
    def generate_embeddings(self) -> np.ndarray:
        """ç”¨èª+å®šç¾©ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç”Ÿæˆ"""
        if not self.terms_data:
            logger.warning("No terms loaded")
            return np.array([])
        
        texts = [t['text_for_embedding'] for t in self.terms_data]
        
        logger.info(f"Generating embeddings for {len(texts)} terms...")
        try:
            embeddings_list = embeddings.embed_documents(texts)
            self.embeddings_matrix = np.array(embeddings_list)
            logger.info(f"Generated embeddings with shape: {self.embeddings_matrix.shape}")
            return self.embeddings_matrix
        except Exception as e:
            logger.error(f"Error generating embeddings: {e}")
            return np.array([])
    
    def perform_clustering(self, min_cluster_size: int = 3) -> Dict[str, Any]:
        """HDBSCANã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ"""
        if self.embeddings_matrix is None or len(self.embeddings_matrix) == 0:
            logger.warning("No embeddings available for clustering")
            return {}
        
        # ç”¨èªæ•°ãƒã‚§ãƒƒã‚¯
        if len(self.terms_data) < self.min_terms:
            logger.warning(f"Not enough terms for meaningful clustering. Have {len(self.terms_data)}, need at least {self.min_terms}")
            return {
                'status': 'skipped',
                'reason': f'Insufficient terms (have {len(self.terms_data)}, need {self.min_terms})',
                'terms_count': len(self.terms_data)
            }
        
        # ãƒ™ã‚¯ãƒˆãƒ«ã‚’æ­£è¦åŒ–ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®ãŸã‚ï¼‰
        normalized_embeddings = normalize(self.embeddings_matrix, norm='l2')
        
        # UMAPæ¬¡å…ƒåœ§ç¸®
        logger.info(f"Applying UMAP dimensional reduction: {normalized_embeddings.shape[1]} -> 20 dimensions")
        umap_reducer = umap.UMAP(
            n_components=20,  # 20æ¬¡å…ƒã«åœ§ç¸®
            n_neighbors=15,  # è¿‘å‚ã‚µãƒ³ãƒ—ãƒ«æ•°
            min_dist=0.1,  # ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®å¯†åº¦åˆ¶å¾¡
            metric='cosine',  # ã‚³ã‚µã‚¤ãƒ³è·é›¢
            random_state=42
        )
        reduced_embeddings = umap_reducer.fit_transform(normalized_embeddings)
        logger.info(f"UMAP reduction complete: shape {reduced_embeddings.shape}")
        
        # HDBSCANå®Ÿè¡Œï¼ˆæ”¹å–„ã•ã‚ŒãŸè¨­å®šï¼‰
        self.clusterer = hdbscan.HDBSCAN(
            min_cluster_size=2,  # æœ€å°ã‚¯ãƒ©ã‚¹ã‚¿ã‚µã‚¤ã‚º
            min_samples=1,
            cluster_selection_epsilon=0.3,  # ã‚¯ãƒ©ã‚¹ã‚¿é¸æŠã®æŸ”è»Ÿæ€§
            cluster_selection_method='leaf',  # ã‚ˆã‚Šå¤šãã®ç‚¹ã‚’å«ã‚€
            metric='euclidean',  # åœ§ç¸®å¾Œã®ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢
            allow_single_cluster=True,
            prediction_data=True
        )
        
        self.clusters = self.clusterer.fit_predict(reduced_embeddings)
        
        # åœ§ç¸®å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚‚ä¿å­˜ï¼ˆå¯è¦–åŒ–ç”¨ï¼‰
        self.reduced_embeddings = reduced_embeddings
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã®çµ±è¨ˆ
        unique_clusters = set(self.clusters)
        n_clusters = len([c for c in unique_clusters if c >= 0])
        n_noise = sum(1 for c in self.clusters if c == -1)
        
        logger.info(f"Found {n_clusters} clusters, {n_noise} noise points")
        
        # ã‚·ãƒ«ã‚¨ãƒƒãƒˆä¿‚æ•°è¨ˆç®—ï¼ˆãƒã‚¤ã‚ºç‚¹ã‚’é™¤å¤–ã€åœ§ç¸®å¾Œã®ãƒ‡ãƒ¼ã‚¿ã§è¨ˆç®—ï¼‰
        silhouette = None
        if n_clusters >= 2:
            mask = self.clusters >= 0
            if sum(mask) >= 2:
                silhouette = silhouette_score(
                    self.reduced_embeddings[mask], 
                    self.clusters[mask]
                )
                logger.info(f"Silhouette score: {silhouette:.3f}")
        
        return {
            'n_clusters': n_clusters,
            'n_noise': n_noise,
            'silhouette_score': silhouette,
            'cluster_labels': self.clusters.tolist()
        }
    
    async def name_clusters_with_llm(self, cluster_terms: Dict[int, List[str]]) -> Dict[int, str]:
        """LLMã‚’ä½¿ç”¨ã—ã¦ã‚¯ãƒ©ã‚¹ã‚¿ã«åå‰ã‚’ä»˜ã‘ã‚‹"""
        cluster_names = {}
        
        for cluster_id, terms in cluster_terms.items():
            if cluster_id == -1:  # ãƒã‚¤ã‚ºã‚¯ãƒ©ã‚¹ã‚¿ã¯ã‚¹ã‚­ãƒƒãƒ—
                continue
            
            # ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®ç”¨èªã¨å®šç¾©ã‚’æº–å‚™
            cluster_info = []
            for term in terms[:10]:  # æœ€å¤§10å€‹ã®ä»£è¡¨çš„ãªç”¨èª
                term_data = next(t for t in self.terms_data if t['term'] == term)
                cluster_info.append(f"- {term}: {term_data['definition'][:100]}")
            
            prompt = f"""
ä»¥ä¸‹ã®å°‚é–€ç”¨èªã‚°ãƒ«ãƒ¼ãƒ—ã«é©åˆ‡ãªã‚«ãƒ†ã‚´ãƒªåã‚’ä»˜ã‘ã¦ãã ã•ã„ã€‚
ã‚«ãƒ†ã‚´ãƒªåã¯çŸ­ãï¼ˆ1-3èªï¼‰ã€æ—¥æœ¬èªã§ã€æŠ€è¡“åˆ†é‡ã‚’è¡¨ã™ã‚‚ã®ã«ã—ã¦ãã ã•ã„ã€‚

ç”¨èªã‚°ãƒ«ãƒ¼ãƒ—:
{chr(10).join(cluster_info)}

ã‚«ãƒ†ã‚´ãƒªåã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„:
"""
            
            try:
                response = await llm.ainvoke(prompt)
                cluster_names[cluster_id] = response.content.strip()
                logger.info(f"Cluster {cluster_id} named: {cluster_names[cluster_id]}")
            except Exception as e:
                logger.error(f"Error naming cluster {cluster_id}: {e}")
                cluster_names[cluster_id] = f"ã‚¯ãƒ©ã‚¹ã‚¿{cluster_id}"
        
        return cluster_names

    async def llm_judge_candidate_synonym(
        self,
        spec_term: str,
        spec_def: str,
        candidate_term: str
    ) -> bool:
        """
        LLMã§å€™è£œç”¨èªãŒé¡ç¾©èªã‹ã©ã†ã‹ã‚’åˆ¤å®šï¼ˆå®šç¾©ãªã—å€™è£œç”¨èªå‘ã‘ï¼‰

        Args:
            spec_term: å°‚é–€ç”¨èª
            spec_def: å°‚é–€ç”¨èªã®å®šç¾©
            candidate_term: å€™è£œç”¨èª

        Returns:
            True: é¡ç¾©èª, False: éé¡ç¾©èªï¼ˆåŒ…å«é–¢ä¿‚ãƒ»é–¢é€£èªï¼‰
        """
        import json

        prompt_template = get_synonym_judgment_single_definition_prompt()

        try:
            response = await llm.ainvoke(
                prompt_template.format(
                    spec_term=spec_term,
                    spec_def=spec_def,
                    candidate_term=candidate_term
                )
            )

            # JSONè§£æ
            content = response.content.strip()
            # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()

            result = json.loads(content)
            is_synonym = result.get("is_synonym", True)
            reason = result.get("reason", "")

            logger.debug(f"LLMåˆ¤å®š: '{spec_term}' â†” '{candidate_term}': {is_synonym} ({reason})")
            return is_synonym

        except Exception as e:
            logger.warning(f"LLMåˆ¤å®šå¤±æ•— '{spec_term}' â†” '{candidate_term}': {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä¿å®ˆçš„ã«é¡ç¾©èªã¨åˆ¤å®šï¼ˆå½é™°æ€§ã‚’é¿ã‘ã‚‹ï¼‰
            return True

    async def llm_judge_synonyms_bulk(
        self,
        term: str,
        definition: str,
        candidates: List[Dict[str, Any]],
        specialized_terms: List[Dict[str, Any]],
        spec_count: int
    ) -> List[int]:
        """
        LLMã§1ã¤ã®å°‚é–€ç”¨èªã«å¯¾ã—ã¦è¤‡æ•°ã®å€™è£œç”¨èªã‚’ã¾ã¨ã‚ã¦é¡ç¾©èªåˆ¤å®šï¼ˆãƒãƒ«ã‚¯å‡¦ç†ï¼‰

        Args:
            term: å°‚é–€ç”¨èª
            definition: å°‚é–€ç”¨èªã®å®šç¾©
            candidates: å€™è£œç”¨èªãƒªã‚¹ãƒˆ [{"term": "å€™è£œ1", "similarity": 0.9, "is_specialized": True}]
            specialized_terms: å…¨å°‚é–€ç”¨èªãƒªã‚¹ãƒˆï¼ˆå®šç¾©å–å¾—ç”¨ï¼‰
            spec_count: å°‚é–€ç”¨èªã®æ•°

        Returns:
            é¡ç¾©èªã¨åˆ¤å®šã•ã‚ŒãŸå€™è£œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚¹ãƒˆ [0, 2, 5]
        """
        import json
        import re

        # å€™è£œãƒªã‚¹ãƒˆã‚’ç•ªå·ä»˜ãã§ä½œæˆ
        candidate_list = []
        for i, cand in enumerate(candidates):
            cand_term = cand['term']
            is_specialized = cand.get('is_specialized', False)

            # å®šç¾©å–å¾—
            if is_specialized:
                cand_def = next(
                    (t.get('definition', '') for t in specialized_terms if t['term'] == cand_term),
                    ''
                )
            else:
                # å€™è£œç”¨èªã¯å®šç¾©ãªã—
                cand_def = ''

            cand_def_str = cand_def if cand_def else "ï¼ˆå®šç¾©ãªã—ï¼‰"
            candidate_list.append(f"{i+1}. {cand_term}: {cand_def_str}")

        candidates_text = "\n".join(candidate_list)

        prompt = f"""å°‚é–€ç”¨èª: {term}
å®šç¾©: {definition or "ï¼ˆå®šç¾©ãªã—ï¼‰"}

ä»¥ä¸‹ã®å€™è£œç”¨èªã®ä¸­ã§ã€ä¸Šè¨˜ã®å°‚é–€ç”¨èªã®é¡ç¾©èªã‚’å…¨ã¦é¸ã‚“ã§ãã ã•ã„ã€‚

ã€é¡ç¾©èªã®åˆ¤å®šåŸºæº–ã€‘
- ã»ã¼åŒã˜æ„å‘³ã‚’æŒã¤ç”¨èªï¼ˆåŒç¾©èªï¼‰
- æ‹¬å¼§æ›¸ãã§ç¤ºã•ã‚Œã‚‹åˆ¥åè¡¨è¨˜ï¼ˆä¾‹: ã€Œã‚¬ã‚¹è»¸å—ï¼ˆç©ºæ°—è»¸å—ï¼‰ã€ã®å ´åˆã€ã€Œç©ºæ°—è»¸å—ã€ã¯é¡ç¾©èªï¼‰
- ç•°ãªã‚‹å‘¼ã³æ–¹ã ãŒå®Ÿè³ªçš„ã«åŒã˜ã‚‚ã®ã‚’æŒ‡ã™ç”¨èª

ã€é™¤å¤–ã™ã‚‹ç”¨èªã€‘
- åŒ…å«é–¢ä¿‚ï¼ˆä¸€æ–¹ãŒä»–æ–¹ã®ä¸€éƒ¨ã‚„ä¸Šä½æ¦‚å¿µï¼‰
- é–¢é€£èªï¼ˆé–¢é€£ã¯ã‚ã‚‹ãŒåŒã˜ã‚‚ã®ã§ã¯ãªã„ï¼‰

å€™è£œç”¨èª:
{candidates_text}

é¡ç¾©èªã®ç•ªå·ã®ã¿ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¿”ã—ã¦ãã ã•ã„ï¼ˆä¾‹: 1,3,5ï¼‰
é¡ç¾©èªãŒãªã„å ´åˆã¯ã€Œãªã—ã€ã¨è¿”ã—ã¦ãã ã•ã„ã€‚
ç•ªå·ä»¥å¤–ã®èª¬æ˜ã¯ä¸è¦ã§ã™ã€‚"""

        try:
            response = await llm.ainvoke(prompt)
            content = response.content.strip()

            # ãƒ‘ãƒ¼ã‚¹å‡¦ç†
            if content in ["ãªã—", "ç„¡ã—", "None", "none", ""]:
                return []

            # ç•ªå·ã‚’æŠ½å‡ºï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã¾ãŸã¯ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šï¼‰
            numbers = re.findall(r'\d+', content)
            synonym_indices = [int(num) - 1 for num in numbers if 0 < int(num) <= len(candidates)]

            logger.debug(f"LLMåˆ¤å®šï¼ˆãƒãƒ«ã‚¯ï¼‰: '{term}' â†’ {len(synonym_indices)}/{len(candidates)}ä»¶ãŒé¡ç¾©èª")
            return synonym_indices

        except Exception as e:
            logger.warning(f"LLMãƒãƒ«ã‚¯åˆ¤å®šå¤±æ•— '{term}': {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ç©ºãƒªã‚¹ãƒˆï¼ˆä¿å®ˆçš„ã«é¡ç¾©èªãªã—ã¨ã™ã‚‹ï¼‰
            return []

    async def llm_judge_synonym_with_definitions(
        self,
        term1: str,
        def1: str,
        term2: str,
        def2: str
    ) -> bool:
        """
        LLMã§2ã¤ã®ç”¨èªãŒé¡ç¾©èªã‹ã©ã†ã‹ã‚’åˆ¤å®šï¼ˆä¸¡æ–¹ã®å®šç¾©ã‚ã‚Šï¼‰
        â€»ã“ã®é–¢æ•°ã¯å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã«æ®‹ã™ãŒã€ãƒãƒ«ã‚¯å‡¦ç†ã®æ–¹ãŒåŠ¹ç‡çš„

        Args:
            term1: ç”¨èª1
            def1: ç”¨èª1ã®å®šç¾©
            term2: ç”¨èª2
            def2: ç”¨èª2ã®å®šç¾©

        Returns:
            True: é¡ç¾©èª, False: éé¡ç¾©èªï¼ˆåŒ…å«é–¢ä¿‚ãƒ»é–¢é€£èªï¼‰
        """
        import json

        prompt_template = get_synonym_judgment_with_definitions_prompt()

        try:
            response = await llm.ainvoke(
                prompt_template.format(
                    term1=term1,
                    def1=def1 or "ï¼ˆå®šç¾©ãªã—ï¼‰",
                    term2=term2,
                    def2=def2 or "ï¼ˆå®šç¾©ãªã—ï¼‰"
                )
            )

            # JSONè§£æ
            content = response.content.strip()
            # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’é™¤å»
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()

            result = json.loads(content)
            is_synonym = result.get("is_synonym", True)
            reason = result.get("reason", "")

            logger.debug(f"LLMåˆ¤å®š: '{term1}' â†” '{term2}': {is_synonym} ({reason})")
            return is_synonym

        except Exception as e:
            logger.warning(f"LLMåˆ¤å®šå¤±æ•— '{term1}' â†” '{term2}': {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ä¿å®ˆçš„ã«é¡ç¾©èªã¨åˆ¤å®šï¼ˆå½é™°æ€§ã‚’é¿ã‘ã‚‹ï¼‰
            return True

    async def extract_semantic_synonyms_hybrid(
        self,
        specialized_terms: List[Dict[str, Any]],
        candidate_terms: List[Dict[str, Any]],
        similarity_threshold: float = None,  # Deprecated: äº’æ›æ€§ã®ãŸã‚æ®‹ã™ãŒæœªä½¿ç”¨
        max_synonyms: int = 5,
        use_llm_naming: bool = True,
        use_llm_for_candidates: bool = True
    ) -> Dict[str, Any]:
        """
        2æ®µéšã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã«ã‚ˆã‚‹æ„å‘³ãƒ™ãƒ¼ã‚¹é¡ç¾©èªæŠ½å‡º

        ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: HDBSCANã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚° + LLMåˆ¤å®šã®ã¿
        (ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ã¯å‰Šé™¤: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãªã®ã§å†—é•·)

        Args:
            specialized_terms: å°‚é–€ç”¨èªãƒªã‚¹ãƒˆ [{"term": "ETC", "definition": "...", "text": "ETC: ..."}]
            candidate_terms: å€™è£œç”¨èªãƒªã‚¹ãƒˆ [{"term": "éçµ¦æ©Ÿ", "text": "éçµ¦æ©Ÿ"}]
            similarity_threshold: éæ¨å¥¨ï¼ˆäº’æ›æ€§ã®ãŸã‚æ®‹å­˜ã€æœªä½¿ç”¨ï¼‰
            max_synonyms: å„ç”¨èªã®æœ€å¤§é¡ç¾©èªæ•°
            use_llm_naming: LLMã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹ã‚¿å‘½åã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹

        Returns:
            {
                "synonyms": {
                    "ETC": [
                        {"term": "é›»å‹•ã‚¿ãƒ¼ãƒœãƒãƒ£ãƒ¼ã‚¸ãƒ£", "similarity": 0.92, "is_specialized": True},
                        {"term": "éçµ¦æ©Ÿ", "similarity": 0.87, "is_specialized": False}
                    ]
                },
                "clusters": {"ETC": 0, "éçµ¦æ©Ÿ": 0},
                "cluster_names": {0: "è»¸å—æŠ€è¡“", 1: "é›»å‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ "}
            }
        """
        # äº’æ›æ€§ã®ãŸã‚ã®è­¦å‘Š
        if similarity_threshold is not None:
            logger.warning(f"similarity_threshold={similarity_threshold} is deprecated and ignored. "
                          "Filtering now uses HDBSCAN clustering + LLM judgment only.")
        logger.info(f"Starting hybrid semantic synonym extraction: {len(specialized_terms)} specialized, {len(candidate_terms)} candidates")

        # terms_dataã«å°‚é–€ç”¨èªã‚’ä¿å­˜ï¼ˆLLMå‘½åç”¨ï¼‰
        self.terms_data = specialized_terms

        # 1. ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆï¼ˆçµ±ä¸€å½¢å¼: term + context/definitionï¼‰
        logger.info("Generating embeddings for specialized terms (term + definition)...")
        spec_texts = [t['text'] for t in specialized_terms]
        spec_embeddings_list = self.embeddings.embed_documents(spec_texts)
        spec_embeddings = np.array(spec_embeddings_list)

        # å€™è£œç”¨èª: 'text'ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½¿ç”¨ï¼ˆå‘¨è¾ºãƒ†ã‚­ã‚¹ãƒˆ or ç”¨èªã®ã¿ï¼‰
        logger.info("Generating embeddings for candidate terms (term + context)...")
        cand_texts = [t.get('text', t['term']) for t in candidate_terms]
        cand_embeddings_list = self.embeddings.embed_documents(cand_texts)
        cand_embeddings = np.array(cand_embeddings_list)

        # 2. çµ±åˆ
        all_embeddings = np.vstack([spec_embeddings, cand_embeddings])
        all_terms = specialized_terms + candidate_terms
        logger.info(f"Combined embeddings shape: {all_embeddings.shape}")

        # 3. æ­£è¦åŒ–ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ç”¨ï¼‰
        normalized_embeddings = normalize(all_embeddings, norm='l2')

        # 4. UMAPæ¬¡å…ƒåœ§ç¸®
        logger.info("Applying UMAP dimensional reduction...")
        # n_componentsã¯ãƒ‡ãƒ¼ã‚¿æ•°ã‚ˆã‚Šå°ã•ãã™ã‚‹å¿…è¦ãŒã‚ã‚‹
        n_samples = len(all_embeddings)
        n_components = min(20, max(2, n_samples // 2))  # ãƒ‡ãƒ¼ã‚¿æ•°ã®åŠåˆ†ä»¥ä¸‹ã€æœ€ä½2
        n_neighbors = min(15, max(2, n_samples // 3))   # ãƒ‡ãƒ¼ã‚¿æ•°ã®1/3ä»¥ä¸‹ã€æœ€ä½2

        logger.info(f"UMAP params: n_samples={n_samples}, n_components={n_components}, n_neighbors={n_neighbors}")

        umap_reducer = umap.UMAP(
            n_components=n_components,
            n_neighbors=n_neighbors,
            min_dist=0.1,
            metric='cosine',
            random_state=42
        )
        reduced_embeddings = umap_reducer.fit_transform(normalized_embeddings)
        logger.info(f"UMAP reduction complete: shape {reduced_embeddings.shape}")

        # 5. HDBSCANã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
        logger.info("Performing HDBSCAN clustering...")
        # min_cluster_sizeã‚’å‹•çš„ã«èª¿æ•´: ãƒ‡ãƒ¼ã‚¿æ•°ã®3%ä»¥ä¸Šã€æœ€ä½2
        min_cluster_size = max(2, int(n_samples * 0.03))
        logger.info(f"HDBSCAN min_cluster_size: {min_cluster_size}")
        clusterer = hdbscan.HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=1,
            cluster_selection_epsilon=0.5,
            cluster_selection_method='leaf',
            metric='euclidean',
            allow_single_cluster=True,
            prediction_data=True
        )
        clusters = clusterer.fit_predict(reduced_embeddings)

        n_clusters = len([c for c in set(clusters) if c >= 0])
        n_noise = sum(1 for c in clusters if c == -1)
        logger.info(f"Clustering complete: {n_clusters} clusters, {n_noise} noise points")

        # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã®è©³ç´°ãƒ­ã‚°ã‚’å‡ºåŠ›
        logger.info("\n" + "="*60)
        logger.info("Clustering Details:")
        logger.info("="*60)
        spec_count = len(specialized_terms)

        # å°‚é–€ç”¨èªã®ã‚¯ãƒ©ã‚¹ã‚¿é…ç½®ã‚’è¡¨ç¤º
        for cluster_id in sorted(set(clusters)):
            cluster_terms = []
            for idx, c_id in enumerate(clusters):
                if c_id == cluster_id:
                    term_obj = all_terms[idx]
                    term_name = term_obj['term']
                    is_spec = idx < spec_count
                    marker = "â˜…" if is_spec else "â—‹"
                    cluster_terms.append(f"{marker}{term_name}")

            cluster_label = f"Cluster {cluster_id}" if cluster_id >= 0 else "Noise"
            logger.info(f"\n{cluster_label} ({len(cluster_terms)} terms):")
            logger.info("  " + ", ".join(cluster_terms))

        logger.info("\n" + "="*60)

        # 6. å°‚é–€ç”¨èªã”ã¨ã«é¡ç¾©èªæŠ½å‡ºã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒãƒƒãƒ”ãƒ³ã‚°
        synonyms_dict = {}
        cluster_mapping = {}  # term -> cluster_id ã®ãƒãƒƒãƒ”ãƒ³ã‚°

        # å…¨å°‚é–€ç”¨èªã®é¡ç¾©èªå€™è£œã‚’ä¸€åº¦ã«åé›†ï¼ˆãƒãƒ«ã‚¯å‡¦ç†ç”¨ï¼‰
        all_llm_tasks = []
        all_task_metadata = []
        term_to_candidates = {}  # term_name -> [(sim_item, task_idx), ...]

        for idx in range(spec_count):
            spec_term = specialized_terms[idx]
            term_name = spec_term['term']
            cluster_id = clusters[idx]

            # ã‚¯ãƒ©ã‚¹ã‚¿IDã‚’è¨˜éŒ²ï¼ˆãƒã‚¤ã‚ºã‚¯ãƒ©ã‚¹ã‚¿ã‚‚å«ã‚€ï¼‰
            cluster_mapping[term_name] = int(cluster_id)

            # ãƒã‚¤ã‚ºã‚¯ãƒ©ã‚¹ã‚¿ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆé¡ç¾©èªæŠ½å‡ºã®ã¿ï¼‰
            if cluster_id == -1:
                logger.debug(f"Term '{term_name}' is in noise cluster, skipping synonym extraction")
                continue

            # åŒä¸€ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®ä»–ã®ç”¨èªã‚’æ¤œç´¢
            same_cluster_indices = [
                i for i, c in enumerate(clusters)
                if c == cluster_id and i != idx
            ]

            if not same_cluster_indices:
                continue

            # é¡ä¼¼åº¦è¨ˆç®—ï¼ˆè¨˜éŒ²ç”¨ã®ã¿ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã«ã¯ä½¿ç”¨ã—ãªã„ï¼‰
            term_embedding = normalized_embeddings[idx]
            similarities = []

            for other_idx in same_cluster_indices:
                other_embedding = normalized_embeddings[other_idx]
                # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆè¨˜éŒ²ã®ã¿ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãªã—ï¼‰
                similarity = float(np.dot(term_embedding, other_embedding))

                other_term = all_terms[other_idx]
                other_term_name = other_term['term']
                is_specialized = other_idx < spec_count

                # è‡ªåˆ†è‡ªèº«ã¯é™¤å¤–
                if other_term_name == term_name:
                    continue

                # åŒ…å«é–¢ä¿‚ï¼ˆrelated_termsï¼‰ã«å«ã¾ã‚Œã‚‹ç”¨èªã¯é¡ç¾©èªã‹ã‚‰é™¤å¤–
                related_terms = spec_term.get('related_terms', [])
                if other_term_name in related_terms:
                    logger.debug(f"Skipping '{other_term_name}' for '{term_name}': in related_terms (inclusion relationship)")
                    continue

                # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãƒ•ã‚£ãƒ«ã‚¿ã‚’å‰Šé™¤: ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ãªã®ã§å…¨ã¦å€™è£œã«è¿½åŠ 
                similarities.append({
                    'term': other_term_name,
                    'similarity': similarity,  # è¨˜éŒ²ã®ã¿
                    'is_specialized': is_specialized
                })

            # LLMåˆ¤å®šç”¨ã‚¿ã‚¹ã‚¯ã‚’åé›†ï¼ˆãƒãƒ«ã‚¯åˆ¤å®š: 1å°‚é–€ç”¨èªã«ã¤ã1ã‚¿ã‚¹ã‚¯ï¼‰
            if use_llm_for_candidates and similarities:
                spec_def = spec_term.get('definition', '')

                # å…¨å€™è£œã‚’ã¾ã¨ã‚ã¦1å›ã®LLMå‘¼ã³å‡ºã—ã§åˆ¤å®šã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
                task = self.llm_judge_synonyms_bulk(
                    term_name,
                    spec_def,
                    similarities,
                    specialized_terms,
                    spec_count
                )
                all_llm_tasks.append(task)
                all_task_metadata.append((term_name, similarities))

        # å…¨LLMåˆ¤å®šã‚’ã‚»ãƒãƒ•ã‚©ã§åˆ¶é™ä»˜ãä¸¦åˆ—å®Ÿè¡Œï¼ˆTPM/RPMåˆ¶é™å¯¾ç­–ï¼‰
        if use_llm_for_candidates and all_llm_tasks:
            import asyncio

            # åŒæ™‚å®Ÿè¡Œæ•°åˆ¶é™ã‚’è¨­å®šã‹ã‚‰å–å¾—
            max_concurrent = getattr(cfg, 'max_concurrent_llm_requests', 10)

            logger.info(f"Running LLM judgment: {len(all_llm_tasks)} tasks (max {max_concurrent} concurrent)")

            # ã‚»ãƒãƒ•ã‚©ã‚’ä½¿ç”¨ã—ã¦åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶é™
            semaphore = asyncio.Semaphore(max_concurrent)

            async def run_with_semaphore(task):
                async with semaphore:
                    return await task

            # å…¨ã‚¿ã‚¹ã‚¯ã‚’ã‚»ãƒãƒ•ã‚©ä»˜ãã§å®Ÿè¡Œ
            results = await asyncio.gather(
                *[run_with_semaphore(task) for task in all_llm_tasks],
                return_exceptions=True
            )

            # çµæœã‚’å„å°‚é–€ç”¨èªã«æŒ¯ã‚Šåˆ†ã‘
            for (term_name, similarities), result in zip(all_task_metadata, results):
                if isinstance(result, Exception):
                    logger.warning(f"LLM bulk judgment failed for '{term_name}': {result}")
                    term_to_candidates[term_name] = []
                    continue

                # resultã¯é¡ç¾©èªã¨åˆ¤å®šã•ã‚ŒãŸå€™è£œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚¹ãƒˆ
                synonym_indices = result
                llm_filtered_similarities = [similarities[idx] for idx in synonym_indices if idx < len(similarities)]

                # çµæœã‚’ä¿å­˜ï¼ˆå¾Œã§max_synonymsé©ç”¨ï¼‰
                term_to_candidates[term_name] = llm_filtered_similarities
                logger.debug(f"After LLM filtering for '{term_name}': {len(llm_filtered_similarities)}/{len(similarities)} synonyms remain")

        # LLMåˆ¤å®šçµæœã‚’synonyms_dictã«åæ˜ 
        for term_name, filtered_similarities in term_to_candidates.items():
            # é¡ä¼¼åº¦é †ã«ã‚½ãƒ¼ãƒˆ
            filtered_similarities.sort(key=lambda x: x['similarity'], reverse=True)

            # ä¸Šä½Nå€‹ã®ã¿ä¿å­˜
            if filtered_similarities:
                synonyms_dict[term_name] = filtered_similarities[:max_synonyms]
                logger.debug(f"Found {len(filtered_similarities[:max_synonyms])} synonyms for '{term_name}'")

        logger.info(f"Extracted semantic synonyms for {len(synonyms_dict)} terms")
        logger.info(f"Mapped {len(cluster_mapping)} terms to clusters")

        # LLMã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹ã‚¿å‘½åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        cluster_names = {}
        if use_llm_naming:
            try:
                # ã‚¯ãƒ©ã‚¹ã‚¿IDã”ã¨ã«ç”¨èªã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
                cluster_terms_map = {}
                for idx, cluster_id in enumerate(clusters[:spec_count]):
                    if cluster_id >= 0:  # ãƒã‚¤ã‚ºã‚¯ãƒ©ã‚¹ã‚¿ã‚’é™¤å¤–
                        if cluster_id not in cluster_terms_map:
                            cluster_terms_map[cluster_id] = []
                        cluster_terms_map[cluster_id].append(specialized_terms[idx]['term'])

                logger.info(f"Naming {len(cluster_terms_map)} clusters with LLM...")
                cluster_names = await self.name_clusters_with_llm(cluster_terms_map)
                logger.info(f"LLM naming complete: {cluster_names}")
            except Exception as e:
                logger.error(f"LLM cluster naming failed: {e}. Using default names.", exc_info=True)
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¯ãƒ©ã‚¹ã‚¿N
                unique_clusters = set(c for c in cluster_mapping.values() if c >= 0)
                cluster_names = {cid: f"ã‚¯ãƒ©ã‚¹ã‚¿{cid}" for cid in unique_clusters}

        # ã‚¯ãƒ©ã‚¹ã‚¿æƒ…å ±ã¨ã‚¯ãƒ©ã‚¹ã‚¿åã‚’å«ã‚€çµæœã‚’è¿”ã™
        return {
            'synonyms': synonyms_dict,
            'clusters': cluster_mapping,
            'cluster_names': cluster_names
        }

    def _ensure_bidirectional_synonyms(
        self,
        synonyms_dict: Dict[str, List[Dict[str, Any]]]
    ) -> Dict[str, List[Dict[str, Any]]]:
        """
        é¡ç¾©èªé–¢ä¿‚ã®åŒæ–¹å‘æ€§ã‚’ä¿è¨¼ã™ã‚‹

        Aâ†’Bã®é¡ç¾©èªé–¢ä¿‚ãŒã‚ã‚‹å ´åˆã€Bâ†’Aã‚‚è¿½åŠ ã™ã‚‹
        ä¾‹: "ETC" â†’ ["é›»å‹•ã‚¿ãƒ¼ãƒœãƒãƒ£ãƒ¼ã‚¸ãƒ£"] ã®å ´åˆã€
            "é›»å‹•ã‚¿ãƒ¼ãƒœãƒãƒ£ãƒ¼ã‚¸ãƒ£" â†’ ["ETC"] ã‚‚è¿½åŠ 

        Args:
            synonyms_dict: å…ƒã®é¡ç¾©èªè¾æ›¸ {term: [{"term": ..., "similarity": ...}]}

        Returns:
            åŒæ–¹å‘æ€§ã‚’ä¿è¨¼ã—ãŸé¡ç¾©èªè¾æ›¸
        """
        from collections import defaultdict

        # å…¨ã¦ã®é¡ç¾©èªé–¢ä¿‚ã‚’åé›†
        synonym_pairs = defaultdict(dict)  # {term1: {term2: similarity, ...}}

        for term1, synonyms in synonyms_dict.items():
            for syn_info in synonyms:
                term2 = syn_info['term']
                similarity = syn_info.get('similarity', 0.85)
                is_specialized = syn_info.get('is_specialized', False)

                # åŒæ–¹å‘ã«ç™»éŒ²
                synonym_pairs[term1][term2] = {
                    'similarity': similarity,
                    'is_specialized': is_specialized
                }
                # é€†æ–¹å‘ã‚‚ç™»éŒ²ï¼ˆå­˜åœ¨ã—ãªã„å ´åˆã®ã¿ï¼‰
                if term2 not in synonym_pairs or term1 not in synonym_pairs[term2]:
                    synonym_pairs[term2][term1] = {
                        'similarity': similarity,
                        'is_specialized': is_specialized
                    }

        # Dict[str, List[Dict]]å½¢å¼ã«å¤‰æ›
        bidirectional_dict = {}
        for term, syn_map in synonym_pairs.items():
            bidirectional_dict[term] = [
                {
                    'term': syn_term,
                    'similarity': info['similarity'],
                    'is_specialized': info['is_specialized']
                }
                for syn_term, info in syn_map.items()
            ]

        logger.info(f"Ensured bidirectional synonyms: {len(synonyms_dict)} â†’ {len(bidirectional_dict)} terms")
        return bidirectional_dict

    def update_semantic_synonyms_to_db(
        self,
        synonyms_dict: Dict[str, List[Dict[str, Any]]],
        cluster_mapping: Dict[str, int] = None,
        cluster_names: Dict[int, str] = None,
        collection_name: str = None
    ):
        """
        æŠ½å‡ºã—ãŸæ„å‘³çš„é¡ç¾©èªã¨ã‚¯ãƒ©ã‚¹ã‚¿æƒ…å ±ã‚’DBã«ä¿å­˜

        é‡è¦: cluster_mappingã®å…¨ç”¨èªã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã€é¡ç¾©èªãŒãªã„ç”¨èªã‚‚domainãŒæ›´æ–°ã•ã‚Œã‚‹

        Args:
            synonyms_dict: é¡ç¾©èªè¾æ›¸ {term: [{"term": ..., "similarity": ...}]}
            cluster_mapping: ã‚¯ãƒ©ã‚¹ã‚¿ãƒãƒƒãƒ”ãƒ³ã‚° {term: cluster_id} â€»å…¨ç”¨èªã‚’å«ã‚€
            cluster_names: ã‚¯ãƒ©ã‚¹ã‚¿åãƒãƒƒãƒ”ãƒ³ã‚° {cluster_id: "è»¸å—æŠ€è¡“"}
            collection_name: ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³åï¼ˆæŒ‡å®šã•ã‚Œãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ï¼‰
        """
        if not cluster_mapping:
            logger.warning("No cluster_mapping provided, skipping domain update")
            return 0

        # collection_nameãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
        if collection_name is None:
            from rag.config import Config
            cfg = Config()
            collection_name = cfg.collection_name
            logger.warning(f"âš ï¸ collection_name not provided, using default: '{collection_name}'")
        else:
            logger.info(f"âœ“ Using provided collection_name: '{collection_name}'")

        engine = create_engine(self.connection_string)

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çŠ¶æ…‹ç¢ºèªï¼ˆUPDATEå‰ï¼‰
        with engine.connect() as conn:
            check_result = conn.execute(
                text(f"SELECT COUNT(*) FROM {self.jargon_table_name} WHERE collection_name = :cname"),
                {"cname": collection_name}
            ).scalar()
            logger.info(f"ğŸ“Š Target collection '{collection_name}' has {check_result} terms in database before update")

            if check_result == 0:
                logger.error(f"âŒ No terms found in collection '{collection_name}' - UPDATE will fail!")
                return 0

        updated_count = 0
        synonyms_count = 0
        no_synonyms_count = 0
        noise_count = 0

        # é¡ç¾©èªã®åŒæ–¹å‘æ€§ã‚’ä¿è¨¼: Aâ†’Bã®é–¢ä¿‚ãŒã‚ã‚Œã°Bâ†’Aã‚‚è¿½åŠ 
        bidirectional_synonyms = self._ensure_bidirectional_synonyms(synonyms_dict)

        with engine.begin() as conn:
            # cluster_mappingã®å…¨ç”¨èªã‚’ãƒ«ãƒ¼ãƒ—ï¼ˆé¡ç¾©èªãªã—ã§ã‚‚domainæ›´æ–°ï¼‰
            for term, cluster_id in cluster_mapping.items():
                # 1. é¡ç¾©èªã‚’å–å¾—ï¼ˆåŒæ–¹å‘æ€§ä¿è¨¼æ¸ˆã¿ï¼‰
                synonyms = bidirectional_synonyms.get(term, [])
                synonym_terms = [s['term'] for s in synonyms]

                # 2. domainã‚’æ±ºå®šï¼ˆå…¨ç”¨èªã«å¿…ãšå€¤ã‚’è¨­å®šï¼‰
                if cluster_id >= 0:
                    # LLMå‘½åãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    if cluster_names and cluster_id in cluster_names:
                        domain = cluster_names[cluster_id]
                    else:
                        domain = f"ã‚¯ãƒ©ã‚¹ã‚¿{cluster_id}"
                else:
                    domain = "æœªåˆ†é¡"
                    noise_count += 1

                # çµ±è¨ˆã‚«ã‚¦ãƒ³ãƒˆ
                if synonyms:
                    synonyms_count += 1
                else:
                    no_synonyms_count += 1

                try:
                    # 3. ç„¡æ¡ä»¶ã§ä¸Šæ›¸ãï¼ˆCOALESCEãªã—ï¼‰
                    # Note: 'aliases'åˆ—ã«é¡ç¾©èªã‚’ä¿å­˜ï¼ˆsemantic_synonymsã¯å­˜åœ¨ã—ãªã„ï¼‰
                    # WHEREå¥ã‚’è¤‡åˆã‚­ãƒ¼ã«ä¿®æ­£
                    result = conn.execute(
                        text(f"""
                            UPDATE {self.jargon_table_name}
                            SET aliases = :synonyms,
                                domain = :domain
                            WHERE collection_name = :collection_name AND term = :term
                        """),
                        {
                            "collection_name": collection_name,
                            "term": term,
                            "synonyms": synonym_terms,
                            "domain": domain
                        }
                    )

                    # å®Ÿéš›ã«æ›´æ–°ã•ã‚ŒãŸè¡Œæ•°ã‚’ç¢ºèª
                    if result.rowcount == 0:
                        logger.warning(f"âš ï¸ UPDATE failed for term='{term}', collection='{collection_name}' (0 rows updated - term not found in DB)")
                    else:
                        updated_count += 1
                        logger.debug(f"âœ“ Updated term='{term}': domain='{domain}', synonyms={len(synonym_terms)} ({synonym_terms})")

                except Exception as e:
                    logger.error(f"Error updating term '{term}': {e}", exc_info=True)

        logger.info(f"Updated domain field for {updated_count} terms in database:")
        logger.info(f"  - With synonyms: {synonyms_count} terms")
        logger.info(f"  - Without synonyms: {no_synonyms_count} terms")
        logger.info(f"  - Noise cluster (æœªåˆ†é¡): {noise_count} terms")

        return updated_count

    async def analyze_and_save(self, output_path: str = "output/term_clusters.json", include_hierarchy: bool = True, use_llm_naming: bool = False) -> Dict[str, Any]:
        """å®Œå…¨ãªåˆ†æã‚’å®Ÿè¡Œã—ã¦çµæœã‚’ä¿å­˜"""
        
        # 1. ç”¨èªã‚’èª­ã¿è¾¼ã¿
        self.load_terms_from_db()
        
        if len(self.terms_data) == 0:
            logger.warning("No terms found in database")
            return {'status': 'error', 'message': 'No terms found'}
        
        # 2. ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç”Ÿæˆ
        self.generate_embeddings()
        
        # 3. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œ
        clustering_result = self.perform_clustering()
        
        if clustering_result.get('status') == 'skipped':
            # ç”¨èªæ•°ãŒå°‘ãªã„å ´åˆã¯ç°¡æ˜“ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã®ã¿
            logger.info("Using manual categories due to insufficient terms")
            
            # æ—¢å­˜ã®domainãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ä½¿ç”¨
            domain_groups = {}
            for term in self.terms_data:
                domain = term.get('domain', 'ãã®ä»–')
                if domain not in domain_groups:
                    domain_groups[domain] = []
                domain_groups[domain].append(term['term'])
            
            result = {
                'status': 'manual_categorization',
                'timestamp': datetime.now().isoformat(),
                'total_terms': len(self.terms_data),
                'categories': domain_groups,
                'message': f'Using manual categorization. Will switch to clustering when {self.min_terms}+ terms available.'
            }
        else:
            # 4. ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®ç”¨èªã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            cluster_terms = {}
            for i, cluster_id in enumerate(self.clusters):
                if cluster_id not in cluster_terms:
                    cluster_terms[cluster_id] = []
                cluster_terms[cluster_id].append(self.terms_data[i]['term'])
            
            # 5. ã‚¯ãƒ©ã‚¹ã‚¿ã«åå‰ã‚’ä»˜ã‘ã‚‹
            if use_llm_naming:
                cluster_names = await self.name_clusters_with_llm(cluster_terms)
            else:
                # LLMã‚’ä½¿ã‚ãªã„å ´åˆã¯ç°¡å˜ãªåå‰ã‚’ä»˜ã‘ã‚‹
                cluster_names = {}
                for cluster_id in cluster_terms.keys():
                    cluster_names[cluster_id] = f"ã‚¯ãƒ©ã‚¹ã‚¿{cluster_id}"
            
            # 6. çµæœã‚’æ•´ç†
            categorized_terms = {}
            for cluster_id, terms in cluster_terms.items():
                category_name = cluster_names.get(cluster_id, f"ã‚¯ãƒ©ã‚¹ã‚¿{cluster_id}" if cluster_id >= 0 else "æœªåˆ†é¡")
                categorized_terms[category_name] = {
                    'terms': terms,
                    'count': len(terms),
                    'cluster_id': int(cluster_id)  # Convert numpy int64 to Python int
                }
            
            result = {
                'status': 'success',
                'timestamp': datetime.now().isoformat(),
                'total_terms': len(self.terms_data),
                'clustering_stats': clustering_result,
                'categories': categorized_terms
            }
            
            # éšå±¤æ§‹é€ ã‚’è¿½åŠ 
            if include_hierarchy:
                hierarchy_structure = self.build_hierarchical_structure()
                result['hierarchy'] = hierarchy_structure
        
        # 7. çµæœã‚’ä¿å­˜
        output_file = Path(output_path)
        output_file.parent.mkdir(exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Analysis results saved to {output_path}")
        return result
    
    def suggest_related_terms(self, term: str, n_suggestions: int = 5) -> List[Dict[str, Any]]:
        """æŒ‡å®šç”¨èªã®é–¢é€£èªã‚’ææ¡ˆï¼ˆåŒä¸€ã‚¯ãƒ©ã‚¹ã‚¿ã‹ã‚‰ï¼‰"""
        if self.clusters is None or self.embeddings_matrix is None:
            logger.warning("Clustering not performed yet")
            return []
        
        # æŒ‡å®šç”¨èªã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ¤œç´¢
        term_idx = None
        for i, t in enumerate(self.terms_data):
            if t['term'] == term:
                term_idx = i
                break
        
        if term_idx is None:
            logger.warning(f"Term '{term}' not found")
            return []
        
        # åŒã˜ã‚¯ãƒ©ã‚¹ã‚¿ã®ç”¨èªã‚’æ¤œç´¢
        cluster_id = self.clusters[term_idx]
        if cluster_id == -1:
            logger.info(f"Term '{term}' is in noise cluster")
            return []
        
        # åŒä¸€ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®ä»–ã®ç”¨èªã‚’è·é›¢é †ã«ã‚½ãƒ¼ãƒˆ
        same_cluster_indices = [
            i for i, c in enumerate(self.clusters) 
            if c == cluster_id and i != term_idx
        ]
        
        if not same_cluster_indices:
            return []
        
        # è·é›¢è¨ˆç®—
        term_embedding = self.embeddings_matrix[term_idx]
        distances = []
        for idx in same_cluster_indices:
            dist = np.linalg.norm(term_embedding - self.embeddings_matrix[idx])
            distances.append((idx, dist))
        
        # è·é›¢ã§ã‚½ãƒ¼ãƒˆ
        distances.sort(key=lambda x: x[1])
        
        # ä¸Šä½Nå€‹ã‚’è¿”ã™
        suggestions = []
        for idx, dist in distances[:n_suggestions]:
            suggestions.append({
                'term': self.terms_data[idx]['term'],
                'definition': self.terms_data[idx]['definition'],
                'distance': float(dist),
                'similarity': float(1 / (1 + dist))  # ç°¡æ˜“çš„ãªé¡ä¼¼åº¦ã‚¹ã‚³ã‚¢
            })
        
        return suggestions
    
    def analyze_condensed_tree(self) -> Dict[str, Any]:
        """HDBSCANã®Condensed Treeã‹ã‚‰éšå±¤æ§‹é€ ã‚’æŠ½å‡º"""
        if self.clusterer is None:
            logger.warning("Clustering not performed yet")
            return {}
        
        # Condensed Treeã‚’å–å¾—
        tree = self.clusterer.condensed_tree_
        tree_df = tree.to_pandas()
        
        # éšå±¤æƒ…å ±ã‚’æ•´ç†
        hierarchy_info = {
            'tree_data': [],
            'cluster_hierarchy': {},
            'term_hierarchy': []
        }
        
        # 1. ãƒ„ãƒªãƒ¼æ§‹é€ ã®è§£æ
        for _, row in tree_df.iterrows():
            parent_id = int(row['parent'])
            child_id = int(row['child'])
            lambda_val = float(row['lambda_val'])
            child_size = int(row['child_size'])
            
            # ã‚¯ãƒ©ã‚¹ã‚¿ã®åˆ†é›¢æƒ…å ±ã‚’è¨˜éŒ²
            if child_size > 1:  # ã‚¯ãƒ©ã‚¹ã‚¿ã®å ´åˆ
                hierarchy_info['tree_data'].append({
                    'parent': parent_id,
                    'child': child_id,
                    'lambda': lambda_val,
                    'size': child_size,
                    'type': 'cluster'
                })
                
                # ã‚¯ãƒ©ã‚¹ã‚¿ã®è¦ªå­é–¢ä¿‚ã‚’è¨˜éŒ²
                if child_id not in hierarchy_info['cluster_hierarchy']:
                    hierarchy_info['cluster_hierarchy'][child_id] = {
                        'parent': parent_id,
                        'lambda': lambda_val,
                        'size': child_size,
                        'depth': self._calculate_depth(child_id, tree_df)
                    }
        
        # 2. æœ€çµ‚ã‚¯ãƒ©ã‚¹ã‚¿ã¨Condensed Tree IDã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æ§‹ç¯‰
        # Condensed Treeã®æœ«ç«¯ãƒãƒ¼ãƒ‰ï¼ˆä»–ã®ã‚¯ãƒ©ã‚¹ã‚¿ã®è¦ªã«ãªã£ã¦ã„ãªã„ï¼‰ã‚’ç‰¹å®š
        all_parents = set([row['parent'] for row in hierarchy_info['tree_data']])
        final_clusters = []
        for cluster_id, info in hierarchy_info['cluster_hierarchy'].items():
            if cluster_id not in all_parents:
                final_clusters.append(cluster_id)
        
        # æœ€çµ‚ã‚¯ãƒ©ã‚¹ã‚¿ã‚’ã‚½ãƒ¼ãƒˆã—ã¦ã€ãƒ©ãƒ™ãƒ«ï¼ˆ0,1,2...ï¼‰ã«ãƒãƒƒãƒ”ãƒ³ã‚°
        final_clusters.sort()
        cluster_id_mapping = {i: tree_id for i, tree_id in enumerate(final_clusters)}
        
        logger.info(f"Cluster ID mapping: {cluster_id_mapping}")
        
        # 3. å„ç”¨èªã®éšå±¤ãƒ¬ãƒ™ãƒ«ã‚’åˆ¤å®š
        if self.clusters is not None:
            for idx, term_data in enumerate(self.terms_data):
                cluster_label = self.clusters[idx]
                if cluster_label >= 0:
                    # å®Ÿéš›ã®Condensed Tree IDã‚’å–å¾—
                    actual_tree_id = cluster_id_mapping.get(cluster_label)
                    
                    if actual_tree_id:
                        # ã‚¯ãƒ©ã‚¹ã‚¿å½¢æˆæ™‚ã®lambdaå€¤ã‚’å–å¾—
                        cluster_info = hierarchy_info['cluster_hierarchy'].get(actual_tree_id, {})
                        lambda_val = cluster_info.get('lambda', 0.0)
                        depth = cluster_info.get('depth', 0)
                    else:
                        # ãƒãƒƒãƒ”ãƒ³ã‚°ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                        lambda_val = 0.0
                        depth = 0
                    
                    # lambdaå€¤ã«åŸºã¥ã„ã¦éšå±¤ãƒ¬ãƒ™ãƒ«ã‚’åˆ¤å®š
                    if lambda_val > 0.89:  # ã‚ˆã‚Šå…·ä½“çš„ãªé–¾å€¤
                        level = "å…·ä½“çš„æ¦‚å¿µ"
                    elif lambda_val > 0.88:
                        level = "ä¸­é–“æ¦‚å¿µ"
                    else:
                        level = "ä¸Šä½æ¦‚å¿µ"
                    
                    hierarchy_info['term_hierarchy'].append({
                        'term': term_data['term'],
                        'cluster': int(cluster_label),
                        'tree_cluster_id': actual_tree_id if actual_tree_id else -1,
                        'lambda': lambda_val,
                        'depth': depth,
                        'level': level
                    })
        
        # 3. ã‚¯ãƒ©ã‚¹ã‚¿ã®æ°¸ç¶šæ€§æƒ…å ±
        if hasattr(self.clusterer, 'cluster_persistence_'):
            persistence = self.clusterer.cluster_persistence_
            for cluster_id, persist_val in enumerate(persistence):
                if cluster_id in hierarchy_info['cluster_hierarchy']:
                    hierarchy_info['cluster_hierarchy'][cluster_id]['persistence'] = float(persist_val)
        
        logger.info(f"Analyzed condensed tree with {len(hierarchy_info['tree_data'])} nodes")
        return hierarchy_info
    
    def _calculate_depth(self, node_id: int, tree_df) -> int:
        """ãƒãƒ¼ãƒ‰ã®éšå±¤æ·±ã•ã‚’è¨ˆç®—"""
        depth = 0
        current_node = node_id
        
        # è¦ªã‚’è¾¿ã£ã¦æ·±ã•ã‚’è¨ˆç®—
        while True:
            parent_rows = tree_df[tree_df['child'] == current_node]
            if parent_rows.empty:
                break
            current_node = int(parent_rows.iloc[0]['parent'])
            depth += 1
            
            # ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢
            if depth > 100:
                break
        
        return depth
    
    def build_hierarchical_structure(self) -> Dict[str, Any]:
        """éšå±¤æ§‹é€ ã‚’æ§‹ç¯‰ã—ã¦æ•´ç†"""
        if self.clusterer is None or self.clusters is None:
            logger.warning("Clustering not performed yet")
            return {}
        
        # Condensed Treeè§£æ
        hierarchy_info = self.analyze_condensed_tree()
        
        # éšå±¤ã”ã¨ã«ç”¨èªã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        hierarchical_groups = {
            'ä¸Šä½æ¦‚å¿µ': [],
            'ä¸­é–“æ¦‚å¿µ': [],
            'å…·ä½“çš„æ¦‚å¿µ': [],
            'æœªåˆ†é¡': []
        }
        
        # ç”¨èªã‚’éšå±¤ãƒ¬ãƒ™ãƒ«ã”ã¨ã«åˆ†é¡
        for term_info in hierarchy_info.get('term_hierarchy', []):
            level = term_info.get('level', 'æœªåˆ†é¡')
            hierarchical_groups[level].append({
                'term': term_info['term'],
                'lambda': term_info.get('lambda', 0),
                'depth': term_info.get('depth', 0)
            })
        
        # æœªåˆ†é¡ã®ç”¨èªï¼ˆãƒã‚¤ã‚ºãƒã‚¤ãƒ³ãƒˆï¼‰ã‚’è¿½åŠ 
        for idx, term_data in enumerate(self.terms_data):
            if self.clusters[idx] == -1:
                hierarchical_groups['æœªåˆ†é¡'].append({
                    'term': term_data['term'],
                    'lambda': 0,
                    'depth': -1
                })
        
        # çµæœã‚’æ•´ç†
        result = {
            'hierarchical_groups': hierarchical_groups,
            'tree_statistics': {
                'total_clusters': len(hierarchy_info.get('cluster_hierarchy', {})),
                'max_depth': max([info['depth'] for info in hierarchy_info.get('term_hierarchy', [{'depth': 0}])]),
                'persistence_scores': {
                    cid: info.get('persistence', 0) 
                    for cid, info in hierarchy_info.get('cluster_hierarchy', {}).items()
                }
            },
            'condensed_tree_raw': hierarchy_info
        }
        
        return result

# â”€â”€ Main Execution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    analyzer = TermClusteringAnalyzer(PG_URL, min_terms=3)
    
    # åˆ†æå®Ÿè¡Œ
    result = await analyzer.analyze_and_save()
    
    # çµæœè¡¨ç¤º
    print("\n" + "="*50)
    print("Term Clustering Analysis Results")
    print("="*50)
    
    if result['status'] == 'manual_categorization':
        print(f"Status: Manual categorization (not enough terms)")
        print(f"Total terms: {result['total_terms']}")
        print(f"\nCategories:")
        for category, terms in result['categories'].items():
            print(f"\n[{category}]: {len(terms)} terms")
            for term in terms[:5]:
                print(f"  - {term}")
            if len(terms) > 5:
                print(f"  ... and {len(terms)-5} more")
    
    elif result['status'] == 'success':
        stats = result['clustering_stats']
        print(f"Total terms: {result['total_terms']}")
        print(f"Clusters found: {stats['n_clusters']}")
        print(f"Noise points: {stats['n_noise']}")
        if stats['silhouette_score']:
            print(f"Silhouette score: {stats['silhouette_score']:.3f}")
        
        print(f"\nCategories:")
        for category, info in result['categories'].items():
            print(f"\n[{category}]: {info['count']} terms")
            for term in info['terms'][:5]:
                print(f"  - {term}")
            if len(info['terms']) > 5:
                print(f"  ... and {len(info['terms'])-5} more")
        
        # éšå±¤æ§‹é€ ã®è¡¨ç¤º
        if 'hierarchy' in result:
            print(f"\n" + "="*50)
            print("Hierarchical Structure (based on HDBSCAN Condensed Tree)")
            print("="*50)
            
            hierarchy = result['hierarchy']
            groups = hierarchy.get('hierarchical_groups', {})
            
            for level_name in ['ä¸Šä½æ¦‚å¿µ', 'ä¸­é–“æ¦‚å¿µ', 'å…·ä½“çš„æ¦‚å¿µ', 'æœªåˆ†é¡']:
                if level_name in groups and groups[level_name]:
                    print(f"\n[{level_name}]:")
                    for item in groups[level_name][:10]:
                        depth_indicator = "  " * max(0, item.get('depth', 0))
                        lambda_str = f"(Î»={item.get('lambda', 0):.3f})" if item.get('lambda', 0) > 0 else ""
                        print(f"{depth_indicator}- {item['term']} {lambda_str}")
                    if len(groups[level_name]) > 10:
                        print(f"  ... and {len(groups[level_name])-10} more")
            
            # ãƒ„ãƒªãƒ¼çµ±è¨ˆã®è¡¨ç¤º
            tree_stats = hierarchy.get('tree_statistics', {})
            if tree_stats:
                print(f"\nTree Statistics:")
                print(f"  Total clusters: {tree_stats.get('total_clusters', 0)}")
                print(f"  Max depth: {tree_stats.get('max_depth', 0)}")
    
    print(f"\nFull results saved to: output/term_clusters.json")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())