"""
å°‚é–€ç”¨èªæŠ½å‡ºã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ä»®æƒ³ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦å„å‡¦ç†ã®æ™‚é–“ã‚’è¨ˆæ¸¬
"""
import asyncio
import time
import tempfile
import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# Windowsç’°å¢ƒã§ã®çµµæ–‡å­—å‡ºåŠ›å¯¾å¿œ
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®šï¼ˆè©³ç´°ãƒ­ã‚°ã‚’æœ‰åŠ¹åŒ–ï¼‰
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s: %(message)s'
)

from src.rag.config import Config
from src.rag.term_extraction import TermExtractor
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain_community.vectorstores import PGVector


def generate_dummy_documents(output_dir: Path, num_docs: int = 10):
    """
    ãƒ†ã‚¹ãƒˆç”¨ã®ä»®æƒ³ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆ

    Args:
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        num_docs: ç”Ÿæˆã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
    """
    output_dir.mkdir(parents=True, exist_ok=True)

    # å°‚é–€ç”¨èªã‚’å«ã‚€ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
    sample_texts = [
        """
        # èˆ¹èˆ¶ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°æŠ€è¡“æ–‡æ›¸

        ## ä¸»æ©Ÿé–¢ã‚·ã‚¹ãƒ†ãƒ 
        ä¸»æ©Ÿé–¢ï¼ˆMain Engineï¼‰ã¯èˆ¹èˆ¶æ¨é€²ã®ä¸­æ ¸ã‚’ãªã™ã‚·ã‚¹ãƒ†ãƒ ã§ã‚ã‚Šã€SFOCï¼ˆSpecific Fuel Oil Consumptionï¼‰ã®
        æœ€é©åŒ–ãŒé‡è¦ã§ã™ã€‚BMSã‚·ã‚¹ãƒ†ãƒ ï¼ˆBallast Management Systemï¼‰ã¨ã®é€£æºã«ã‚ˆã‚Šã€ç‡ƒæ–™æ¶ˆè²»åŠ¹ç‡ã‚’æœ€å¤§åŒ–ã—ã¾ã™ã€‚

        ## æ’ã‚¬ã‚¹å‡¦ç†è£…ç½®
        NOxï¼ˆçª’ç´ é…¸åŒ–ç‰©ï¼‰ã‚„SOxï¼ˆç¡«é»„é…¸åŒ–ç‰©ï¼‰ã®æ’å‡ºå‰Šæ¸›ã®ãŸã‚ã€SCRï¼ˆSelective Catalytic Reductionï¼‰ã‚·ã‚¹ãƒ†ãƒ ã‚’
        æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚EGRï¼ˆExhaust Gas Recirculationï¼‰æŠ€è¡“ã¨ã®çµ„ã¿åˆã‚ã›ã«ã‚ˆã‚Šã€IMOè¦åˆ¶å€¤ã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã€‚

        ## é›»åŠ›ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
        DGï¼ˆDiesel Generatorï¼‰ã«ã‚ˆã‚‹é›»åŠ›ä¾›çµ¦ã¨ã€PMSï¼ˆPower Management Systemï¼‰ã«ã‚ˆã‚‹è² è·åˆ†æ•£åˆ¶å¾¡ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚
        UPSï¼ˆUninterruptible Power Supplyï¼‰ã«ã‚ˆã‚Šã€é‡è¦ã‚·ã‚¹ãƒ†ãƒ ã¸ã®å®‰å®šä¾›çµ¦ã‚’ç¢ºä¿ã—ã¾ã™ã€‚

        ## è‡ªå‹•åŒ–åˆ¶å¾¡
        IASï¼ˆIntegrated Automation Systemï¼‰ã«ã‚ˆã‚Šã€æ©Ÿé–¢å®¤ã®é›†ä¸­ç›£è¦–åˆ¶å¾¡ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
        PLCï¼ˆProgrammable Logic Controllerï¼‰ãƒ™ãƒ¼ã‚¹ã®åˆ¶å¾¡ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚Šã€é«˜åº¦ãªè‡ªå‹•é‹è»¢ãŒå¯èƒ½ã§ã™ã€‚

        ## èˆªæµ·è¨ˆå™¨ã‚·ã‚¹ãƒ†ãƒ 
        GPSï¼ˆGlobal Positioning Systemï¼‰ã€AISï¼ˆAutomatic Identification Systemï¼‰ã€ECDISï¼ˆElectronic Chart Display
        and Information Systemï¼‰ã‚’çµ±åˆã—ãŸINSï¼ˆIntegrated Navigation Systemï¼‰ã«ã‚ˆã‚Šã€å®‰å…¨ãªèˆªæµ·ã‚’æ”¯æ´ã—ã¾ã™ã€‚

        ## æ¨é€²ã‚·ã‚¹ãƒ†ãƒ 
        CPPï¼ˆControllable Pitch Propellerï¼‰ã«ã‚ˆã‚Šã€å¯å¤‰ãƒ”ãƒƒãƒåˆ¶å¾¡ã‚’å®Ÿç¾ã—ã¾ã™ã€‚FPPï¼ˆFixed Pitch Propellerï¼‰
        ã¨æ¯”è¼ƒã—ã¦ã€ç‡ƒè²»æ€§èƒ½ã¨æ“èˆ¹æ€§èƒ½ãŒå‘ä¸Šã—ã¾ã™ã€‚

        ## è£œæ©Ÿã‚·ã‚¹ãƒ†ãƒ 
        è£œåŠ©ãƒœã‚¤ãƒ©ãƒ¼ï¼ˆAuxiliary Boilerï¼‰ã¯ã€åœæ³Šä¸­ã®è’¸æ°—ä¾›çµ¦ã‚’æ‹…ã„ã¾ã™ã€‚ç†±äº¤æ›å™¨ï¼ˆHeat Exchangerï¼‰ã«ã‚ˆã‚Šã€
        å»ƒç†±å›åã‚’è¡Œã„ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚

        ## å†·å´ã‚·ã‚¹ãƒ†ãƒ 
        ä¸­å¤®å†·å´æ°´ã‚·ã‚¹ãƒ†ãƒ ï¼ˆCentral Cooling Water Systemï¼‰ã«ã‚ˆã‚Šã€å„æ©Ÿå™¨ã¸ã®å†·å´æ°´ä¾›çµ¦ã‚’ä¸€å…ƒç®¡ç†ã—ã¾ã™ã€‚
        æµ·æ°´å†·å´å™¨ï¼ˆSea Water Coolerï¼‰ã¨æ¸…æ°´å†·å´å™¨ï¼ˆFresh Water Coolerï¼‰ã®äºŒæ®µéšå†·å´ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚
        """,
        """
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†æŠ€è¡“

        ## ãƒªãƒ¬ãƒ¼ã‚·ãƒ§ãƒŠãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        RDBMSï¼ˆRelational Database Management Systemï¼‰ã¯ã€SQLï¼ˆStructured Query Languageï¼‰ã«ã‚ˆã‚Š
        ãƒ‡ãƒ¼ã‚¿æ“ä½œã‚’è¡Œã„ã¾ã™ã€‚ACIDç‰¹æ€§ï¼ˆAtomicity, Consistency, Isolation, Durabilityï¼‰ã«ã‚ˆã‚Šã€
        ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã®æ•´åˆæ€§ã‚’ä¿è¨¼ã—ã¾ã™ã€‚

        ## ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ–
        B-Tree ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚„ãƒãƒƒã‚·ãƒ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’é©åˆ‡ã«è¨­è¨ˆã™ã‚‹ã“ã¨ã§ã€ã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå‘ä¸Šã—ã¾ã™ã€‚
        ã‚«ãƒãƒªãƒ³ã‚°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆCovering Indexï¼‰ã«ã‚ˆã‚Šã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚ªãƒ³ãƒªãƒ¼ã‚¹ã‚­ãƒ£ãƒ³ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

        ## ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
        ãƒã‚¹ã‚¿ãƒ¼ã‚¹ãƒ¬ãƒ¼ãƒ–ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆMaster-Slave Replicationï¼‰ã«ã‚ˆã‚Šã€èª­ã¿å–ã‚Šè² è·ã‚’åˆ†æ•£ã—ã¾ã™ã€‚
        ãƒãƒ«ãƒãƒã‚¹ã‚¿ãƒ¼ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆMulti-Master Replicationï¼‰ã§ã¯ã€æ›¸ãè¾¼ã¿ã®é«˜å¯ç”¨æ€§ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

        ## ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°
        æ°´å¹³ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ï¼ˆHorizontal Partitioningï¼‰ã«ã‚ˆã‚Šã€å¤§è¦æ¨¡ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’åˆ†å‰²ç®¡ç†ã—ã¾ã™ã€‚
        å‚ç›´ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ï¼ˆVertical Partitioningï¼‰ã§ã¯ã€ã‚«ãƒ©ãƒ å˜ä½ã§ã®åˆ†å‰²ã‚’è¡Œã„ã¾ã™ã€‚

        ## ã‚¯ã‚¨ãƒªæœ€é©åŒ–
        å®Ÿè¡Œè¨ˆç”»ï¼ˆExecution Planï¼‰ã®åˆ†æã«ã‚ˆã‚Šã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®šã—ã¾ã™ã€‚çµ±è¨ˆæƒ…å ±ï¼ˆStatisticsï¼‰ã®
        æ›´æ–°ã«ã‚ˆã‚Šã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®åˆ¤æ–­ç²¾åº¦ãŒå‘ä¸Šã—ã¾ã™ã€‚
        """,
        """
        # ã‚¯ãƒ©ã‚¦ãƒ‰ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£

        ## ã‚³ãƒ³ãƒ†ãƒŠã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        Kubernetesï¼ˆK8sï¼‰ã«ã‚ˆã‚Šã€ã‚³ãƒ³ãƒ†ãƒŠã®è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€ç®¡ç†ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
        Podã€Serviceã€Deploymentãªã©ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’å®šç¾©ã™ã‚‹ã“ã¨ã§ã€å®£è¨€çš„ãªæ§‹æˆç®¡ç†ãŒå¯èƒ½ã§ã™ã€‚

        ## ã‚µãƒ¼ãƒ“ã‚¹ãƒ¡ãƒƒã‚·ãƒ¥
        Istio ã‚„ Linkerd ãªã©ã®ã‚µãƒ¼ãƒ“ã‚¹ãƒ¡ãƒƒã‚·ãƒ¥ï¼ˆService Meshï¼‰ã«ã‚ˆã‚Šã€ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹é–“ã®
        é€šä¿¡åˆ¶å¾¡ã€ç›£è¦–ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚’å¼·åŒ–ã—ã¾ã™ã€‚

        ## CI/CD ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
        GitLab CIã€GitHub Actionsã€Jenkins ãªã©ã‚’ä½¿ç”¨ã—ãŸç¶™ç¶šçš„ã‚¤ãƒ³ãƒ†ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆCIï¼‰ã¨
        ç¶™ç¶šçš„ãƒ‡ãƒªãƒãƒªãƒ¼ï¼ˆCDï¼‰ã«ã‚ˆã‚Šã€ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚’è‡ªå‹•åŒ–ã—ã¾ã™ã€‚

        ## ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ as Code
        Terraformã€CloudFormationã€Ansible ãªã©ã‚’ä½¿ç”¨ã—ãŸIaCï¼ˆInfrastructure as Codeï¼‰ã«ã‚ˆã‚Šã€
        ã‚¤ãƒ³ãƒ•ãƒ©ã®æ§‹æˆç®¡ç†ã‚’ã‚³ãƒ¼ãƒ‰åŒ–ã—ã¾ã™ã€‚

        ## ç›£è¦–ã¨ãƒ­ã‚®ãƒ³ã‚°
        Prometheusã€Grafana ã«ã‚ˆã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–ã€ELK ã‚¹ã‚¿ãƒƒã‚¯ï¼ˆElasticsearch, Logstash, Kibanaï¼‰
        ã«ã‚ˆã‚‹ãƒ­ã‚°é›†ç´„ãƒ»åˆ†æã‚’å®Ÿæ–½ã—ã¾ã™ã€‚
        """
    ]

    print(f"ğŸ”§ ç”Ÿæˆä¸­: {num_docs}ä»¶ã®ä»®æƒ³ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")

    for i in range(num_docs):
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
        text = sample_texts[i % len(sample_texts)]

        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã«å°‘ã—å†…å®¹ã‚’å¤‰ãˆã‚‹
        doc_text = f"# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ {i+1}\n\n" + text + f"\n\n## è¿½åŠ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ {i+1}\nå°‚é–€ç”¨èªæŠ½å‡ºãƒ†ã‚¹ãƒˆç”¨ã®è¿½åŠ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã§ã™ã€‚"

        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        file_path = output_dir / f"test_doc_{i+1:03d}.txt"
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(doc_text)

    print(f"âœ… å®Œäº†: {output_dir} ã« {num_docs}ä»¶ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆ")
    return output_dir


class PerformanceTimer:
    """å‡¦ç†æ™‚é–“æ¸¬å®šç”¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""

    def __init__(self, name: str):
        self.name = name
        self.start_time = None
        self.end_time = None

    def __enter__(self):
        self.start_time = time.time()
        print(f"\nâ±ï¸  [{self.name}] é–‹å§‹...")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.end_time = time.time()
        elapsed = self.end_time - self.start_time
        print(f"âœ… [{self.name}] å®Œäº†: {elapsed:.2f}ç§’")
        return False

    @property
    def elapsed(self):
        if self.end_time and self.start_time:
            return self.end_time - self.start_time
        return None


async def benchmark_term_extraction(num_docs: int = 10):
    """
    å°‚é–€ç”¨èªæŠ½å‡ºã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ

    Args:
        num_docs: ãƒ†ã‚¹ãƒˆã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
    """
    print("=" * 80)
    print(f"ğŸš€ å°‚é–€ç”¨èªæŠ½å‡ºãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š")
    print(f"ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {num_docs}ä»¶")
    print("=" * 80)

    # ã‚¿ã‚¤ãƒãƒ¼è¨˜éŒ²ç”¨
    timers = {}

    # è¨­å®šèª­ã¿è¾¼ã¿
    with PerformanceTimer("è¨­å®šèª­ã¿è¾¼ã¿") as timer:
        config = Config()
    timers["è¨­å®šèª­ã¿è¾¼ã¿"] = timer.elapsed

    # LLMã¨EmbeddingsåˆæœŸåŒ–
    with PerformanceTimer("LLM/EmbeddingsåˆæœŸåŒ–") as timer:
        llm = AzureChatOpenAI(
            azure_endpoint=config.azure_openai_endpoint,
            api_key=config.azure_openai_api_key,
            api_version=config.azure_openai_api_version,
            azure_deployment=config.azure_openai_chat_deployment_name,
            temperature=0.1,
        )

        embeddings = AzureOpenAIEmbeddings(
            azure_endpoint=config.azure_openai_endpoint,
            api_key=config.azure_openai_api_key,
            api_version=config.azure_openai_api_version,
            azure_deployment=config.azure_openai_embedding_deployment_name
        )
    timers["LLM/EmbeddingsåˆæœŸåŒ–"] = timer.elapsed

    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢åˆæœŸåŒ–
    with PerformanceTimer("ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢åˆæœŸåŒ–") as timer:
        pg_url = config.pgvector_connection_string
        vector_store = PGVector(
            collection_name=config.collection_name,
            connection_string=pg_url,
            embedding_function=embeddings,
            pre_delete_collection=False
        )
    timers["ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢åˆæœŸåŒ–"] = timer.elapsed

    # ä»®æƒ³ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆ
    with PerformanceTimer("ä»®æƒ³ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆ") as timer:
        temp_dir = Path(tempfile.mkdtemp(prefix="benchmark_"))
        doc_dir = generate_dummy_documents(temp_dir, num_docs)
    timers["ä»®æƒ³ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆ"] = timer.elapsed

    try:
        # TermExtractoråˆæœŸåŒ–
        with PerformanceTimer("TermExtractoråˆæœŸåŒ–") as timer:
            extractor = TermExtractor(
                config=config,
                llm=llm,
                embeddings=embeddings,
                vector_store=vector_store,
                pg_url=pg_url,
                jargon_table_name=config.jargon_table_name
            )
        timers["TermExtractoråˆæœŸåŒ–"] = timer.elapsed

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆå–å¾—
        files = list(doc_dir.glob("*.txt"))
        print(f"\nğŸ“ å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(files)}ä»¶")

        # å°‚é–€ç”¨èªæŠ½å‡ºå®Ÿè¡Œï¼ˆè©³ç´°è¨ˆæ¸¬ï¼‰
        print("\n" + "=" * 80)
        print("ğŸ“Š å°‚é–€ç”¨èªæŠ½å‡ºãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹")
        print("=" * 80)

        total_start = time.time()

        # extract_from_documents ã®å†…éƒ¨å‡¦ç†ã‚’æ‰‹å‹•ã§è¨ˆæ¸¬
        all_chunks = []
        per_document_texts = []

        # 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿ã¨ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
        with PerformanceTimer("1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿ï¼†ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²") as timer:
            for file_path in files:
                loader = extractor._get_loader(file_path)
                docs = loader.load()
                chunks = extractor.text_splitter.split_documents(docs)
                all_chunks.extend([c.page_content for c in chunks])

                doc_text = "\n".join([c.page_content for c in chunks])
                per_document_texts.append({
                    "file_path": str(file_path),
                    "text": doc_text
                })
        timers["1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿ï¼†ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²"] = timer.elapsed

        print(f"   ğŸ“ ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {len(all_chunks)}")
        print(f"   ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(per_document_texts)}")

        # 2. çµ±è¨ˆçš„å€™è£œæŠ½å‡º
        all_candidates = {}
        with PerformanceTimer("2. çµ±è¨ˆçš„å€™è£œæŠ½å‡º") as timer:
            from collections import defaultdict
            all_candidates = defaultdict(int)
            document_candidate_map = {}

            for doc_info in per_document_texts:
                file_path = doc_info["file_path"]
                text = doc_info["text"]

                doc_candidates = extractor.statistical_extractor.extract_candidates(text)
                document_candidate_map[file_path] = doc_candidates

                for term, freq in doc_candidates.items():
                    all_candidates[term] += freq
        timers["2. çµ±è¨ˆçš„å€™è£œæŠ½å‡º"] = timer.elapsed
        print(f"   ğŸ” æŠ½å‡ºå€™è£œæ•°: {len(all_candidates)}")

        # 3. TF-IDF + C-value è¨ˆç®—
        with PerformanceTimer("3. TF-IDF + C-value è¨ˆç®—") as timer:
            full_text = "\n".join([doc["text"] for doc in per_document_texts])
            documents = extractor._split_into_sentences(full_text)

            tfidf_scores = extractor.statistical_extractor.calculate_tfidf(documents, all_candidates)
            cvalue_scores = extractor.statistical_extractor.calculate_cvalue(all_candidates, full_text=full_text)
        timers["3. TF-IDF + C-value è¨ˆç®—"] = timer.elapsed

        # 4. ã‚¹ã‚³ã‚¢è¨ˆç®—
        with PerformanceTimer("4. åŸºåº•ã‚¹ã‚³ã‚¢è¨ˆç®—") as timer:
            seed_scores = extractor.statistical_extractor.calculate_combined_scores(
                tfidf_scores, cvalue_scores, stage="seed"
            )
            base_scores = extractor.statistical_extractor.calculate_combined_scores(
                tfidf_scores, cvalue_scores, stage="final"
            )
        timers["4. åŸºåº•ã‚¹ã‚³ã‚¢è¨ˆç®—"] = timer.elapsed

        # 5. SemReRankå€™è£œé¸æŠ
        with PerformanceTimer("5. SemReRankå€™è£œé¸æŠ") as timer:
            MAX_SEMRERANK_CANDIDATES = getattr(config, 'max_semrerank_candidates', 1500)

            if len(all_candidates) > MAX_SEMRERANK_CANDIDATES:
                sorted_candidates = sorted(base_scores.items(), key=lambda x: x[1], reverse=True)
                top_candidates = dict(sorted_candidates[:MAX_SEMRERANK_CANDIDATES])
                candidates_for_semrerank = {k: all_candidates[k] for k in top_candidates.keys()}
                seed_scores_for_semrerank = {k: seed_scores[k] for k in top_candidates.keys()}
                base_scores_for_semrerank = top_candidates
            else:
                candidates_for_semrerank = all_candidates
                seed_scores_for_semrerank = seed_scores
                base_scores_for_semrerank = base_scores
        timers["5. SemReRankå€™è£œé¸æŠ"] = timer.elapsed
        print(f"   ğŸ¯ SemReRankå¯¾è±¡: {len(candidates_for_semrerank)}/{len(all_candidates)}")

        # 6. SemReRankå®Ÿè¡Œ
        enhanced_scores = base_scores_for_semrerank
        if extractor.semrerank:
            with PerformanceTimer("6. SemReRankå®Ÿè¡Œ") as timer:
                try:
                    enhanced_scores = extractor.semrerank.enhance_scores(
                        candidates=list(candidates_for_semrerank.keys()),
                        base_scores=base_scores_for_semrerank,
                        seed_scores=seed_scores_for_semrerank
                    )
                except Exception as e:
                    print(f"   âš ï¸  SemReRankå¤±æ•—: {e}")
                    enhanced_scores = base_scores
            timers["6. SemReRankå®Ÿè¡Œ"] = timer.elapsed
        else:
            print("   â­ï¸  SemReRankç„¡åŠ¹")
            timers["6. SemReRankå®Ÿè¡Œ"] = 0

        # 7. é¡ç¾©èªãƒ»é–¢é€£èªæ¤œå‡º
        with PerformanceTimer("7. é¡ç¾©èªãƒ»é–¢é€£èªæ¤œå‡º") as timer:
            synonym_map = extractor.statistical_extractor.detect_variants(
                candidates=list(candidates_for_semrerank.keys())
            )
            related_map = extractor.statistical_extractor.detect_related_terms(
                candidates=list(candidates_for_semrerank.keys()),
                full_text=full_text,
                max_related=config.max_related_terms_per_candidate,
                min_term_length=config.min_related_term_length
            )
        timers["7. é¡ç¾©èªãƒ»é–¢é€£èªæ¤œå‡º"] = timer.elapsed

        # 8. ExtractedTermã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåŒ–
        with PerformanceTimer("8. ExtractedTermåŒ–ï¼†ã‚½ãƒ¼ãƒˆ") as timer:
            from src.rag.advanced_term_extraction import ExtractedTerm
            terms = [
                ExtractedTerm(
                    term=term,
                    score=enhanced_scores[term],
                    tfidf_score=tfidf_scores.get(term, 0.0),
                    cvalue_score=cvalue_scores.get(term, 0.0),
                    frequency=all_candidates.get(term, 0),
                    variants=synonym_map.get(term, []),
                    related_terms=related_map.get(term, [])
                )
                for term in enhanced_scores
            ]
            terms.sort(key=lambda x: x.score, reverse=True)
        timers["8. ExtractedTermåŒ–ï¼†ã‚½ãƒ¼ãƒˆ"] = timer.elapsed
        print(f"   ğŸ“‹ ç·ç”¨èªæ•°: {len(terms)}")

        # 9. è»½é‡LLMãƒ•ã‚£ãƒ«ã‚¿
        abbreviations = [t for t in terms if extractor._is_abbreviation(t.term)]
        non_abbreviations = [t for t in terms if not extractor._is_abbreviation(t.term)]

        print(f"   ğŸ”¤ ç•¥èª: {len(abbreviations)}, éç•¥èª: {len(non_abbreviations)}")

        if config.enable_lightweight_filter and llm:
            with PerformanceTimer("9. è»½é‡LLMãƒ•ã‚£ãƒ«ã‚¿") as timer:
                definition_percentile = getattr(config, 'definition_generation_percentile', 50.0)
                n_candidates = max(1, int(len(non_abbreviations) * definition_percentile / 100))
                candidate_terms = non_abbreviations[:n_candidates]

                filtered_terms = await extractor._lightweight_llm_filter(candidate_terms)
                terms_for_definition = abbreviations + filtered_terms
            timers["9. è»½é‡LLMãƒ•ã‚£ãƒ«ã‚¿"] = timer.elapsed
            print(f"   âœ… é€šé: {len(filtered_terms)}/{len(candidate_terms)}")
        else:
            definition_percentile = 50.0
            n_candidates = max(1, int(len(non_abbreviations) * definition_percentile / 100))
            terms_for_definition = abbreviations + non_abbreviations[:n_candidates]
            timers["9. è»½é‡LLMãƒ•ã‚£ãƒ«ã‚¿"] = 0
            print(f"   â­ï¸  è»½é‡ãƒ•ã‚£ãƒ«ã‚¿ç„¡åŠ¹")

        print(f"   ğŸ¯ å®šç¾©ç”Ÿæˆå¯¾è±¡: {len(terms_for_definition)}")

        # 10. RAGå®šç¾©ç”Ÿæˆï¼ˆãƒãƒ«ã‚¯å‡¦ç†ï¼‰
        if vector_store and llm:
            with PerformanceTimer("10. RAGå®šç¾©ç”Ÿæˆï¼ˆãƒãƒ«ã‚¯å‡¦ç†ï¼‰") as timer:
                await extractor._bulk_generate_definitions(terms_for_definition)
            timers["10. RAGå®šç¾©ç”Ÿæˆï¼ˆãƒãƒ«ã‚¯å‡¦ç†ï¼‰"] = timer.elapsed

            defined_count = sum(1 for t in terms_for_definition if t.definition)
            print(f"   âœ… å®šç¾©ç”Ÿæˆå®Œäº†: {defined_count}/{len(terms_for_definition)}")
        else:
            timers["10. RAGå®šç¾©ç”Ÿæˆï¼ˆãƒãƒ«ã‚¯å‡¦ç†ï¼‰"] = 0
            print(f"   â­ï¸  å®šç¾©ç”Ÿæˆã‚¹ã‚­ãƒƒãƒ—")

        # 11. é‡é‡LLMãƒ•ã‚£ãƒ«ã‚¿
        if llm:
            with PerformanceTimer("11. é‡é‡LLMãƒ•ã‚£ãƒ«ã‚¿") as timer:
                from src.rag.prompts import get_technical_term_judgment_prompt
                from langchain_core.output_parsers import StrOutputParser

                terms_with_def = [t for t in terms if t.definition]
                technical_terms = []

                if terms_with_def:
                    prompt = get_technical_term_judgment_prompt()
                    chain = prompt | llm | StrOutputParser()

                    batch_size = config.llm_filter_batch_size

                    for i in range(0, len(terms_with_def), batch_size):
                        batch = terms_with_def[i:i+batch_size]
                        batch_inputs = [{"term": t.term, "definition": t.definition} for t in batch]

                        try:
                            result_texts = await chain.abatch(batch_inputs)
                            for term, result_text in zip(batch, result_texts):
                                result = extractor._parse_llm_json(result_text)
                                if result and result.get("is_technical", False):
                                    term.metadata["confidence"] = result.get("confidence", 0.0)
                                    term.metadata["reason"] = result.get("reason", "")
                                    technical_terms.append(term)
                        except Exception as e:
                            print(f"   âš ï¸  ãƒãƒƒãƒå¤±æ•—: {e}")

            timers["11. é‡é‡LLMãƒ•ã‚£ãƒ«ã‚¿"] = timer.elapsed
            print(f"   âœ… å°‚é–€ç”¨èª: {len(technical_terms)}/{len(terms_with_def)}")
        else:
            timers["11. é‡é‡LLMãƒ•ã‚£ãƒ«ã‚¿"] = 0
            print(f"   â­ï¸  é‡é‡ãƒ•ã‚£ãƒ«ã‚¿ã‚¹ã‚­ãƒƒãƒ—")

        total_elapsed = time.time() - total_start
        timers["ç·å‡¦ç†æ™‚é–“"] = total_elapsed

    finally:
        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šé™¤
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)

    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 80)
    print("ğŸ“Š å‡¦ç†æ™‚é–“ã‚µãƒãƒªãƒ¼")
    print("=" * 80)

    max_label_len = max(len(label) for label in timers.keys())

    for label, elapsed in timers.items():
        if elapsed is not None and elapsed > 0:
            percentage = (elapsed / total_elapsed * 100) if label != "ç·å‡¦ç†æ™‚é–“" else 100
            bar_length = int(percentage / 2)
            bar = "â–ˆ" * bar_length
            print(f"{label:<{max_label_len}} : {elapsed:7.2f}ç§’ {bar} {percentage:5.1f}%")

    print("=" * 80)

    # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š
    print("\nğŸ” ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ")
    print("-" * 80)

    # ç·å‡¦ç†æ™‚é–“ã‚’é™¤å¤–ã—ã¦ã‚½ãƒ¼ãƒˆ
    processing_timers = {k: v for k, v in timers.items() if k != "ç·å‡¦ç†æ™‚é–“" and v > 0}
    sorted_timers = sorted(processing_timers.items(), key=lambda x: x[1], reverse=True)

    print("\nâš ï¸  å‡¦ç†æ™‚é–“TOP5:")
    for i, (label, elapsed) in enumerate(sorted_timers[:5], 1):
        percentage = elapsed / total_elapsed * 100
        print(f"  {i}. {label}: {elapsed:.2f}ç§’ ({percentage:.1f}%)")

    print("\n" + "=" * 80)
    print(f"âœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†: ç·å‡¦ç†æ™‚é–“ {total_elapsed:.2f}ç§’")
    print("=" * 80)


if __name__ == "__main__":
    import sys

    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°ã‚’å¼•æ•°ã‹ã‚‰å–å¾—ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10ä»¶ï¼‰
    num_docs = int(sys.argv[1]) if len(sys.argv) > 1 else 10

    asyncio.run(benchmark_term_extraction(num_docs))
